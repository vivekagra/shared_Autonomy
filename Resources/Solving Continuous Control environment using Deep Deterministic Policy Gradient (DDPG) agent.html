<!DOCTYPE html>
<!-- saved from url=(0129)https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d -->
<html xmlns:cc="http://creativecommons.org/ns#" class="gr__medium_com"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent</title><link rel="canonical" href="https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d"><meta name="title" content="Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent"><meta name="referrer" content="origin"><meta name="description" content="The learning algorithm I chose is based on the paper “Continuous control with deep reinforcement learning” with my own modification based on my experiment and intuition in solving the “Reacher…"><meta name="theme-color" content="#000000"><meta property="og:title" content="Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent"><meta property="twitter:title" content="Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent"><meta property="og:url" content="https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*6yFHxD539Cg3wy8N5UJzfQ.jpeg"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="Learning Algorithm"><meta name="twitter:description" content="Learning Algorithm"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*6yFHxD539Cg3wy8N5UJzfQ.jpeg"><link rel="author" href="https://medium.com/@kinwo"><meta name="author" content="Henry"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="Henry"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2018-10-31T11:13:57.137Z"><meta name="twitter:creator" content="@kinwo"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="4 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/5e94f82f366d"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/5e94f82f366d"><meta property="al:android:url" content="medium://p/5e94f82f366d"><meta property="al:web:url" content="https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/5e94f82f366d"><script async="" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/branch-latest.min.js.download"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":1462,"url":"https://cdn-images-1.medium.com/max/2400/1*6yFHxD539Cg3wy8N5UJzfQ.jpeg"},"url":"https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d","dateCreated":"2018-10-31T11:13:57.137Z","datePublished":"2018-10-31T11:13:57.137Z","dateModified":"2019-03-23T09:13:09.382Z","headline":"Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent","name":"Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent","articleId":"5e94f82f366d","thumbnailUrl":"https://cdn-images-1.medium.com/max/2400/1*6yFHxD539Cg3wy8N5UJzfQ.jpeg","keywords":["Tag:Machine Learning","Tag:Pytorch","Tag:Deep Learning","Tag:Unity","LockedPostSource:1","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"Henry","url":"https://medium.com/@kinwo"},"creator":["Henry"],"publisher":{"@type":"Organization","name":"Medium","url":"https://medium.com/","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/385/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d"}</script><meta name="parsely-link" content="https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/m2.css"><link rel="stylesheet" href="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/main-branding-base.DUpq82k2YI6OvEW6173IfA.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/analytics.js.download"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/190/190/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/150/150/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/95/95/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/75/75/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope="" class="postShowScreen browser-chrome os-windows is-withMagicUnderlines v-glyph v-glyph--m2 is-js" data-gr-c-s-loaded="true" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1562635603554" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="1519" height="674"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"></div></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLightest js-metabar is-hiddenWhenMinimized is-maximized"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div></div><div class="metabar-block u-flex0"><div class="buttonSet buttonSet--wide"><label class="button button--small button--chromeless button--withIcon button--withSvgIcon inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--buttonNormal u-baseColor--placeholderNormal" title="Search Medium"><span class="svgIcon svgIcon--search svgIcon--25px u-baseColor--iconLight"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent" type="search" placeholder="Search Medium" required="true"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless u-xs-top1" href="https://medium.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a><button class="button button--small button--chromeless is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--activity js-notificationsButton u-marginRight16 u-xs-marginRight10 u-lineHeight0 u-size25x25" title="Notifications" aria-label="Notifications" data-action="open-notifications"><span class="svgIcon svgIcon--bell svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-293 409 25 25"><path d="M-273.327 423.67l-1.673-1.52v-3.646a5.5 5.5 0 0 0-6.04-5.474c-2.86.273-4.96 2.838-4.96 5.71v3.41l-1.68 1.553c-.204.19-.32.456-.32.734V427a1 1 0 0 0 1 1h3.49a3.079 3.079 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59c0-.28-.12-.55-.327-.74zm-7.173 5.63c-.842 0-1.55-.546-1.812-1.3h3.624a1.92 1.92 0 0 1-1.812 1.3zm6.35-2.45h-12.7v-2.347l1.63-1.51c.236-.216.37-.522.37-.843v-3.41c0-2.35 1.72-4.356 3.92-4.565a4.353 4.353 0 0 1 4.78 4.33v3.645c0 .324.137.633.376.85l1.624 1.477v2.373z"></path></svg></span></button><a class="button button--small button--upsellNav button--withChrome u-baseColor--buttonNormal u-xs-hide js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---nav_full" data-disable-client-nav="true" data-scroll="native">Upgrade</a><button class="button button--chromeless u-baseColor--buttonNormal is-inSiteNavBar js-userActions" aria-haspopup="true" data-action="open-userActions"><div class="avatar"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/0_toB60eKEH3klSlAD.jpg" class="avatar-image avatar-image--icon" alt="Vivek Agrawal"></div></button></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full" lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls" data-post-id="5e94f82f366d" data-source="post_page" data-tracking-context="postPage" data-scroll="native"><section name="8c3d" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="5956" id="5956" class="graf graf--h3 graf--leading graf--title">Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG)&nbsp;agent</h1><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@kinwo?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="110010fcd179" data-action-type="hover" data-user-id="110010fcd179" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/1_gglDpLlbtkty3-SwvDclyg@2x.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Henry"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 70 70" xmlns="http://www.w3.org/2000/svg"><path d="M5.53538374,19.9430227 C11.180401,8.78497536 22.6271155,1.6 35.3571429,1.6 C48.0871702,1.6 59.5338847,8.78497536 65.178902,19.9430227 L66.2496695,19.401306 C60.4023065,7.84329843 48.5440457,0.4 35.3571429,0.4 C22.17024,0.4 10.3119792,7.84329843 4.46461626,19.401306 L5.53538374,19.9430227 Z"></path><path d="M65.178902,49.9077131 C59.5338847,61.0657604 48.0871702,68.2507358 35.3571429,68.2507358 C22.6271155,68.2507358 11.180401,61.0657604 5.53538374,49.9077131 L4.46461626,50.4494298 C10.3119792,62.0074373 22.17024,69.4507358 35.3571429,69.4507358 C48.5440457,69.4507358 60.4023065,62.0074373 66.2496695,50.4494298 L65.178902,49.9077131 Z"></path></svg></div></div></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://medium.com/@kinwo" data-action="show-user-card" data-action-value="110010fcd179" data-action-type="hover" data-user-id="110010fcd179" dir="auto">Henry</a><span class="followState js-followState" data-user-id="110010fcd179"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="toggle-block-user" data-action-value="110010fcd179" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="toggle-subscribe-user" data-action-value="110010fcd179" data-action-source="post_header_lockup-110010fcd179-------------------------follow_byline" data-subscribe-source="post_header_lockup" data-follow-context-entity-id="5e94f82f366d"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2018-10-31T11:13:57.137Z">Oct 31, 2018</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="4 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div><figure name="8ef2" id="8ef2" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 533px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 76.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*6yFHxD539Cg3wy8N5UJzfQ.jpeg" data-width="2500" data-height="1904" data-is-featured="true" data-action="zoom" data-action-value="1*6yFHxD539Cg3wy8N5UJzfQ.jpeg" data-scroll="native"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/1_6yFHxD539Cg3wy8N5UJzfQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="55"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*6yFHxD539Cg3wy8N5UJzfQ.jpeg" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/1_6yFHxD539Cg3wy8N5UJzfQ(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*6yFHxD539Cg3wy8N5UJzfQ.jpeg"></noscript></div></div><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/photos/wbu4q8xk2Kc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/photos/wbu4q8xk2Kc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">rawpixel</a> on&nbsp;<a href="https://unsplash.com/search/photos/robot-arms?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/search/photos/robot-arms?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Unsplash</a></figcaption></figure><h3 name="3737" id="3737" class="graf graf--h3 graf-after--figure">Learning Algorithm</h3><p name="a427" id="a427" class="graf graf--p graf-after--h3">The learning algorithm I chose is based on the paper <em class="markup--em markup--p-em">“</em><a href="https://arxiv.org/abs/1509.02971" data-href="https://arxiv.org/abs/1509.02971" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Continuous control with deep reinforcement learning</em></a><em class="markup--em markup--p-em">” </em>with my own modification based on my experiment and intuition in solving the “<a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher" data-href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Reacher multiple ML Agents</a>” environment. My DDPG implementation is modified from the vanilla <a href="https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum" data-href="https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">DDPG agent</a> in solving single agent pendulum environment.</p><p name="0a65" id="0a65" class="graf graf--p graf-after--p">This project is an extension of my previous project in applying <a href="https://github.com/kinwo/deeprl-navigation" data-href="https://github.com/kinwo/deeprl-navigation" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Deep Q-Network (DQN) </a>to solve single agent navigation environment. The difference is this project has a more complex environment with continuous action spaces and multiple agents. To learn more about the basic of DQN, please feel to refer to my <a href="https://github.com/kinwo/deeprl-navigation" data-href="https://github.com/kinwo/deeprl-navigation" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">previous project</a>&nbsp;.</p><p name="acfe" id="acfe" class="graf graf--p graf-after--p">DDPG is a model-free policy based learning algorithm in which the agent will learn directly from the un-processed observation spaces without knowing the domain dynamic information. That means the same algorithm can be applied across domains which is a huge step forward comparing with the traditional planning algorithm.</p><p name="838c" id="838c" class="graf graf--p graf-after--p">In contrast with DQN that learn indirectly through Q-values tables, DDPG learns directly from the observation spaces through policy gradient method which estimates the weights of an optimal policy through gradient ascent which is similar to gradient descent used in neural network. Also, policy based method is better suited in solving continuous action space environment.</p><p name="5747" id="5747" class="graf graf--p graf-after--p">DDPG also employs Actor-Critic model in which the Critic model learns the value function like DQN and uses it to determine how the Actor’s policy based model should change. The Actor brings the advantage of learning in continuous actions space without the need for extra layer of optimization procedures required in a value based function while the Critic supplies the Actor with knowledge of the performance.</p><p name="b5d6" id="b5d6" class="graf graf--p graf-after--p">To mitigate the challenge of unstable learning, a number of techniques are applied like Gradient Clipping, Soft Target Update through twin local / target network and Replay Buffer. The most important one is Replay Buffer where it allows the DDPG agent to learn offline by gathering experiences collected from environment agents and sampling experiences from large Replay Memory Buffer across a set of unrelated experiences. This enables a very effective and quicker training process. Also, Batch Normalization plays an important role to ensure training can happen in mini batch and is GPU hardware optimization friendly.</p><h3 name="161e" id="161e" class="graf graf--h3 graf-after--p">Model Architecture</h3><p name="ff33" id="ff33" class="graf graf--p graf-after--h3">At the beginning of training, I used 20 individual DDPG agents corresponding to 20 agents in the environment and a single Replay Buffer which is shared by all DDPG agents. I then switched to a single DDPG agent with one Replay Buffer that has experiences collected from all 20 environment agents. I believed it’s a more intuitive way to train as that means I only need to train one single “brain” instead of 20 individual “brains”. Training 1 brain will definitely be quicker than 20 brains. I think one “super intelligent brain” can perform better overtime. My assumption seems to have paid off after I made the switch as is shown in the graph below in training result. The learning process is more stable and encouraging. As a retrospective, 20 individuals DDPG agents might be more favourable in a distributed training environment but I haven’t tried.</p><p name="e028" id="e028" class="graf graf--p graf-after--p">The Actor model is a neural network with 2 hidden layers with size of 400 and 300, Tanh is used in the final layer that maps states to actions. Batch normalization is used for mini batch training.</p><p name="b397" id="b397" class="graf graf--p graf-after--p">The Critic model is similar to Actor model except the final layer is a fully connected layer that maps states and actions to Q-values.</p><h3 name="1704" id="1704" class="graf graf--h3 graf-after--p">Hyperparameter</h3><p name="0ef2" id="0ef2" class="graf graf--p graf-after--h3">The learning rate for both Actor and Critic is 1e-3 with soft update of target set to the same 1e-3. Gamma discount is 0.99 with weight decay set to 0. Replay Buffer has size of 1e6. Batch size is 1024 with max time step of 1000 in each episode. A large Replay Buffer is crucial in the success of the learning. Number of time steps plays an important role as the agent needs to have enough time steps to have a good balance between exploitation and exploration.</p><h3 name="8b36" id="8b36" class="graf graf--h3 graf-after--p">Training Result</h3><p name="81b2" id="81b2" class="graf graf--p graf-after--h3">The DDPG agent takes 102 episodes to achieve an average rewards of 30 scores in about 1 hour 30 mins trained in GPU instance. The score becomes stable when it reaches 37 at about 40th episode. A sharp increase of score is observed between 20th and 30th episode.</p><figure name="45c7" id="45c7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 488px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*GmS5PeB9UWHg6C3J" data-width="784" data-height="546" data-action="zoom" data-action-value="0*GmS5PeB9UWHg6C3J" data-scroll="native"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/0_GmS5PeB9UWHg6C3J" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="51"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/0*GmS5PeB9UWHg6C3J"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/0*GmS5PeB9UWHg6C3J"></noscript></div></div></figure><p name="84fd" id="84fd" class="graf graf--p graf-after--figure"><em class="markup--em markup--p-em">Figure 1 — No. of episodes / Scores</em></p><p name="c611" id="c611" class="graf graf--p graf-after--p">The trained DDPG agent performs pretty well in tracing the moving target location for all 20 arms.</p><figure name="2104" id="2104" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 418px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*abi5-NIgfD17_WHl" data-width="1282" data-height="766" data-action="zoom" data-action-value="0*abi5-NIgfD17_WHl" data-scroll="native"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/0_abi5-NIgfD17_WHl" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="43"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/0*abi5-NIgfD17_WHl"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/0*abi5-NIgfD17_WHl"></noscript></div></div></figure><p name="ba73" id="ba73" class="graf graf--p graf-after--figure"><em class="markup--em markup--p-em">Figure 2 — Running the DDPG agent with saved weights</em></p><h3 name="5982" id="5982" class="graf graf--h3 graf-after--p">Future Improvements</h3><p name="aed7" id="aed7" class="graf graf--p graf-after--h3">We could improve the novel DDPG algorithm by applying Priority Experienced Replay in which important experience will be sampled more often. Based on the paper “<a href="https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd" data-href="https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">A novel DDPG method with prioritized experience replay</a>”, it can reduce the training time, improve the stability of the training process and is less sensitive to the change in hyperparameters.</p><p name="a4c7" id="a4c7" class="graf graf--p graf-after--p">Source Code: <a href="https://github.com/kinwo/deeprl-continuous-control" data-href="https://github.com/kinwo/deeprl-continuous-control" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/kinwo/deeprl-continuous-control</a></p><h3 name="6a8f" id="6a8f" class="graf graf--h3 graf-after--p">Credits</h3><ul class="postList"><li name="2567" id="2567" class="graf graf--li graf-after--h3">Continuous control with deep reinforcement learning <a href="https://arxiv.org/abs/1509.02971" data-href="https://arxiv.org/abs/1509.02971" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/1509.02971</a></li><li name="b9e2" id="b9e2" class="graf graf--li graf-after--li">Deep Deterministic Policy Gradient Actor-Critic Model <a href="https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum" data-href="https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum</a></li><li name="7537" id="7537" class="graf graf--li graf-after--li graf--trailing">A novel DDPG method with prioritized experience replay <a href="https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd" data-href="https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd</a></li></ul></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"><div class="postMetaInline postMetaInline--acknowledgments u-paddingTop5 u-paddingBottom20 js-postMetaAcknowledgments"></div></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://medium.com/tag/machine-learning?source=post" data-action-source="post">Machine Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/pytorch?source=post" data-action-source="post">Pytorch</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/deep-learning?source=post" data-action-source="post">Deep Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/unity?source=post" data-action-source="post">Unity</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5e94f82f366d" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----5e94f82f366d---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboarding" data-action="multivote" data-action-value="5e94f82f366d" data-action-type="long-press" data-action-source="post_actions_footer-----5e94f82f366d---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="5e94f82f366d"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker" data-action="show-recommends" data-action-value="5e94f82f366d">139 claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/5e94f82f366d/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/5e94f82f366d/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5e94f82f366d" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="110010fcd179"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="toggle-block-user" data-action-value="110010fcd179" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="toggle-subscribe-user" data-action-value="110010fcd179" data-action-source="footer_card-110010fcd179-------------------------follow_footer" data-subscribe-source="footer_card" data-follow-context-entity-id="5e94f82f366d"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://medium.com/@kinwo?source=footer_card" title="Go to the profile of Henry" aria-label="Go to the profile of Henry" data-action-source="footer_card" data-user-id="110010fcd179" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/1_gglDpLlbtkty3-SwvDclyg@2x(1).jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Henry"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 12px); height: calc(100% + 12px); top:-6px; left:-6px"><svg viewBox="0 0 114 114" xmlns="http://www.w3.org/2000/svg"><path d="M7.66922967,32.092726 C17.0070768,13.6353618 35.9421928,1.75 57,1.75 C78.0578072,1.75 96.9929232,13.6353618 106.33077,32.092726 L107.66923,31.4155801 C98.0784505,12.4582656 78.6289015,0.25 57,0.25 C35.3710985,0.25 15.9215495,12.4582656 6.33077033,31.4155801 L7.66922967,32.092726 Z"></path><path d="M106.33077,81.661427 C96.9929232,100.118791 78.0578072,112.004153 57,112.004153 C35.9421928,112.004153 17.0070768,100.118791 7.66922967,81.661427 L6.33077033,82.338573 C15.9215495,101.295887 35.3710985,113.504153 57,113.504153 C78.6289015,113.504153 98.0784505,101.295887 107.66923,82.338573 L106.33077,81.661427 Z"></path></svg></div></div></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://medium.com/@kinwo" property="cc:attributionName" title="Go to the profile of Henry" aria-label="Go to the profile of Henry" rel="author cc:attributionUrl" data-user-id="110010fcd179" dir="auto">Henry</a></h3><div class="ui-caption u-textColorGreenNormal u-fontSize13 u-tintSpectrum u-accentColor--textNormal u-marginBottom7">Medium member since Apr 2018</div><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">I bring ideas from simple vision to reality. I am a startup tech lead with interests in mobile, iOS, Blockchain, Decentrailized App, AI &amp; Deep Learning</p></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="5e94f82f366d" data-scroll="native"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1032 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="d7b0ae6f8320" data-source="placement_card_footer_grid---------0-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/generating-optical-flow-using-nvidia-flownet2-pytorch-implementation-d7b0ae6f8320?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/1*Un3UEItWCuxit6Uf1BkaDA.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/generating-optical-flow-using-nvidia-flownet2-pytorch-implementation-d7b0ae6f8320?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Also tagged Pytorch</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Generating Optical Flow using NVIDIA flownet2-pytorch Implementation</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@markgituma" data-action="show-user-card" data-action-value="e69ad71e0901" data-action-type="hover" data-user-id="e69ad71e0901" data-collection-slug="towards-data-science" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/1_F6I809_7aeWMNG7XixGCug.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Mark Gituma"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@markgituma?source=placement_card_footer_grid---------0-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-43" data-action-value="e69ad71e0901" data-action-type="hover" data-user-id="e69ad71e0901" data-collection-slug="towards-data-science" dir="auto">Mark Gituma</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/generating-optical-flow-using-nvidia-flownet2-pytorch-implementation-d7b0ae6f8320?source=placement_card_footer_grid---------0-43" data-action="open-post" data-action-value="https://towardsdatascience.com/generating-optical-flow-using-nvidia-flownet2-pytorch-implementation-d7b0ae6f8320?source=placement_card_footer_grid---------0-43" data-action-source="preview-listing"><time datetime="2019-07-04T19:59:20.757Z">Jul 5</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="11 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="d7b0ae6f8320" data-is-label-padded="true" data-source="placement_card_footer_grid-----d7b0ae6f8320----0-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="d7b0ae6f8320" data-action-type="long-press" data-action-source="placement_card_footer_grid-----d7b0ae6f8320----0-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="d7b0ae6f8320">10</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="d7b0ae6f8320" data-action-source="placement_card_footer_grid-----d7b0ae6f8320----0-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="4c1b3fb7f756" data-source="placement_card_footer_grid---------1-60" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756?source=placement_card_footer_grid---------1-60" data-action-source="placement_card_footer_grid---------1-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/1*Ro0eljp76VT_HabjtDyPDg.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756?source=placement_card_footer_grid---------1-60" data-action-source="placement_card_footer_grid---------1-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Related reads</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">How to match DeepMind’s Deep Q-Learning score in Breakout</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@fabiograetz" data-action="show-user-card" data-action-value="fb820388a7e9" data-action-type="hover" data-user-id="fb820388a7e9" data-collection-slug="towards-data-science" dir="auto"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/1_N_Mws7QxNmUVGfEt4dpIfA.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Fabio M. Graetz"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@fabiograetz?source=placement_card_footer_grid---------1-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-60" data-action-value="fb820388a7e9" data-action-type="hover" data-user-id="fb820388a7e9" data-collection-slug="towards-data-science" dir="auto">Fabio M. Graetz</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756?source=placement_card_footer_grid---------1-60" data-action="open-post" data-action-value="https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756?source=placement_card_footer_grid---------1-60" data-action-source="preview-listing"><time datetime="2018-08-26T20:22:56.301Z">Aug 27, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="4c1b3fb7f756" data-is-label-padded="true" data-source="placement_card_footer_grid-----4c1b3fb7f756----1-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="4c1b3fb7f756" data-action-type="long-press" data-action-source="placement_card_footer_grid-----4c1b3fb7f756----1-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="4c1b3fb7f756">621</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="4c1b3fb7f756" data-action-source="placement_card_footer_grid-----4c1b3fb7f756----1-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="7400cc67b8d9" data-source="placement_card_footer_grid---------2-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------2-43" data-action-source="placement_card_footer_grid---------2-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/0*KfHijq1bO1nDV5Dl.jpg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------2-43" data-action-source="placement_card_footer_grid---------2-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Also tagged Machine Learning</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Predicting Who’s going to Survive on Titanic Dataset</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@vincentkernn" data-action="show-user-card" data-action-value="9848578f8495" data-action-type="hover" data-user-id="9848578f8495" data-collection-slug="towards-data-science" dir="auto"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/2_5VLESjo09eI1gMxevfyehg.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Vincent Tatan"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@vincentkernn?source=placement_card_footer_grid---------2-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-43" data-action-value="9848578f8495" data-action-type="hover" data-user-id="9848578f8495" data-collection-slug="towards-data-science" dir="auto">Vincent Tatan</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------2-43" data-action="open-post" data-action-value="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------2-43" data-action-source="preview-listing"><time datetime="2019-07-09T01:05:39.793Z">Jul 9</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="11 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="7400cc67b8d9" data-is-label-padded="true" data-source="placement_card_footer_grid-----7400cc67b8d9----2-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="7400cc67b8d9" data-action-type="long-press" data-action-source="placement_card_footer_grid-----7400cc67b8d9----2-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="7400cc67b8d9">3</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="7400cc67b8d9" data-action-source="placement_card_footer_grid-----7400cc67b8d9----2-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream-editor cardChromeless u-marginBottom20 u-paddingLeft20 u-paddingRight20 js-responsesStreamEditor"><div class="inlineNewPostControl js-inlineNewPostControl" data-action-scope="_actionscope_7"><div class="inlineEditor is-collapsed is-postEditMode js-inlineEditor" data-action="focus-editor"><div class="u-paddingTop20 js-block js-inlineEditorContent"><div class="inlineEditor-header"><div class="inlineEditor-avatar u-paddingRight20"><div class="avatar u-inline"><img src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/0_toB60eKEH3klSlAD(1).jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Vivek Agrawal"></div></div><div class="inlineEditor-headerContent"><div class="inlineEditor-placeholder js-inlineEditorPrompt">Be the first to write a response…</div><div class="inlineEditor-author u-accentColor--textNormal">Vivek Agrawal</div></div></div></div></div></div></div><div class="responsesStream js-responsesStream"></div><div class="container u-hide js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-fixed u-sm-hide js-postShareWidget u-transition--fadeOut300" data-scroll="fixed" style="transform: translateY(150px);"><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5e94f82f366d" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----5e94f82f366d---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="5e94f82f366d" data-action-type="long-press" data-action-source="post_share_widget-----5e94f82f366d---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="5e94f82f366d">139</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5e94f82f366d" data-action-source="post_share_widget-----5e94f82f366d---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/5e94f82f366d/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/5e94f82f366d/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a></li></ul></div></aside><div class="highlightMenu" data-action-scope="_actionscope_3"><div class="highlightMenu-inner"><div class="buttonSet"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="quote" data-action-source="quote_menu--------------------------highlight_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="quote-respond" data-action-source="quote_menu--------------------------respond_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless button--highlightMenu js-highlightMenuTwitterShare" href="https://medium.com/p/5e94f82f366d/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action="twitter"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="highlight" data-action-source="quote_menu--------------------------privatenote_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div><div class="postMeterBar  u-width100pct u-fixed u-overflowHidden u-backgroundWhite u-boxShadowTop u-borderBottomLightest u-bottom0 js-meterBanner" role="alert" aria-live="assertive" data-action-scope="_actionscope_6" style=""><div class="u-borderBox u-backgroundWhite u-marginAuto u-xs-marginHorizontal10 u-paddingLeft20 u-paddingRight100 u-xs-paddingVertical15 u-xs-paddingHorizontal10 u-maxWidth1040 u-sm-maxWidth740 "><div class="u-flexCenter u-xs-height72"><a class="link link--noUnderline u-baseColor--link js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---post_counter_icons--5e94f82f366d-----------------110010fcd179" data-post-id="5e94f82f366d" data-disable-client-nav="true" data-scroll="native"><div class="u-marginLeft65 u-marginRight40 u-paddingRight4 u-xs-marginLeft20 u-xs-marginRight10"><div class="slotMachine-viewPort u-height100 u-overflowHidden u-relative"><div class="slotMachine-barrel u-textColorGreenNormal u-fontSize42"><div class="slotMachine-number u-flexCenter u-justifyContentCenter u-height100">3</div><div class="slotMachine-number u-flexCenter u-justifyContentCenter u-height100">2</div></div></div></div></a><div class="u-flex1 u-marginTop8 u-xs-marginLeft10 u-xs-marginTop0"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-xs-hide"><div class="ui-brand2"><a class=" js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---post_counter_text--5e94f82f366d-----------------110010fcd179" data-post-id="5e94f82f366d" data-disable-client-nav="true" data-scroll="native">Smart stories. New ideas. No ads. $5/month.</a></div></div><div class="uiScale uiScale-ui--small uiScale-caption--regular u-xs-show"><div class="ui-h4"><a class=" js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---post_counter_text--5e94f82f366d-----------------110010fcd179" data-post-id="5e94f82f366d" data-disable-client-nav="true">Smart stories. New ideas. No ads. $5/month.</a></div></div></div><div class="u-flex0 u-marginLeft20 u-xs-marginLeft10 js-expandMeterButton"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--withIconRight button--withIconAndLabel" data-action="expand-meter"><span class="button-label  js-buttonLabel"> Details </span><span class="svgIcon svgIcon--arrowDown svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M3.854 5.146a.5.5 0 0 0-.708.708L7.5 10.207l4.354-4.353a.5.5 0 1 0-.708-.708L7.5 8.793 3.854 5.146z" fill-rule="evenodd"></path></svg></span></button></div><div class="u-flex0 u-marginLeft40 u-flexCenter u-textColorNormal u-xs-marginLeft10 u-xs-hide js-dismissMeterIndicator"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-dismissMeterButton" data-action="dismiss-meter"><span class="svgIcon svgIcon--removeThin svgIcon--19px"><svg class="svgIcon-use" width="19" height="19"><path d="M13.792 4.6l-4.29 4.29-4.29-4.29-.612.613 4.29 4.29-4.29 4.29.613.612 4.29-4.29 4.29 4.29.612-.613-4.29-4.29 4.29-4.29" fill-rule="evenodd"></path></svg></span></button></div></div><div class="postMeterBar-details uiScale uiScale-ui--regular uiScale-caption--regular u-marginLeft137 u-sm-marginLeft137 u-xs-margin0 u-paddingRight20 u-xs-padding0 js-moreInfo"><p class="ui-body u-xs-borderTopLight u-marginTop0 u-marginBottom10 u-xs-paddingTop10">Members get unlimited access to the best stories on Medium — but you can read three for free this month. Upgrade for $5/month.</p><div class="u-marginTop20 u-xs-marginTop10 u-paddingBottom25 u-xs-paddingBottom0"><a class="button button--filled button--dark button--withChrome u-xs-sizeFullWidth js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---post_counter--5e94f82f366d-----------------110010fcd179" data-post-id="5e94f82f366d" data-disable-client-nav="true" data-scroll="native">Upgrade</a></div></div></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"38048-00ee67b","currentUser":{"userId":"be025283e717","username":"vivekagr12199","name":"Vivek Agrawal","email":"vivekagr12199@gmail.com","imageId":"0*toB60eKEH3klSlAD.jpg","createdAt":1555886025012,"lastPostCreatedAt":1562172632717,"isVerified":true,"subscriberEmail":"","onboardingStatus":1,"googleAccountId":"101203869735328059129","googleEmail":"vivekagr12199@gmail.com","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":true,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":true,"isWriterProgramOptedOut":false,"writerProgramVersion":5,"writerProgramEnrolledAt":1555886025012,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":true},"currentUserHasUnverifiedEmail":false,"isAuthenticated":true,"isCurrentUserVerified":true,"language":"en-in","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.yPFOKEJjWpZVGvh54bK7SQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.cUeGLoj7sHNpuw-bFOuTeQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.6FOGY6ouqNMySk5yHMmW3A.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.d2Ua48DeSXmhnx6BikN_Iw.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.lGxiZyGF1luZqFEkF5YgGw.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.RzL89au1QllN2FPu92pO7w.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.cg-IrTO8WysnMg0c__FJHA.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.b2XUkpbaZgVu0fuO48AaIw.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.wvhZeq_iZ2e-AecOAxjiBg.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.qf6Gl40snMZcEXwnIYeTVw.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.WhICKSRsJMorvucd3Nl7Jg.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1562635587877:40a62b8ff900","useragent":{"browser":"chrome","family":"chrome","os":"windows","version":75,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":1,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"isSeoBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_international_tax_withholding":true,"enable_international_tax_withholding_documentation":true,"enable_revised_first_partner_program_distro_on_email":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_primary_topic_for_mobile":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_embedding_based_diversification":true,"enable_pub_newsletters":true,"enable_lite_pub_header_menu":true,"enable_lite_claps":true,"enable_lite_post_manager_gear_menu":true,"enable_live_user_post_scoring":true,"enable_lite_post_highlights":true,"enable_lite_post_highlights_view_only":true,"enable_tick_landing_page":true,"enable_lite_private_notes":true,"enable_trumpland_landing_page":true,"enable_lite_email_sign_in_flow":true,"enable_daily_read_digest_promo":true,"enable_lite_paywall_alert":true,"enable_serve_recs_from_ml_rank_homepage":true,"enable_serve_recs_from_ml_rank_digest":true,"enable_serve_recs_from_ml_rank_app_highlights":true,"enable_lite_google_captcha":true,"enable_lite_branch_io":true,"enable_lite_notifications":true,"enable_ticks_digest_promo":true,"enable_lite_verify_email_butter_bar":true,"enable_lite_unread_notification_count":true},"xsrfToken":"KdR9CGheyWKQ","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"0f4e2a9c4e735bee\",\"ot-tracer-traceid\":\"147064c33a355507\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"li_post_page","type":0,"url":"www.calendly.com"},{"promptId":"li_home_page","type":1,"url":"mediumuserfeedback.typeform.com/to/GcFjEO"},{"promptId":"li_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"IN","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618"]}}
// ]]></script><script charset="UTF-8" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/main-base.bundle.yPFOKEJjWpZVGvh54bK7SQ.js.download" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"5e94f82f366d","versionId":"3498a4b520ec","creatorId":"110010fcd179","creator":{"userId":"110010fcd179","name":"Henry","username":"kinwo","createdAt":1373892401122,"imageId":"1*gglDpLlbtkty3-SwvDclyg@2x.jpeg","backgroundImageId":"","bio":"I bring ideas from simple vision to reality. I am a startup tech lead with interests in mobile, iOS, Blockchain, Decentrailized App, AI & Deep Learning","twitterScreenName":"kinwo","socialStats":{"userId":"110010fcd179","usersFollowedCount":222,"usersFollowedByCount":245,"type":"SocialStats"},"social":{"userId":"be025283e717","targetUserId":"110010fcd179","type":"Social"},"facebookAccountId":"10154499198825201","allowNotes":1,"mediumMemberAt":1524957726000,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollectionId":"","title":"Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent","detectedLanguage":"en","latestVersion":"3498a4b520ec","latestPublishedVersion":"3498a4b520ec","hasUnpublishedEdits":false,"latestRev":97,"createdAt":1540983603299,"updatedAt":1553332389382,"acceptedAt":0,"firstPublishedAt":1540984437137,"latestPublishedAt":1542718769419,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"Learning Algorithm","bodyModel":{"paragraphs":[{"name":"5956","type":3,"text":"Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent","markups":[]},{"name":"8ef2","type":4,"text":"Photo by rawpixel on Unsplash","markups":[{"type":3,"start":9,"end":17,"href":"https://unsplash.com/photos/wbu4q8xk2Kc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText","title":"","rel":"","anchorType":0},{"type":3,"start":21,"end":29,"href":"https://unsplash.com/search/photos/robot-arms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*6yFHxD539Cg3wy8N5UJzfQ.jpeg","originalWidth":2500,"originalHeight":1904,"isFeatured":true}},{"name":"3737","type":3,"text":"Learning Algorithm","markups":[]},{"name":"a427","type":1,"text":"The learning algorithm I chose is based on the paper “Continuous control with deep reinforcement learning” with my own modification based on my experiment and intuition in solving the “Reacher multiple ML Agents” environment. My DDPG implementation is modified from the vanilla DDPG agent in solving single agent pendulum environment.","markups":[{"type":3,"start":54,"end":105,"href":"https://arxiv.org/abs/1509.02971","title":"","rel":"","name":"","anchorType":0,"creatorIds":[],"userId":""},{"type":3,"start":185,"end":211,"href":"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher","title":"","rel":"","name":"","anchorType":0,"creatorIds":[],"userId":""},{"type":3,"start":278,"end":288,"href":"https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum","title":"","rel":"","name":"","anchorType":0,"creatorIds":[],"userId":""},{"type":2,"start":53,"end":107,"href":"","title":"","rel":"","name":"","anchorType":0,"creatorIds":[],"userId":""}]},{"name":"0a65","type":1,"text":"This project is an extension of my previous project in applying Deep Q-Network (DQN) to solve single agent navigation environment. The difference is this project has a more complex environment with continuous action spaces and multiple agents. To learn more about the basic of DQN, please feel to refer to my previous project .","markups":[{"type":3,"start":64,"end":85,"href":"https://github.com/kinwo/deeprl-navigation","title":"","rel":"noopener","anchorType":0},{"type":3,"start":309,"end":325,"href":"https://github.com/kinwo/deeprl-navigation","title":"","rel":"noopener","anchorType":0}]},{"name":"acfe","type":1,"text":"DDPG is a model-free policy based learning algorithm in which the agent will learn directly from the un-processed observation spaces without knowing the domain dynamic information. That means the same algorithm can be applied across domains which is a huge step forward comparing with the traditional planning algorithm.","markups":[]},{"name":"838c","type":1,"text":"In contrast with DQN that learn indirectly through Q-values tables, DDPG learns directly from the observation spaces through policy gradient method which estimates the weights of an optimal policy through gradient ascent which is similar to gradient descent used in neural network. Also, policy based method is better suited in solving continuous action space environment.","markups":[]},{"name":"5747","type":1,"text":"DDPG also employs Actor-Critic model in which the Critic model learns the value function like DQN and uses it to determine how the Actor’s policy based model should change. The Actor brings the advantage of learning in continuous actions space without the need for extra layer of optimization procedures required in a value based function while the Critic supplies the Actor with knowledge of the performance.","markups":[]},{"name":"b5d6","type":1,"text":"To mitigate the challenge of unstable learning, a number of techniques are applied like Gradient Clipping, Soft Target Update through twin local / target network and Replay Buffer. The most important one is Replay Buffer where it allows the DDPG agent to learn offline by gathering experiences collected from environment agents and sampling experiences from large Replay Memory Buffer across a set of unrelated experiences. This enables a very effective and quicker training process. Also, Batch Normalization plays an important role to ensure training can happen in mini batch and is GPU hardware optimization friendly.","markups":[]},{"name":"161e","type":3,"text":"Model Architecture","markups":[]},{"name":"ff33","type":1,"text":"At the beginning of training, I used 20 individual DDPG agents corresponding to 20 agents in the environment and a single Replay Buffer which is shared by all DDPG agents. I then switched to a single DDPG agent with one Replay Buffer that has experiences collected from all 20 environment agents. I believed it’s a more intuitive way to train as that means I only need to train one single “brain” instead of 20 individual “brains”. Training 1 brain will definitely be quicker than 20 brains. I think one “super intelligent brain” can perform better overtime. My assumption seems to have paid off after I made the switch as is shown in the graph below in training result. The learning process is more stable and encouraging. As a retrospective, 20 individuals DDPG agents might be more favourable in a distributed training environment but I haven’t tried.","markups":[]},{"name":"e028","type":1,"text":"The Actor model is a neural network with 2 hidden layers with size of 400 and 300, Tanh is used in the final layer that maps states to actions. Batch normalization is used for mini batch training.","markups":[]},{"name":"b397","type":1,"text":"The Critic model is similar to Actor model except the final layer is a fully connected layer that maps states and actions to Q-values.","markups":[]},{"name":"1704","type":3,"text":"Hyperparameter","markups":[]},{"name":"0ef2","type":1,"text":"The learning rate for both Actor and Critic is 1e-3 with soft update of target set to the same 1e-3. Gamma discount is 0.99 with weight decay set to 0. Replay Buffer has size of 1e6. Batch size is 1024 with max time step of 1000 in each episode. A large Replay Buffer is crucial in the success of the learning. Number of time steps plays an important role as the agent needs to have enough time steps to have a good balance between exploitation and exploration.","markups":[]},{"name":"8b36","type":3,"text":"Training Result","markups":[]},{"name":"81b2","type":1,"text":"The DDPG agent takes 102 episodes to achieve an average rewards of 30 scores in about 1 hour 30 mins trained in GPU instance. The score becomes stable when it reaches 37 at about 40th episode. A sharp increase of score is observed between 20th and 30th episode.","markups":[]},{"name":"45c7","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*GmS5PeB9UWHg6C3J","originalWidth":784,"originalHeight":546}},{"name":"84fd","type":1,"text":"Figure 1 — No. of episodes / Scores","markups":[{"type":2,"start":0,"end":35}]},{"name":"c611","type":1,"text":"The trained DDPG agent performs pretty well in tracing the moving target location for all 20 arms.","markups":[]},{"name":"2104","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*abi5-NIgfD17_WHl","originalWidth":1282,"originalHeight":766}},{"name":"ba73","type":1,"text":"Figure 2 — Running the DDPG agent with saved weights","markups":[{"type":2,"start":0,"end":52}]},{"name":"5982","type":3,"text":"Future Improvements","markups":[]},{"name":"aed7","type":1,"text":"We could improve the novel DDPG algorithm by applying Priority Experienced Replay in which important experience will be sampled more often. Based on the paper “A novel DDPG method with prioritized experience replay”, it can reduce the training time, improve the stability of the training process and is less sensitive to the change in hyperparameters.","markups":[{"type":3,"start":160,"end":214,"href":"https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd","title":"","rel":"","anchorType":0}]},{"name":"a4c7","type":1,"text":"Source Code: https://github.com/kinwo/deeprl-continuous-control","markups":[{"type":3,"start":13,"end":63,"href":"https://github.com/kinwo/deeprl-continuous-control","title":"","rel":"nofollow","anchorType":0}]},{"name":"6a8f","type":3,"text":"Credits","markups":[]},{"name":"2567","type":9,"text":"Continuous control with deep reinforcement learning https://arxiv.org/abs/1509.02971","markups":[{"type":3,"start":52,"end":84,"href":"https://arxiv.org/abs/1509.02971","title":"","rel":"","anchorType":0}]},{"name":"b9e2","type":9,"text":"Deep Deterministic Policy Gradient Actor-Critic Model https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum","markups":[{"type":3,"start":54,"end":134,"href":"https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum","title":"","rel":"","anchorType":0}]},{"name":"7537","type":9,"text":"A novel DDPG method with prioritized experience replay https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd","markups":[{"type":3,"start":55,"end":189,"href":"https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd","title":"","rel":"","anchorType":0}]}],"sections":[{"name":"8c3d","startIndex":0}]},"postDisplay":{"coverless":true}},"virtuals":{"allowNotes":true,"previewImage":{"imageId":"1*6yFHxD539Cg3wy8N5UJzfQ.jpeg","filter":"","backgroundSize":"","originalWidth":2500,"originalHeight":1904,"strategy":"resample","height":0,"width":0},"wordCount":869,"imageCount":3,"readingTime":3.829245283018868,"subtitle":"Learning Algorithm","usersBySocialRecommends":[],"noIndex":false,"recommends":11,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":76459,"metadata":{"postCount":76459,"coverImage":{"id":"1*Objn6iYe6g4-DLDV67JKWA.jpeg","originalWidth":1904,"originalHeight":1068,"isFeatured":true}},"type":"Tag"},{"slug":"pytorch","name":"Pytorch","postCount":751,"metadata":{"postCount":751,"coverImage":{"id":"1*X3TNZbdu06LWNCpHixIFxQ.jpeg","originalWidth":5472,"originalHeight":3648,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":18762,"metadata":{"postCount":18762,"coverImage":{"id":"1*uyQ61XHkTY2qHhovclaLIQ.png","originalWidth":1073,"originalHeight":741,"isFeatured":true}},"type":"Tag"},{"slug":"unity","name":"Unity","postCount":4067,"metadata":{"postCount":4067,"coverImage":{"id":"1*C8q8echAmfXhCaaGCRBgQg.png","originalWidth":4209,"originalHeight":1253}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":0,"links":{"entries":[{"url":"https://unsplash.com/photos/wbu4q8xk2Kc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText","alts":[],"httpStatus":200},{"url":"https://github.com/kinwo/deeprl-continuous-control","alts":[],"httpStatus":200},{"url":"https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum","alts":[],"httpStatus":200},{"url":"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher","alts":[],"httpStatus":200},{"url":"https://arxiv.org/abs/1509.02971","alts":[],"httpStatus":200},{"url":"https://github.com/kinwo/deeprl-navigation","alts":[],"httpStatus":200},{"url":"https://www.semanticscholar.org/paper/A-novel-DDPG-method-with-prioritized-experience-Hou-Liu/027d002d205e49989d734603ff0c2f7cbfa6b6dd","alts":[],"httpStatus":200},{"url":"https://unsplash.com/search/photos/robot-arms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1542718770400},"isLockedPreviewOnly":false,"metaDescription":"","totalClapCount":139,"sectionCount":1,"readingList":0,"topics":[]},"coverless":true,"slug":"solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":false,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":2,"uniqueSlug":"solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"1*6yFHxD539Cg3wy8N5UJzfQ.jpeg","originalWidth":2500,"originalHeight":1904,"isFeatured":true}},{"name":"5956","type":3,"text":"Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"Learning Algorithm"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d","approvedHomeCollectionId":"","newsletterId":"","webCanonicalUrl":"https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d","mediumUrl":"https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":true,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":true,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":1,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"isProxyPost":false,"proxyPostFaviconUrl":"","proxyPostProviderName":"","proxyPostType":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"hideMeter":false,"meteringInfo":{"postIds":["5e94f82f366d"],"maxUnlockCount":3,"unlocksRemaining":2,"windowLength":3,"currentMeterCount":9},"collectionUserRelations":[],"mode":null,"references":{"User":{"110010fcd179":{"userId":"110010fcd179","name":"Henry","username":"kinwo","createdAt":1373892401122,"imageId":"1*gglDpLlbtkty3-SwvDclyg@2x.jpeg","backgroundImageId":"","bio":"I bring ideas from simple vision to reality. I am a startup tech lead with interests in mobile, iOS, Blockchain, Decentrailized App, AI & Deep Learning","twitterScreenName":"kinwo","socialStats":{"userId":"110010fcd179","usersFollowedCount":222,"usersFollowedByCount":245,"type":"SocialStats"},"social":{"userId":"be025283e717","targetUserId":"110010fcd179","type":"Social"},"facebookAccountId":"10154499198825201","allowNotes":1,"mediumMemberAt":1524957726000,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Social":{"110010fcd179":{"userId":"be025283e717","targetUserId":"110010fcd179","type":"Social"}},"SocialStats":{"110010fcd179":{"userId":"110010fcd179","usersFollowedCount":222,"usersFollowedByCount":245,"type":"SocialStats"}}}})
// ]]></script><script>window.PARSELY = window.PARSELY || { autotrack: false }</script><script id="parsely-cfg" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/p.js.download"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/main-common-async.bundle.cUeGLoj7sHNpuw-bFOuTeQ.js.download"></script><div id="weava-permanent-marker" date="1562635603837"></div><script charset="UTF-8" src="./Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent_files/main-notes.bundle.RzL89au1QllN2FPu92pO7w.js.download"></script><div id="weava-ui-wrapper"><div class="weava-drop-area-wrapper"><div class="weava-drop-area"></div>
<div class="weava-drop-area-text">Drop here!</div></div></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>