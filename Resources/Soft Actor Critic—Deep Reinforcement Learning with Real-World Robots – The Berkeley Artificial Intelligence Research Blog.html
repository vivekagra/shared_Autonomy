<!DOCTYPE html>
<!-- saved from url=(0046)https://bair.berkeley.edu/blog/2018/12/14/sac/ -->
<html prefix="og: http://ogp.me/ns#" class="gr__bair_berkeley_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="The BAIR Blog">
    <meta name="author" content="Daniel Seita">
    <meta name="keywords" content="">
    <link rel="canonical" href="http://bair.berkeley.edu/blog/2018/12/14/sac/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for The Berkeley Artificial Intelligence Research Blog" href="https://bair.berkeley.edu/blog/feed.xml">
    <link rel="stylesheet" href="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/pixyll.css" type="text/css">
    <link rel="stylesheet" href="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/bair-blog.css" type="text/css">

    <!-- Fonts -->
    <link href="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/css" rel="stylesheet" type="text/css">
    <link href="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/css(1)" rel="stylesheet" type="text/css">
    
      <link href="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots">
    <meta property="og:description" content="The BAIR Blog">
    <meta property="og:url" content="http://bair.berkeley.edu/blog/2018/12/14/sac/">
    <meta property="og:site_name" content="The Berkeley Artificial Intelligence Research Blog">
    <meta property="og:image" content="http://bair.berkeley.edu/blog/assets/sac/ant.png">
    <meta property="og:image:url" content="http://bair.berkeley.edu/blog/assets/sac/ant.png">

    <script async="" src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/analytics.js.download"></script><script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        messageStyle: "none",
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/MathJax.js.download" id="">
    </script>

    <!-- Daniel Seita: I added this to handle Samaneh's post. -->
    <script type="text/javascript">
    function update_im_font1(option_font) {
        document.getElementById("im_font1").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
    }
    function update_im_font2(option_font) {
        document.getElementById("im_font2").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
    }
    function update_im_font3(option_font) {
        document.getElementById("im_font3").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
    }
    function update_im_font4(option_font) {
        document.getElementById("im_font4").src="http://bair.berkeley.edu/static/blog/mcgan/"+option_font+".png";
    }
    </script>


<script type="text/javascript" async="" src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/embed.js.download"></script><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link rel="preload" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.73c498778035470a16f391458a5d5cc4.css"><link rel="preload" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.5e2845671155c097129ebd8a2aeb308d.js"><link rel="preload" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.b8bf14b0bfe753b64dddaad74e2e663a.js"><link rel="preload" as="script" href="https://disqus.com/next/config.js"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><script src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/alfalfa_min.d078e4f2a4721192a99e02601a767617.js.download" async="" charset="UTF-8"></script><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style></head>

<body class="site" data-gr-c-s-loaded="true"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <div class="blog-logo-container">
        <a href="https://bair.berkeley.edu/blog/"><img class="bair-logo" src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/BAIR_Logo.png"></a>
      </div>
      <nav class="site-nav bair-blog-site-nav">
        <a href="https://bair.berkeley.edu/blog/subscribe/">Subscribe</a>
<a href="https://bair.berkeley.edu/blog/about/">About</a>
<a href="https://bair.berkeley.edu/blog/archive/">Archive</a>
<a href="http://bair.berkeley.edu/">BAIR</a>

      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <div class="post-header mb2">
  <h1>Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots</h1>
  <div class="post-meta">
  
  <a href="https://people.eecs.berkeley.edu/~haarnoja/">Tuomas Haarnoja</a>, <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr Pong</a>, <a href="https://hartikainen.github.io/">Kristian Hartikainen</a>, Aurick Zhou, Murtaza Dalal, and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br> &nbsp;&nbsp;
  
  
  Dec 14, 2018
  
  </div>
</div>



<article class="post-content">
  <p>We are announcing the release of our state-of-the-art off-policy model-free
reinforcement learning algorithm, soft actor-critic (SAC). This algorithm has
been developed jointly at UC Berkeley and Google, and we have been using
it internally for our robotics experiment. Soft actor-critic is, to our
knowledge, one of the most efficient model-free algorithms available today,
making it especially well-suited for real-world robotic learning. In this post,
we will benchmark SAC against state-of-the-art model-free RL algorithms and
showcase a spectrum of real-world robot examples, ranging from manipulation to
locomotion. We also release our implementation of SAC, which is particularly
designed for real-world robotic systems.</p>

<!--more-->

<h1 id="desired-features-for-deep-rl-for-real-robots">Desired Features for Deep RL for Real Robots</h1>

<p>What makes an ideal deep RL algorithm for real-world systems? Real-world
experimentation brings additional challenges, such as constant interruptions in
the data stream, requirement for a low-latency inference and smooth exploration
to avoid mechanical wear and tear on the robot, which set additional requirement
for both the algorithm and also the implementation of the algorithm.</p>

<p>Regarding the algorithm, several properties are desirable:</p>

<ul>
  <li><strong>Sample Efficiency</strong>. Learning skills in the real world can take a
substantial amount of time. Prototyping a new task takes several trials, and
the total time required to learn a new skill quickly adds up. Thus good sample
complexity is the first prerequisite for successful skill acquisition.</li>
  <li><strong>No Sensitive Hyperparameters</strong>. In the real world, we want to avoid
parameter tuning for the obvious reason. Maximum entropy RL provides a robust
framework that minimizes the need for hyperparameter tuning.</li>
  <li><strong>Off-Policy Learning</strong>. An algorithm is off-policy if we can reuse data collected
for another task. In a typical scenario, we need to adjust parameters and
shape the reward function when prototyping a new task, and use of an
off-policy algorithm allows reusing the already collected data.</li>
</ul>

<p>Soft actor-critic (SAC), described below, is an off-policy model-free deep RL 
algorithm that is well aligned with these requirements. In particular, we show 
that it is sample efficient enough to solve real-world robot tasks in only a 
handful of hours, robust to hyperparameters and works on a variety of simulated 
environments with a single set of hyperparameters.</p>

<p>In addition to the desired algorithmic properties, experimentation in the
real-world sets additional requirements for the implementation. Our release
supports many of these features that we have found crucial when learning with
real robots, perhaps the most importantly:</p>

<ul>
  <li><strong>Asynchronous Sampling</strong>. Inference needs to be fast to minimize delay in the
control loop, and we typically want to keep training during the environment
resets too. Therefore, data sampling and training should run in independent
threads or processes.</li>
  <li><strong>Stop / Resume Training</strong>. When working with real hardware, whatever can go
wrong, will go wrong. We should expect constant interruptions in the data
stream.</li>
  <li><strong>Action smoothing</strong>. Typical Gaussian exploration makes the actuators jitter
at high frequency, potentially damaging the hardware. Thus temporally
correlating the exploration is important.</li>
</ul>

<h1 id="soft-actor-critic">Soft Actor-Critic</h1>

<p>Soft actor-critic is based on the maximum entropy reinforcement learning
framework, which considers the entropy augmented objective</p>

<div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style="text-align: center;"><nobr><span class="math" id="MathJax-Span-1" style="width: 23.674em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.479em; height: 0px; font-size: 128%;"><span style="position: absolute; clip: rect(2.15em, 1018.4em, 5.393em, -999.998em); top: -4.021em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.08em;"></span></span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5" style="font-family: MathJax_Math-italic;">π<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span><span class="mo" id="MathJax-Span-6" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main; padding-left: 0.275em;">=</span><span class="msubsup" id="MathJax-Span-8" style="padding-left: 0.275em;"><span style="display: inline-block; position: relative; width: 1.135em; height: 0px;"><span style="position: absolute; clip: rect(3.205em, 1000.63em, 4.104em, -999.998em); top: -3.982em; left: 0em;"><span class="texatom" id="MathJax-Span-9"><span class="mrow" id="MathJax-Span-10"><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_AMS;">E</span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span><span style="position: absolute; top: -3.826em; left: 0.666em;"><span class="texatom" id="MathJax-Span-12"><span class="mrow" id="MathJax-Span-13"><span class="mi" id="MathJax-Span-14" style="font-size: 70.7%; font-family: MathJax_Math-italic;">π<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span></span></span><span class="mrow" id="MathJax-Span-15" style="padding-left: 0.158em;"><span class="mo" id="MathJax-Span-16" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">[</span></span><span class="texatom" id="MathJax-Span-17"><span class="mrow" id="MathJax-Span-18"><span class="munderover" id="MathJax-Span-19"><span style="display: inline-block; position: relative; width: 1.447em; height: 0px;"><span style="position: absolute; clip: rect(2.932em, 1001.41em, 4.572em, -999.998em); top: -3.982em; left: 0em;"><span class="mo" id="MathJax-Span-20" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span><span style="position: absolute; clip: rect(3.439em, 1000.24em, 4.221em, -999.998em); top: -2.928em; left: 0.588em;"><span class="texatom" id="MathJax-Span-21"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span></span></span><span class="mi" id="MathJax-Span-24" style="font-family: MathJax_Math-italic; padding-left: 0.158em;">r</span><span class="mo" id="MathJax-Span-25" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-26"><span style="display: inline-block; position: relative; width: 0.783em; height: 0px;"><span style="position: absolute; clip: rect(3.4em, 1000.43em, 4.104em, -999.998em); top: -3.982em; left: 0em;"><span class="texatom" id="MathJax-Span-27"><span class="mrow" id="MathJax-Span-28"><span class="mi" id="MathJax-Span-29" style="font-family: MathJax_Main-bold;">s</span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span><span style="position: absolute; top: -3.826em; left: 0.471em;"><span class="mi" id="MathJax-Span-30" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span></span></span><span class="mo" id="MathJax-Span-31" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-32" style="padding-left: 0.158em;"><span style="display: inline-block; position: relative; width: 0.9em; height: 0px;"><span style="position: absolute; clip: rect(3.4em, 1000.55em, 4.104em, -999.998em); top: -3.982em; left: 0em;"><span class="texatom" id="MathJax-Span-33"><span class="mrow" id="MathJax-Span-34"><span class="mi" id="MathJax-Span-35" style="font-family: MathJax_Main-bold;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span><span style="position: absolute; top: -3.826em; left: 0.549em;"><span class="mi" id="MathJax-Span-36" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span></span></span><span class="mo" id="MathJax-Span-37" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-38" style="font-family: MathJax_Main; padding-left: 0.236em;">−</span><span class="mi" id="MathJax-Span-39" style="font-family: MathJax_Math-italic; padding-left: 0.236em;">α</span><span class="mi" id="MathJax-Span-40" style="font-family: MathJax_Main; padding-left: 0.158em;">log</span><span class="mo" id="MathJax-Span-41"></span><span class="mo" id="MathJax-Span-42" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-43" style="font-family: MathJax_Math-italic;">π<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span><span class="mo" id="MathJax-Span-44" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-45"><span style="display: inline-block; position: relative; width: 0.9em; height: 0px;"><span style="position: absolute; clip: rect(3.4em, 1000.55em, 4.104em, -999.998em); top: -3.982em; left: 0em;"><span class="texatom" id="MathJax-Span-46"><span class="mrow" id="MathJax-Span-47"><span class="mi" id="MathJax-Span-48" style="font-family: MathJax_Main-bold;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span><span style="position: absolute; top: -3.826em; left: 0.549em;"><span class="mi" id="MathJax-Span-49" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span></span></span><span class="texatom" id="MathJax-Span-50"><span class="mrow" id="MathJax-Span-51"><span class="mo" id="MathJax-Span-52" style="font-family: MathJax_Main;">|</span></span></span><span class="msubsup" id="MathJax-Span-53"><span style="display: inline-block; position: relative; width: 0.783em; height: 0px;"><span style="position: absolute; clip: rect(3.4em, 1000.43em, 4.104em, -999.998em); top: -3.982em; left: 0em;"><span class="texatom" id="MathJax-Span-54"><span class="mrow" id="MathJax-Span-55"><span class="mi" id="MathJax-Span-56" style="font-family: MathJax_Main-bold;">s</span></span></span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span><span style="position: absolute; top: -3.826em; left: 0.471em;"><span class="mi" id="MathJax-Span-57" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.986em;"></span></span></span></span><span class="mo" id="MathJax-Span-58" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-59" style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-60" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">]</span></span></span><span class="mo" id="MathJax-Span-61" style="font-family: MathJax_Main; padding-left: 0.158em;">,</span></span><span style="display: inline-block; width: 0px; height: 4.025em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.648em; border-left: 0px solid; width: 0px; height: 3.952em;"></span></span></nobr></span></div><script type="math/tex; mode=display" id="MathJax-Element-1">J(\pi) = \mathbb{E}_{\pi} \left[ {\sum_{t} r(\mathbf{s}_t, \mathbf{a}_t)
- \alpha \log (\pi(\mathbf{a}_t|\mathbf{s}_t))} \right],</script>

<p>where <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-62" style="width: 1em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.783em; height: 0px; font-size: 128%;"><span style="position: absolute; clip: rect(1.695em, 1000.78em, 2.563em, -999.998em); top: -2.255em; left: 0em;"><span class="mrow" id="MathJax-Span-63"><span class="msubsup" id="MathJax-Span-64"><span style="display: inline-block; position: relative; width: 0.783em; height: 0px;"><span style="position: absolute; clip: rect(3.431em, 1000.44em, 4.125em, -999.998em); top: -3.991em; left: 0em;"><span class="texatom" id="MathJax-Span-65"><span class="mrow" id="MathJax-Span-66"><span class="mi" id="MathJax-Span-67" style="font-family: MathJax_Main-bold;">s</span></span></span><span style="display: inline-block; width: 0px; height: 3.995em;"></span></span><span style="position: absolute; top: -3.861em; left: 0.436em;"><span class="mi" id="MathJax-Span-68" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.995em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.259em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-2">\mathbf{s}_t</script> and <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-69" style="width: 1.131em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.87em; height: 0px; font-size: 128%;"><span style="position: absolute; clip: rect(1.695em, 1000.87em, 2.563em, -999.998em); top: -2.255em; left: 0em;"><span class="mrow" id="MathJax-Span-70"><span class="msubsup" id="MathJax-Span-71"><span style="display: inline-block; position: relative; width: 0.87em; height: 0px;"><span style="position: absolute; clip: rect(3.431em, 1000.57em, 4.125em, -999.998em); top: -3.991em; left: 0em;"><span class="texatom" id="MathJax-Span-72"><span class="mrow" id="MathJax-Span-73"><span class="mi" id="MathJax-Span-74" style="font-family: MathJax_Main-bold;">a</span></span></span><span style="display: inline-block; width: 0px; height: 3.995em;"></span></span><span style="position: absolute; top: -3.861em; left: 0.566em;"><span class="mi" id="MathJax-Span-75" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.995em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.259em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-3">\mathbf{a}_t</script> are the state and the action, and the
expectation is taken over the policy and the true dynamics of the system. <gistnote class="gistnote-highlight" highlightid="31c3f642-e448-4fa5-9689-a697ad44205f" colornum="2" style="background-color: rgb(255, 222, 112);" id="31c3f642-e448-4fa5-9689-a697ad44205f">In
other words, the optimal policy not only maximizes the expected return (first
summand) but also the expected entropy of itself (second summand).</gistnote> The trade-off
between the two is controlled by the non-negative temperature parameter
<span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-76" style="width: 0.783em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.61em; height: 0px; font-size: 128%;"><span style="position: absolute; clip: rect(1.695em, 1000.57em, 2.389em, -999.998em); top: -2.255em; left: 0em;"><span class="mrow" id="MathJax-Span-77"><span class="mi" id="MathJax-Span-78" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.259em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-4">\alpha</script>, and we can always recover the conventional, maximum expected return
objective by setting <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-79" style="width: 3.084em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.389em; height: 0px; font-size: 128%;"><span style="position: absolute; clip: rect(1.478em, 1002.35em, 2.433em, -999.998em); top: -2.255em; left: 0em;"><span class="mrow" id="MathJax-Span-80"><span class="mi" id="MathJax-Span-81" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-82" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mn" id="MathJax-Span-83" style="font-family: MathJax_Main; padding-left: 0.263em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.259em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.108em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-5">\alpha=0</script>. In a <a href="https://arxiv.org/abs/1812.05905">technical report</a>, we show that we can 
view this objective as an entropy constrained maximization of the expected 
return, and learn the temperature parameter automatically instead of treating 
it as a hyperparameter.</p>

<p>This objective can be interpreted in several ways. <gistnote class="gistnote-highlight" highlightid="793bc765-c1c6-45bd-b8d3-a06c29925494" colornum="2" style="background-color: rgb(255, 222, 112);" id="793bc765-c1c6-45bd-b8d3-a06c29925494">We can view the entropy term
as an uninformative (uniform) prior over the policy<gistnote class="gistnote-highlight" highlightid="f09f7dc5-1ff3-45ee-bfc2-0c8bb4438990" colornum="2" style="background-color: rgb(255, 222, 112);" id="f09f7dc5-1ff3-45ee-bfc2-0c8bb4438990">,</gistnote></gistnote><gistnote class="gistnote-highlight" highlightid="f09f7dc5-1ff3-45ee-bfc2-0c8bb4438990" colornum="2" style="background-color: rgb(255, 222, 112);"> but we can also view it as
a regularizer or as an attempt to trade off between exploration (maximize
entropy) and exploitation (maximize return).</gistnote> In our <a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">previous post</a>, we gave
a broader overview and proposed applications that are unique to maximum entropy
RL, and a probabilistic view of the objective is discussed in a <a href="https://arxiv.org/abs/1805.00909">recent
tutorial</a>.  Soft actor-critic maximizes this objective by parameterizing a
Gaussian policy and a Q-function with a neural network, and optimizing them
using approximate dynamic programming. We defer further details of soft
actor-critic to the <a href="https://arxiv.org/abs/1812.05905">technical report</a>. In this post, we will view the objective as
a grounded way to derive better reinforcement learning algorithms that perform
consistently and are sample efficient enough to be applicable to real-world
robotic applications, and—perhaps surprisingly—can yield state-of-the-art
performance under the conventional, maximum expected return objective (without
entropy regularization) in simulated benchmarks.</p>

<h1 id="simulated-benchmarks">Simulated Benchmarks</h1>

<p>Before we jump into real-world experiments, we compare SAC on standard benchmark
tasks to other popular deep RL algorithms, deep deterministic policy gradient
(DDPG), twin delayed deep deterministic policy gradient (TD3), and proximal
policy optimization (PPO). The figures below compare the algorithms on three
challenging locomotion tasks, HalfCheetah, Ant, and Humanoid, from OpenAI Gym.
The solid lines depict the total average return and the shadings correspond to
the best and the worst trial over five random seeds. Indeed, soft actor-critic,
which is shown in blue, achieves the best performance, and—what’s even more
important for real-world applications—it performs well also in the worst case.
We have included more benchmark results in the <a href="https://arxiv.org/abs/1812.05905">technical report</a>.</p>

<p style="text-align:center;">
    <img src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/ant.png" height="190" style="margin: 2px;">
    <img src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/cheetah.png" height="190" style="margin: 2px;">
    <img src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/humanoid.png" height="190" style="margin: 2px;">
    <br>
</p>

<h1 id="deep-rl-in-the-real-world">Deep RL in the Real World</h1>

<p>We tested soft actor-critic in the real world by solving three tasks from 
scratch without relying on simulation or demonstrations. 
Our first real-world task involves the Minitaur robot, a small-scale quadruped
with eight direct-drive actuators. The action space consists of the swing angle
and the extension of each leg, which are then mapped to desired motor positions
and tracked with a PD controller. <gistnote class="gistnote-highlight" highlightid="c9a332ec-4cbc-4f29-9a2f-66dff0b5a60b" colornum="2" style="background-color: rgb(255, 222, 112);" id="c9a332ec-4cbc-4f29-9a2f-66dff0b5a60b">The observations include the motor angles as
well as roll and pitch angles and angular velocities of the base.</gistnote> This learning
task presents substantial challenges for real-world reinforcement learning. <gistnote class="gistnote-highlight" highlightid="f783017a-023f-417e-b871-d0fd4ae0ef49" colornum="2" style="background-color: rgb(255, 222, 112);" id="f783017a-023f-417e-b871-d0fd4ae0ef49">The
robot is underactuated, and must therefore delicately balance contact forces on
the legs to make forward progress.</gistnote> An untrained policy can lose balance and
fall, and too many falls will eventually damage the robot, making
sample-efficient learning essentially. The video below illustrates the learned
skill. Although we trained our policy only on flat terrain, we then tested it on
varied terrains and obstacles. <gistnote class="gistnote-highlight" highlightid="fbf047b9-3247-4918-ac5d-a6aeca069341" colornum="2" style="background-color: rgb(255, 222, 112);" id="fbf047b9-3247-4918-ac5d-a6aeca069341">Because soft actor-critic learns robust policies,
due to entropy maximization at training time, the policy can readily generalize
to these perturbations without any additional learning.</gistnote></p>

<p style="text-align:center;">
    <img src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/Minitaur-evaluation-all.gif" height="180" width="320">
    <br>
<i>
The Minitaur robot (Google, Tuomas Haarnoja, Sehoon Ha, Jie Tan, and
Sergey Levine).
</i>
</p>

<p>Our second real-world robotic task involves training a 3-finger dexterous
robotic hand to manipulate an object. The hand is based on the Dynamixel Claw
hand, discussed in <a href="https://bair.berkeley.edu/blog/2018/08/31/dexterous-manip/">another post</a>. This hand has 9 DoFs, each controlled by a
Dynamixel servo-motor. <gistnote class="gistnote-highlight" highlightid="9f2225a1-efb5-47fd-beea-c29802c9d495" colornum="2" style="background-color: rgb(255, 222, 112);" id="9f2225a1-efb5-47fd-beea-c29802c9d495">The policy controls the hand by sending target joint
angle positions for the on-board PID controller</gistnote>. <gistnote class="gistnote-highlight" highlightid="58663591-5853-4a0a-aa65-ab557409cd14" colornum="2" style="background-color: rgb(255, 222, 112);" id="58663591-5853-4a0a-aa65-ab557409cd14">The manipulation task requires
the hand to rotate a ``valve’‘-like object as shown in the animation below. In
order to perceive the valve, the robot must use raw RGB images shown in the
inset at the bottom right</gistnote>. The robot must rotate the valve so that the colored 
peg faces the right (see video below). The initial position of the valve is reset
uniformly at random for each episode, forcing the policy to learn to use the raw
RGB images to perceive the current valve orientation. A small motor is attached
to the valve to automate resets and to provide the ground truth position for the
determination of the reward function. The position of this motor is not provided
to the policy. This task is exceptionally challenging due to both the perception
challenges and the need to control a hand with 9 degrees of freedom.</p>

<p style="text-align:center;">
    <img src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/dclaw_vision_rollouts_combined.gif" height="240" width="320">
    <br>
<i>
Rotating a valve with a dexterous hand, learned directly from raw pixels  
(UC Berkeley, Kristian Hartikainen, Vikash Kumar, Henry Zhu, Abhishek Gupta, 
Tuomas Haarnoja, and Sergey Levine).
</i>
</p>

<p>In the final task, we trained a 7-DoF Sawyer robot to stack Lego blocks. The
policy receives the joint positions and velocities, as well as end-effector
force as an input and outputs torque commands to each of the seven joints. The
biggest challenge is to accurately align the studs before exerting a
downward force to overcome the friction between them.</p>

<p style="text-align:center;">
    <img src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/sawyer.gif" height="240" width="320">
    <br>
<i>
Stacking Legos with Sawyer (UC Berkeley, Aurick Zhou, Tuomas Haarnoja, and
Sergey Levine).
</i>
</p>

<p>Soft actor-critic solves all of these tasks quickly: the Minitaur
locomotion and the block-stacking tasks both take 2 hours, and the valve-turning
task from image observations takes 20 hours. We also learned a policy for the
valve-turning task without images by providing the actual valve position as an
observation to the policy. Soft actor-critic can learn this easier version of
the valve task in 3 hours. For comparison, <a href="https://arxiv.org/abs/1810.06045">prior work</a> has used PPO to learn
the same task without images in 7.4 hours.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Soft actor-critic is a step towards feasible deep RL with real-world robots.
Work still needs to be done to scale up these methods to more challenging tasks,
but we believe we are getting closer to the critical point where deep RL can
become a practical solution for robotic tasks. Meanwhile, you can connect your
robot to our toolbox and get learning started!</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We would like to thank the amazing teams at Google and UC
Berkeley—specifically Pieter Abbeel, Abhishek Gupta, Sehoon Ha, Vikash Kumar,
Sergey Levine, Jie Tan, George Tucker, Vincent Vanhoucke, Henry Zhu—who
contributed to the development of the algorithm, spent long days running
experiments, and provided the support and resources that made the project
possible.</p>

<p>Links:</p>
<ul>
  <li><a href="https://sites.google.com/view/sac-and-applications">Project website</a></li>
  <li><a href="https://arxiv.org/abs/1812.05905">Technical description of SAC</a></li>
  <li><a href="https://github.com/rail-berkeley/softlearning">softlearning</a> (our robot learning toolbox, including a SAC implementation in Tensorflow)</li>
  <li><a href="https://github.com/vitchyr/rlkit">rlkit</a> (another SAC implementation from UC Berkeley in PyTorch)</li>
</ul>


</article>

<div class="generic-box">
Subscribe to our <a href="https://bair.berkeley.edu/blog/feed.xml">RSS feed</a>.

  <div class="share-page">
<div class="share-links" style="text-align:center;">
  Spread the word:   
    
      <a class="fa fa-facebook" href="https://facebook.com/sharer.php?u=http://bair.berkeley.edu/blog/2018/12/14/sac/" rel="nofollow" target="_blank" title="Share on Facebook"></a>
    

    
      <a class="fa fa-twitter" href="https://twitter.com/intent/tweet?text=Soft%20Actor%20Critic%E2%80%94Deep%20Reinforcement%20Learning%20with%20Real-World%20Robots&amp;url=http://bair.berkeley.edu/blog/2018/12/14/sac/" rel="nofollow" target="_blank" title="Share on Twitter"></a>
    

    
      <a class="fa fa-google-plus" href="https://plus.google.com/share?url=http://bair.berkeley.edu/blog/2018/12/14/sac/" rel="nofollow" target="_blank" title="Share on Google+"></a>
    

    
      <a class="fa fa-linkedin" href="http://www.linkedin.com/shareArticle?url=http://bair.berkeley.edu/blog/2018/12/14/sac/&amp;title=Soft%20Actor%20Critic%E2%80%94Deep%20Reinforcement%20Learning%20with%20Real-World%20Robots" rel="nofollow" target="_blank" title="Share on LinkedIn"></a>
    

    

    

    
      <a class="fa fa-reddit" href="http://reddit.com/submit?url=http://bair.berkeley.edu/blog/2018/12/14/sac/&amp;title=Soft%20Actor%20Critic%E2%80%94Deep%20Reinforcement%20Learning%20with%20Real-World%20Robots" rel="nofollow" target="_blank" title="Share on Reddit"></a>
    

    

    
      <a class="fa fa-hacker-news" onclick="parent.postMessage(&#39;submit&#39;,&#39;*&#39;)" href="https://news.ycombinator.com/submitlink?u=http://bair.berkeley.edu/blog/2018/12/14/sac/&amp;t=Soft%20Actor%20Critic%E2%80%94Deep%20Reinforcement%20Learning%20with%20Real-World%20Robots" rel="nofollow" target="_blank" title="Share on Hacker News"></a>
    
  </div>
</div>


</div>




  <h1>Comments</h1>
  <div id="disqus_thread"><iframe id="dsq-app2130" name="dsq-app2130" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/saved_resource.html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 1621px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></div>
  <script type="text/javascript">
    var disqus_shortname  = 'bair-blog';
    var disqus_identifier = '/2018/12/14/sac';
    var disqus_title      = 'Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots';

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
  </div>
</footer>

  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-101338021-1', 'auto');
  ga('send', 'pageview');

</script>

  


<div id="weava-permanent-marker" date="1562634737114"></div><iframe style="display: none;" src="./Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog_files/saved_resource(1).html"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Size4, sans-serif;"></div></div><div id="weava-ui-wrapper"><div class="weava-drop-area-wrapper"><div class="weava-drop-area"></div>
<div class="weava-drop-area-text">Drop here!</div></div></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>