<!DOCTYPE html>
<!-- saved from url=(0092)https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199 -->
<html xmlns:cc="http://creativecommons.org/ns#" class="gr__medium_com"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium</title><link rel="canonical" href="https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199"><meta name="title" content="RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium"><meta name="referrer" content="always"><meta name="description" content="Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a high barrier in learning the concepts and the lingos. In this…"><meta name="theme-color" content="#000000"><meta property="og:title" content="RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium"><meta property="twitter:title" content="RL— Introduction to Deep Reinforcement Learning"><meta property="og:url" content="https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/0*JjsLueA2KATdLD5Z"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a…"><meta name="twitter:description" content="Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a…"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/0*JjsLueA2KATdLD5Z"><link rel="author" href="https://medium.com/@jonathan_hui"><meta name="author" content="Jonathan Hui"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="Jonathan Hui"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2018-10-14T20:10:00.410Z"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="20 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/35c25e04c199"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/35c25e04c199"><meta property="al:android:url" content="medium://p/35c25e04c199"><meta property="al:web:url" content="https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/35c25e04c199"><script async="" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/branch-latest.min.js.download"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":1428,"url":"https://cdn-images-1.medium.com/max/2400/0*JjsLueA2KATdLD5Z"},"url":"https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199","dateCreated":"2018-10-14T20:10:00.410Z","datePublished":"2018-10-14T20:10:00.410Z","dateModified":"2019-01-07T00:07:11.183Z","headline":"RL— Introduction to Deep Reinforcement Learning","name":"RL— Introduction to Deep Reinforcement Learning","articleId":"35c25e04c199","thumbnailUrl":"https://cdn-images-1.medium.com/max/2400/0*JjsLueA2KATdLD5Z","keywords":["Tag:Machine Learning","Tag:Artificial Intelligence","Tag:Data Science","Tag:Programming","Tag:Deep Learning","Topic:Machine Learning","Topic:Data Science","LockedPostSource:0","Elevated:false","LayerCake:3"],"author":{"@type":"Person","name":"Jonathan Hui","url":"https://medium.com/@jonathan_hui"},"creator":["Jonathan Hui"],"publisher":{"@type":"Organization","name":"Medium","url":"https://medium.com/","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/385/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199"}</script><meta name="parsely-link" content="https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/m2.css"><link rel="stylesheet" href="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/main-branding-base.DUpq82k2YI6OvEW6173IfA.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/analytics.js.download"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/190/190/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/150/150/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/95/95/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/75/75/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope="" class="postShowScreen browser-chrome os-windows is-withMagicUnderlines v-glyph v-glyph--m2 is-js" data-gr-c-s-loaded="true" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1562635685622" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="1519" height="674"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"><div class="paragraphControls js-paragraphControl js-paragraphControl-3a7c u-noUserSelect is-visible" style="top: 8468px;"><span class="paragraphControls-itemText"><button class="button button--chromeless" data-action="select-anchor" data-action-value="3a7c">Top highlight</button></span></div></div></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLightest js-metabar is-hiddenWhenMinimized metabar--affixed is-minimized"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="u-xs-show js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-xs-hide js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0"><span class="svgIcon svgIcon--logoWordmark svgIcon--112x22px u-xs-hide u-flex"><svg class="svgIcon-use" height="22" width="112" viewBox="0 0 111.5 22"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3c-1.5 0-2.6-1.2-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1c.6-2.1 2.5-3.5 4.6-3.4 2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></span><span class="svgIcon svgIcon--logoWordmark svgIcon--122x45px u-xs-show u-flex"><svg class="svgIcon-use" width="122" height="45"><path d="M61.6 31.806c0 .412 0 .505.28.758l1.574 1.537v.065h-6.979v-2.95a4.852 4.852 0 0 1-4.627 3.203c-3.588 0-6.192-2.81-6.192-7.981 0-4.843 2.81-8.075 6.754-8.075a4.122 4.122 0 0 1 4.056 2.51v-6.51a.806.806 0 0 0-.319-.787l-1.499-1.443v-.065l6.951-.815v20.553zm-5.125-.937v-9.714a2.614 2.614 0 0 0-2.08-.975c-1.695 0-3.334 1.537-3.334 6.099 0 4.271 1.414 5.78 3.175 5.78a2.81 2.81 0 0 0 2.24-1.19zm9.752 3.297V21.051a.88.88 0 0 0-.281-.786L64.4 18.672v-.065h6.98v13.302c0 .412 0 .505.28.758l1.536 1.443v.066l-6.97-.01zm-.253-20.356a2.81 2.81 0 1 1 5.62 0 2.81 2.81 0 0 1-5.62 0zm24.234 17.967c0 .413 0 .534.281.787l1.574 1.537v.065h-7.017v-3.363a5.077 5.077 0 0 1-4.805 3.616c-3.11 0-4.778-2.267-4.778-6.557 0-2.07 0-4.337.066-6.885a.796.796 0 0 0-.281-.76l-1.546-1.545v-.065h6.923v8.552c0 2.81.412 4.684 2.173 4.684a2.81 2.81 0 0 0 2.267-1.415v-9.367a.88.88 0 0 0-.28-.787l-1.556-1.602v-.065h6.979v13.17zm23.756 2.39c0-.507.094-6.952.094-8.432 0-2.81-.44-4.75-2.417-4.75a3.138 3.138 0 0 0-2.482 1.35c.198.876.292 1.772.28 2.67 0 1.948-.065 4.43-.093 6.913a.796.796 0 0 0 .281.759l1.574 1.442v.066h-7.045c0-.468.094-6.95.094-8.431 0-2.857-.44-4.75-2.389-4.75a2.81 2.81 0 0 0-2.323 1.387v9.555c0 .412 0 .506.281.759l1.537 1.442v.066h-6.97V21.098a.88.88 0 0 0-.281-.787l-1.546-1.639v-.065h6.98v3.334a5.002 5.002 0 0 1 5.002-3.587c2.323 0 3.896 1.292 4.562 3.747a5.433 5.433 0 0 1 5.245-3.747c3.11 0 4.872 2.295 4.872 6.632 0 2.07-.066 4.43-.094 6.913a.75.75 0 0 0 .318.759l1.537 1.443v.065h-7.017zm-87.671-2.043l2.07 1.977v.065H17.862v-.065l2.107-1.977a.796.796 0 0 0 .281-.759V18.728c0-.534 0-1.255.094-1.873l-7.082 17.564h-.084L5.843 18.26c-.16-.402-.206-.43-.31-.702v10.595c-.087.71.034 1.429.348 2.07l2.95 3.879v.065H1v-.065l2.95-3.888a3.69 3.69 0 0 0 .347-2.06v-11.71a2.267 2.267 0 0 0-.487-1.602l-2.089-2.708v-.065h7.494l6.277 13.686 5.527-13.686h7.335v.065l-2.061 2.296a.806.806 0 0 0-.319.786v16.15a.75.75 0 0 0 .319.759zm8.215-6.332v.065c0 4.01 2.07 5.62 4.497 5.62a5.105 5.105 0 0 0 4.777-2.894h.066c-.844 3.963-3.298 5.836-6.97 5.836-3.962 0-7.7-2.389-7.7-7.925 0-5.817 3.747-8.14 7.887-8.14 3.335 0 6.886 1.573 6.886 6.632v.806h-9.443zm0-.806h4.618v-.815c0-4.122-.852-5.218-2.136-5.218-1.555 0-2.5 1.64-2.5 6.033h.018z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><span class="svgIcon svgIcon--straightLine svgIcon--29px u-marginTop1 u-marginLeft10 u-xs-hide"><svg class="svgIcon-use" width="2" height="29"><path d="M1 29V1" stroke="#D5D5D5" stroke-width=".5" fill="none" stroke-linecap="round"></path></svg></span><a class="link link--noUnderline u-baseColor--link metabar-topic" href="https://medium.com/topic/machine-learning">Machine Learning</a></div><div class="metabar-block u-flex0"><div class="buttonSet buttonSet--wide"><label class="button button--small button--chromeless button--withIcon button--withSvgIcon inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--buttonNormal u-baseColor--placeholderNormal" title="Search Medium"><span class="svgIcon svgIcon--search svgIcon--25px u-baseColor--iconLight"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent" type="search" placeholder="Search Medium" required="true"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless u-xs-top1" href="https://medium.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a><button class="button button--small button--chromeless is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--activity js-notificationsButton u-marginRight16 u-xs-marginRight10 u-lineHeight0 u-size25x25" title="Notifications" aria-label="Notifications" data-action="open-notifications"><span class="svgIcon svgIcon--bell svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-293 409 25 25"><path d="M-273.327 423.67l-1.673-1.52v-3.646a5.5 5.5 0 0 0-6.04-5.474c-2.86.273-4.96 2.838-4.96 5.71v3.41l-1.68 1.553c-.204.19-.32.456-.32.734V427a1 1 0 0 0 1 1h3.49a3.079 3.079 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59c0-.28-.12-.55-.327-.74zm-7.173 5.63c-.842 0-1.55-.546-1.812-1.3h3.624a1.92 1.92 0 0 1-1.812 1.3zm6.35-2.45h-12.7v-2.347l1.63-1.51c.236-.216.37-.522.37-.843v-3.41c0-2.35 1.72-4.356 3.92-4.565a4.353 4.353 0 0 1 4.78 4.33v3.645c0 .324.137.633.376.85l1.624 1.477v2.373z"></path></svg></span></button><a class="button button--small button--upsellNav button--withChrome u-baseColor--buttonNormal u-xs-hide js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---nav_full" data-disable-client-nav="true" data-scroll="native">Upgrade</a><button class="button button--chromeless u-baseColor--buttonNormal is-inSiteNavBar js-userActions" aria-haspopup="true" data-action="open-userActions"><div class="avatar"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/0_toB60eKEH3klSlAD.jpg" class="avatar-image avatar-image--icon" alt="Vivek Agrawal"></div></button></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full" lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls" data-post-id="35c25e04c199" data-source="post_page" data-tracking-context="postPage" data-scroll="native"><section name="7278" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="c858" id="c858" class="graf graf--h3 graf--leading graf--title">RL— Introduction to Deep Reinforcement Learning</h1><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@jonathan_hui?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_c3Z3aOPBooxEX4tx4RkzLw.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Jonathan Hui"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a><span class="followState js-followState" data-user-id="bd51f1a63813"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="toggle-block-user" data-action-value="bd51f1a63813" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="toggle-subscribe-user" data-action-value="bd51f1a63813" data-action-source="post_header_lockup-bd51f1a63813-------------------------follow_byline" data-subscribe-source="post_header_lockup" data-follow-context-entity-id="35c25e04c199"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2018-10-14T20:10:00.410Z">Oct 15, 2018</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="20 min read"></span></div></div></div><figure name="149c" id="149c" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 521px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*JjsLueA2KATdLD5Z" data-width="2048" data-height="1524" data-is-featured="true" data-action="zoom" data-action-value="0*JjsLueA2KATdLD5Z" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/0_JjsLueA2KATdLD5Z" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="55"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/0*JjsLueA2KATdLD5Z" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/0_JjsLueA2KATdLD5Z(1)"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/0*JjsLueA2KATdLD5Z"></noscript></div></div><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@fossy?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@fossy?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator nofollow noopener" target="_blank">Fab&nbsp;Lentz</a></figcaption></figure><p name="b093" id="b093" class="graf graf--p graf-after--figure">Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning <strong class="markup--strong markup--p-strong">RL</strong> has a high barrier in learning the concepts and the lingos. In this article, we will cover deep RL with an overview of the general landscape. Yet, we will not shy away from equations and lingos. They provide the basics in understanding the concepts deeper. We will not appeal to you that it only takes 20 lines of code to tackle an RL problem. The official answer should be one! But we will try hard to make it approachable.</p><p name="8ae3" id="8ae3" class="graf graf--p graf-after--p">In most AI topics, we create mathematical frameworks to tackle problems. For RL, the answer is the <strong class="markup--strong markup--p-strong">Markov Decision Process (MDP)</strong>. It sounds complicated but it produces an easy framework to model a complex problem. An <strong class="markup--strong markup--p-strong">agent</strong> (e.g. a human) observes the environment and takes <strong class="markup--strong markup--p-strong">actions</strong>. <strong class="markup--strong markup--p-strong">Rewards</strong> are given out but they may be infrequent and delayed. Very often, the long-delayed rewards make it extremely hard to untangle the information and traceback what sequence of actions contributed to the rewards.</p><p name="b7cf" id="b7cf" class="graf graf--p graf-after--p">Markov decision process (MDP) composes of:</p><figure name="a662" id="a662" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 32px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.6%;"></div><img class="graf-image" data-image-id="1*sMZBhROM7Wt4lqHUc6VRqw.jpeg" data-width="1600" data-height="73" data-action="zoom" data-action-value="1*sMZBhROM7Wt4lqHUc6VRqw.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_sMZBhROM7Wt4lqHUc6VRqw.jpeg"></div></figure><figure name="a0e1" id="a0e1" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 292px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*X6h1TXf4QidZbu2NfVl1wg.png" data-width="1680" data-height="700" data-action="zoom" data-action-value="1*X6h1TXf4QidZbu2NfVl1wg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_X6h1TXf4QidZbu2NfVl1wg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="29"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*X6h1TXf4QidZbu2NfVl1wg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_X6h1TXf4QidZbu2NfVl1wg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*X6h1TXf4QidZbu2NfVl1wg.png"></noscript></div></div><figcaption class="imageCaption">Source:<a href="https://drive.google.com/file/d/0BxXI_RttTZAhVXBlMUVkQ1BVVDQ/view" data-href="https://drive.google.com/file/d/0BxXI_RttTZAhVXBlMUVkQ1BVVDQ/view" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank"> left,</a>&nbsp;<a href="https://mitpress.mit.edu/books/reinforcement-learning" data-href="https://mitpress.mit.edu/books/reinforcement-learning" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">right</a></figcaption></figure><p name="d0c4" id="d0c4" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">State</strong> in MDP can be represented as raw images.</p><figure name="4487" id="4487" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 189px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*aosA682yS21d8lhhgL9AHA.jpeg" data-width="2911" data-height="788" data-action="zoom" data-action-value="1*aosA682yS21d8lhhgL9AHA.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_aosA682yS21d8lhhgL9AHA.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="19"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*aosA682yS21d8lhhgL9AHA.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_aosA682yS21d8lhhgL9AHA(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*aosA682yS21d8lhhgL9AHA.jpeg"></noscript></div></div><figcaption class="imageCaption">AlphaGO &amp; Atari&nbsp;Seaquest</figcaption></figure><p name="5e4d" id="5e4d" class="graf graf--p graf-after--figure">Or for robotic controls, we use sensors to measure the joint angles, velocity, and the end-effector pose:</p><figure name="115a" id="115a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 174px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*8snUNu8FgaAs6UMm-o69og.jpeg" data-width="2000" data-height="497" data-action="zoom" data-action-value="1*8snUNu8FgaAs6UMm-o69og.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_8snUNu8FgaAs6UMm-o69og.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="17"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*8snUNu8FgaAs6UMm-o69og.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_8snUNu8FgaAs6UMm-o69og(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*8snUNu8FgaAs6UMm-o69og.jpeg"></noscript></div></div></figure><ul class="postList"><li name="9e62" id="9e62" class="graf graf--li graf-after--figure">An <strong class="markup--strong markup--li-strong">action</strong> can be a move in a chess game or moving a robotic arm or a joystick.</li><li name="be5e" id="be5e" class="graf graf--li graf-after--li">For a GO game, the reward is very sparse: 1 if we win or -1 if we lose. Sometimes, we get rewards more frequently. In the Atari Seaquest game, we score whenever we hit the sharks.</li><li name="90bc" id="90bc" class="graf graf--li graf-after--li">The discount factor discounts future rewards if it is smaller than one. Money earned in the future often has a smaller current value, and we may need it for a purely technical reason to converge the solution better.</li></ul><figure name="ce04" id="ce04" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 54px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 7.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*qR9uqk2c8LNB8LGQ6sU-qg.png" data-width="1500" data-height="116" data-action="zoom" data-action-value="1*qR9uqk2c8LNB8LGQ6sU-qg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_qR9uqk2c8LNB8LGQ6sU-qg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="3"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*qR9uqk2c8LNB8LGQ6sU-qg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_qR9uqk2c8LNB8LGQ6sU-qg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*qR9uqk2c8LNB8LGQ6sU-qg.png"></noscript></div></div></figure><ul class="postList"><li name="f283" id="f283" class="graf graf--li graf-after--figure">We can rollout actions forever or limit the experience to <em class="markup--em markup--li-em">N</em> time steps. This is called the horizon.</li></ul><p name="3537" id="3537" class="graf graf--p graf-after--li">The transition function is the system dynamics. It predicts the next state after taking action. It is called the <strong class="markup--strong markup--p-strong">model </strong>which plays a major role when we discuss Model-based RL later.</p><p name="bcd2" id="bcd2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Convention</strong></p><p name="e8d0" id="e8d0" class="graf graf--p graf-after--p">The concepts in RL come from many research fields including the control theory. Different notations may be used in a different context. Let’s get this out first before any confusion. The state can be written as <em class="markup--em markup--p-em">s</em> or <em class="markup--em markup--p-em">x</em>, and action as <em class="markup--em markup--p-em">a</em> or <em class="markup--em markup--p-em">u</em>. An <strong class="markup--strong markup--p-strong">action</strong> is the same as a <strong class="markup--strong markup--p-strong">control</strong>. We can maximize the rewards or minimizes the costs which are simply the negative of each other. Notations can be in upper or lower case.</p><figure name="8ddf" id="8ddf" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 119px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.900000000000002%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*SJXL97vgexqJkH6PT-VQQw.png" data-width="1600" data-height="271" data-action="zoom" data-action-value="1*SJXL97vgexqJkH6PT-VQQw.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_SJXL97vgexqJkH6PT-VQQw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="11"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*SJXL97vgexqJkH6PT-VQQw.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_SJXL97vgexqJkH6PT-VQQw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*SJXL97vgexqJkH6PT-VQQw.png"></noscript></div></div></figure><h3 name="1b9e" id="1b9e" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Policy</strong></h3><p name="b952" id="b952" class="graf graf--p graf-after--h3">In RL, our focus is finding an optimal <strong class="markup--strong markup--p-strong">policy. </strong>A<strong class="markup--strong markup--p-strong"> </strong>policy tells us how to act from a particular state.</p><figure name="7e8e" id="7e8e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 28px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 3.9%;"></div><img class="graf-image" data-image-id="1*3qVw9kj0Unbh2SjScCt8wg.png" data-width="1400" data-height="55" data-action="zoom" data-action-value="1*3qVw9kj0Unbh2SjScCt8wg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_3qVw9kj0Unbh2SjScCt8wg.png"></div></figure><p name="326c" id="326c" class="graf graf--p graf-after--figure">Like the weights in Deep Learning methods, this policy can be parameterized by <em class="markup--em markup--p-em">θ,</em></p><figure name="e118" id="e118" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 23.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*XmXQfyvadrUd73ZOU1o95g.jpeg" data-width="1392" data-height="327" data-action="zoom" data-action-value="1*XmXQfyvadrUd73ZOU1o95g.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_XmXQfyvadrUd73ZOU1o95g.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="15"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*XmXQfyvadrUd73ZOU1o95g.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_XmXQfyvadrUd73ZOU1o95g(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*XmXQfyvadrUd73ZOU1o95g.jpeg"></noscript></div></div></figure><p name="53de" id="53de" class="graf graf--p graf-after--figure">and we want to find a policy that makes the most rewarding decisions:</p><figure name="fea8" id="fea8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 120px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*7Yt8b1k0POIGR_cnqRvqPw.jpeg" data-width="1424" data-height="244" data-action="zoom" data-action-value="1*7Yt8b1k0POIGR_cnqRvqPw.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_7Yt8b1k0POIGR_cnqRvqPw.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="11"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*7Yt8b1k0POIGR_cnqRvqPw.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_7Yt8b1k0POIGR_cnqRvqPw(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*7Yt8b1k0POIGR_cnqRvqPw.jpeg"></noscript></div></div></figure><p name="43f0" id="43f0" class="graf graf--p graf-after--figure">In real life, nothing is absolute. So our policy can be deterministic or stochastic. For stochastic, the policy outputs a probability distribution instead.</p><figure name="0d02" id="0d02" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 40px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 5.7%;"></div><img class="graf-image" data-image-id="1*fPfloA0G8zrz89OqpD2oJA.png" data-width="1500" data-height="86" data-action="zoom" data-action-value="1*fPfloA0G8zrz89OqpD2oJA.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_fPfloA0G8zrz89OqpD2oJA.png"></div></figure><figure name="5fa9" id="5fa9" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 134px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*37PNsTcH0U5fe1UaCx810A.png" data-width="2400" data-height="458" data-action="zoom" data-action-value="1*37PNsTcH0U5fe1UaCx810A.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_37PNsTcH0U5fe1UaCx810A.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="13"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*37PNsTcH0U5fe1UaCx810A.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_37PNsTcH0U5fe1UaCx810A(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*37PNsTcH0U5fe1UaCx810A.png"></noscript></div></div></figure><p name="cc38" id="cc38" class="graf graf--p graf-after--figure">Finally, let’s put our objective together. In RL, we want to find a sequence of actions that maximize expected rewards or minimize cost.</p><figure name="a21f" id="a21f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 195px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.900000000000002%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*P4alpFJHhNtbbbXGK1UAUQ.jpeg" data-width="1424" data-height="397" data-action="zoom" data-action-value="1*P4alpFJHhNtbbbXGK1UAUQ.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_P4alpFJHhNtbbbXGK1UAUQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="19"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*P4alpFJHhNtbbbXGK1UAUQ.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_P4alpFJHhNtbbbXGK1UAUQ(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*P4alpFJHhNtbbbXGK1UAUQ.jpeg"></noscript></div></div></figure><p name="818d" id="818d" class="graf graf--p graf-after--figure">But there are many ways to solve the problem. For example, we can</p><ul class="postList"><li name="10bb" id="10bb" class="graf graf--li graf-after--p">Analyze how good to reach a certain state or take a specific action (i.e. Value-learning),</li><li name="3140" id="3140" class="graf graf--li graf-after--li">Use the model to find actions that have the maximum rewards (model-based learning), or</li><li name="0c26" id="0c26" class="graf graf--li graf-after--li">Derive a policy directly to maximize rewards (policy gradient).</li></ul><p name="a753" id="a753" class="graf graf--p graf-after--li">We will go through all these approaches shortly.</p><h3 name="87aa" id="87aa" class="graf graf--h3 graf-after--p">Notation</h3><p name="6513" id="6513" class="graf graf--p graf-after--h3">But, in case you want further elaboration for the terms and notation in RL first, this table should help.</p><figure name="5a88" id="5a88" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 430px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*uC5gslmql53IxSRSS1oBsg.jpeg" data-width="1600" data-height="982" data-action="zoom" data-action-value="1*uC5gslmql53IxSRSS1oBsg.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_uC5gslmql53IxSRSS1oBsg.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="45"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*uC5gslmql53IxSRSS1oBsg.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_uC5gslmql53IxSRSS1oBsg(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*uC5gslmql53IxSRSS1oBsg.jpeg"></noscript></div></div><figcaption class="imageCaption">Modified from&nbsp;<a href="https://arxiv.org/pdf/1504.00702.pdf" data-href="https://arxiv.org/pdf/1504.00702.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">source</a></figcaption></figure><h3 name="827c" id="827c" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Model-based RL</strong></h3><p name="ad15" id="ad15" class="graf graf--p graf-after--h3">Intuitively, if we know the rule of the game and how much it costs for every move, we can find the actions that minimize the cost. The <strong class="markup--strong markup--p-strong">model</strong> <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">p </em></strong>(the system dynamics) predicts the next state after taking an action. Mathematically, it is formulated as a probability distribution. In this article, the model can be written as <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">p</em></strong> or <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">f</em></strong>.</p><figure name="6557" id="6557" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 27px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 3.9%;"></div><img class="graf-image" data-image-id="1*J2s5B7bu63hBAmhC-CbWlw.jpeg" data-width="1350" data-height="52" data-action="zoom" data-action-value="1*J2s5B7bu63hBAmhC-CbWlw.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_J2s5B7bu63hBAmhC-CbWlw.jpeg"></div></figure><p name="9aae" id="9aae" class="graf graf--p graf-after--figure">Let’s demonstrate the idea of a model with a cart-pole example. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">p</em></strong> models the angle of the pole after taking action.</p><figure name="9cdd" id="9cdd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 215px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*Z9g2-AyfQbFHZMFDfwbnaQ.png" data-width="1600" data-height="491" data-action="zoom" data-action-value="1*Z9g2-AyfQbFHZMFDfwbnaQ.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_Z9g2-AyfQbFHZMFDfwbnaQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="21"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*Z9g2-AyfQbFHZMFDfwbnaQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*Z9g2-AyfQbFHZMFDfwbnaQ.png"></noscript></div></div></figure><figure name="d49e" id="d49e" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 66px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 9.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*wJ3vb0rN0ZmnkZuOQpUtaA.jpeg" data-width="1600" data-height="150" data-action="zoom" data-action-value="1*wJ3vb0rN0ZmnkZuOQpUtaA.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_wJ3vb0rN0ZmnkZuOQpUtaA.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="5"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*wJ3vb0rN0ZmnkZuOQpUtaA.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*wJ3vb0rN0ZmnkZuOQpUtaA.jpeg"></noscript></div></div></figure><p name="3d80" id="3d80" class="graf graf--p graf-after--figure">Here is the probability distribution output for <em class="markup--em markup--p-em">θ </em>in the next time step for the example above.</p><figure name="980d" id="980d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 206px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*32zFcyhR5eqDClXDbSplJQ.png" data-width="2200" data-height="648" data-action="zoom" data-action-value="1*32zFcyhR5eqDClXDbSplJQ.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_32zFcyhR5eqDClXDbSplJQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="21"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*32zFcyhR5eqDClXDbSplJQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*32zFcyhR5eqDClXDbSplJQ.png"></noscript></div></div></figure><p name="6140" id="6140" class="graf graf--p graf-after--figure">This model describes the law of Physics. But a model can be just the rule of a chess game. The core idea of <strong class="markup--strong markup--p-strong">Model-based RL </strong>is<strong class="markup--strong markup--p-strong"> </strong>using the model and the cost function to locate the optimal path of actions (to be exact — a trajectory of states and actions).</p><figure name="1d63" id="1d63" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 32px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.6%;"></div><img class="graf-image" data-image-id="1*uUKlBZW8EMS4L8jlVMDaWQ.jpeg" data-width="1450" data-height="66" data-action="zoom" data-action-value="1*uUKlBZW8EMS4L8jlVMDaWQ.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_uUKlBZW8EMS4L8jlVMDaWQ.jpeg"></div></figure><figure name="c8f0" id="c8f0" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*oWcm1sR3mZw_iSmiQDUr-Q.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_oWcm1sR3mZw_iSmiQDUr-Q.jpeg"></div></figure><p name="3d7a" id="3d7a" class="graf graf--p graf-after--figure">Let’s get into another example. In the GO game, the model is the rule of the game. According to this rule, we search the possible moves and find the actions to win the game. Of course, the search space is too large and we need to search smarter.</p><figure name="d372" id="d372" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*U_9cl3HV5o_8DUC7gmsDXg.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_U_9cl3HV5o_8DUC7gmsDXg.jpeg"></div><figcaption class="imageCaption">AlphaGO</figcaption></figure><blockquote name="3a7c" id="3a7c" class="graf graf--blockquote graf-after--figure"><span class="markup--quote markup--blockquote-quote is-other" name="anon_de87834a0bdf" data-creator-ids="anon">In model-based RL, we use the model and cost function to find <strong class="markup--strong markup--blockquote-strong">an optimal trajectory</strong> of states and actions (<strong class="markup--strong markup--blockquote-strong">optimal control</strong>).</span></blockquote><p name="cfa4" id="cfa4" class="graf graf--p graf-after--blockquote">Sometimes, we may not know the models. But this does not exclude us from learning them. Indeed, we can use deep learning to model complex motions from sample trajectories or approximate them locally.</p><p name="a791" id="a791" class="graf graf--p graf-after--p">The video below is a nice demonstration of performing tasks by a robot using Model-based RL. Instead of programming the robot arm directly, the robot is trained for 20 minutes to learn each task, mostly by itself. Once it is done, the robot should handle situations that have not trained before. We can move around the objects or change the grasp of the hammer, the robot should manage to complete the task successfully.</p><figure name="52be" id="52be" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="56"></canvas><div class="iframeContainer"><iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/7e8b53ed7b0e11d6e928fc9c3f6a204f?postId=35c25e04c199" data-media-id="7e8b53ed7b0e11d6e928fc9c3f6a204f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FQ4bMcUk6pcw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/saved_resource.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/7e8b53ed7b0e11d6e928fc9c3f6a204f?postId=35c25e04c199" data-media-id="7e8b53ed7b0e11d6e928fc9c3f6a204f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FQ4bMcUk6pcw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="79ca" id="79ca" class="graf graf--p graf-after--figure">The tasks sound pretty simple. But they are not easy to solve. We often make approximations to make it easier. For example, we approximate the system dynamics to be linear and the cost function to be a quadratic equation.</p><figure name="cbba" id="cbba" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 55px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 7.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*bRIOfdX3kSdX1FtPD8ChsA.jpeg" data-width="1680" data-height="132" data-action="zoom" data-action-value="1*bRIOfdX3kSdX1FtPD8ChsA.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_bRIOfdX3kSdX1FtPD8ChsA.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="3"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*bRIOfdX3kSdX1FtPD8ChsA.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*bRIOfdX3kSdX1FtPD8ChsA.jpeg"></noscript></div></div></figure><figure name="3271" id="3271" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 60px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*tq4EeBxSl4XmqAQnsw2d8g.jpeg" data-width="1600" data-height="136" data-action="zoom" data-action-value="1*tq4EeBxSl4XmqAQnsw2d8g.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_tq4EeBxSl4XmqAQnsw2d8g.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="5"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*tq4EeBxSl4XmqAQnsw2d8g.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*tq4EeBxSl4XmqAQnsw2d8g.jpeg"></noscript></div></div></figure><p name="9c7b" id="9c7b" class="graf graf--p graf-after--figure">Then we find the actions that minimize the cost while obeying the model.</p><figure name="c8b1" id="c8b1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 60px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*fJXD75kQup0hYRYBQUy17g.jpeg" data-width="1390" data-height="120" data-action="zoom" data-action-value="1*fJXD75kQup0hYRYBQUy17g.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_fJXD75kQup0hYRYBQUy17g.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="5"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*fJXD75kQup0hYRYBQUy17g.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*fJXD75kQup0hYRYBQUy17g.jpeg"></noscript></div></div></figure><p name="3a0b" id="3a0b" class="graf graf--p graf-after--figure">There are known optimization methods like LQR to solve this kind of objective. To move to non-linear system dynamics, we can apply iLQR which use LQR iteratively to find the optimal solution similar to Newton’s optimization. All these methods are complex and computationally intense. You can find the details in <a href="https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750" data-href="https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750" class="markup--anchor markup--p-anchor" target="_blank">here</a>. But for our context, we just need to know that given a cost function and a model, we can find the corresponding optimal actions.</p><p name="e74d" id="e74d" class="graf graf--p graf-after--p">Model-based RL has a strong competitive edge over other RL methods because it is sample efficiency. Many models can be approximated locally with fewer samples and trajectory planning requires no further samples. If physical simulation takes time, the saving is significant. Therefore, it is popular in robotic control. With other RL methods, the same training may take weeks.</p><p name="f73a" id="f73a" class="graf graf--p graf-after--p">Let’s detail the process a little bit more. The following is the MPC (Model Predictive Control) which run a random or an educated policy to explore space to fit the model. Then, in step 3, we use iLQR to plan the optimal controls. But we only execute the first action in the plan. We observe the state again and replan the trajectory. This allows us to take corrective actions if needed.</p><figure name="12f0" id="12f0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 181px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*RSCF3Zd9Qykxg6V1w5ZRAQ.png" data-width="1600" data-height="413" data-action="zoom" data-action-value="1*RSCF3Zd9Qykxg6V1w5ZRAQ.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_RSCF3Zd9Qykxg6V1w5ZRAQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="17"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*RSCF3Zd9Qykxg6V1w5ZRAQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*RSCF3Zd9Qykxg6V1w5ZRAQ.png"></noscript></div></div><figcaption class="imageCaption"><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf" data-href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener nofollow noopener" target="_blank">Source</a></figcaption></figure><p name="039c" id="039c" class="graf graf--p graf-after--figure">The following figure summarizes the flow. We observe the environments and extract the states. We fit the model and use a trajectory optimization method to plan our path which composes of actions required at each time step.</p><figure name="865d" id="865d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 677px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 96.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*sQG2HXMcoU0_1TZcHAdNaA.jpeg" data-width="1312" data-height="1268" data-action="zoom" data-action-value="1*sQG2HXMcoU0_1TZcHAdNaA.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_sQG2HXMcoU0_1TZcHAdNaA.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="71"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*sQG2HXMcoU0_1TZcHAdNaA.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*sQG2HXMcoU0_1TZcHAdNaA.jpeg"></noscript></div></div></figure><h3 name="96b0" id="96b0" class="graf graf--h3 graf-after--figure">Value learning</h3><p name="1fbd" id="1fbd" class="graf graf--p graf-after--h3">Next, we go to another major RL method called Value Learning. In playing a GO game, it is very hard to plan the next winning move even the rule of the game is well understood. The exponential growth of possibilities makes it too hard to be solved. When the GO champions play the GO game, they evaluate how good a move is and how good to reach a certain board position. Assume we have a cheat sheet scoring every state:</p><figure name="cb9a" id="cb9a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 33px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.7%;"></div><img class="graf-image" data-image-id="1*9MCyxA48Ibc2eLv4VkN0HQ.png" data-width="1400" data-height="66" data-action="zoom" data-action-value="1*9MCyxA48Ibc2eLv4VkN0HQ.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_9MCyxA48Ibc2eLv4VkN0HQ.png"></div></figure><p name="350b" id="350b" class="graf graf--p graf-after--figure">We can simply look at the cheat sheet and find what is the next most rewarding state and take the corresponding action.</p><p name="25bd" id="25bd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Value function</strong> <em class="markup--em markup--p-em">V(s)</em> measures the expected discounted rewards for a state under a policy. Intuitively, it measures the total rewards that you get from a particular state following a specific policy.</p><figure name="7324" id="7324" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 33px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.7%;"></div><img class="graf-image" data-image-id="1*mEkv4hfGxRwv6kHGkMgQ5A.png" data-width="1400" data-height="66" data-action="zoom" data-action-value="1*mEkv4hfGxRwv6kHGkMgQ5A.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_mEkv4hfGxRwv6kHGkMgQ5A.png"></div></figure><p name="294e" id="294e" class="graf graf--p graf-after--figure">In our cart-pole example, we can use the pole stay-up time to measure the rewards. Below, there is a better chance to maintain the pole upright for the state <em class="markup--em markup--p-em">s1 </em>than<em class="markup--em markup--p-em"> s2 </em>(better to be in the position on the left below than the right). For most policies, the state on the left is likely to have a higher value function.</p><figure name="ebc0" id="ebc0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 253px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*QsLkYHRrppgaqNiuI5GB-g.png" data-width="1600" data-height="578" data-action="zoom" data-action-value="1*QsLkYHRrppgaqNiuI5GB-g.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_QsLkYHRrppgaqNiuI5GB-g.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="25"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*QsLkYHRrppgaqNiuI5GB-g.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*QsLkYHRrppgaqNiuI5GB-g.png"></noscript></div></div></figure><p name="4f68" id="4f68" class="graf graf--p graf-after--figure">So how to find out <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">V</em></strong>? One method is the <strong class="markup--strong markup--p-strong">Monte Carlo</strong> method. We run the policy and play out the whole episode until the end to observe the total rewards. For example, we time how long the pole stays up. Then we have multiple Monte Carlo rollouts and we average the results for <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">V</em></strong>.</p><p name="30b6" id="30b6" class="graf graf--p graf-after--p">There are a few ways to find the corresponding optimal policy. In <strong class="markup--strong markup--p-strong">policy evaluation</strong>, we can start with a random policy and evaluate how good each state is. After many iterations, we use <em class="markup--em markup--p-em">V(s)</em> to decide the next best state. Then, we use the model to determine the action that leads us there. For the GO game, this is simple since the rule of the game is known.</p><figure name="2c21" id="2c21" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 338px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.199999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*RIBB9aho7mZZJYw31oVCTQ.jpeg" data-width="1700" data-height="820" data-action="zoom" data-action-value="1*RIBB9aho7mZZJYw31oVCTQ.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_RIBB9aho7mZZJYw31oVCTQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="35"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*RIBB9aho7mZZJYw31oVCTQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*RIBB9aho7mZZJYw31oVCTQ.jpeg"></noscript></div></div></figure><p name="63d2" id="63d2" class="graf graf--p graf-after--figure">Alternatively, after each policy evaluation, we improve the policy based on the value function. We continue the evaluation and refinement. Eventually, we will reach the optimal policy. This is called <strong class="markup--strong markup--p-strong">policy iteration</strong>.</p><figure name="f0d8" id="f0d8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*hdJrrav0zRMjzLsiMRFAjA.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_hdJrrav0zRMjzLsiMRFAjA.jpeg"></div><figcaption class="imageCaption">Modified from&nbsp;<a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" data-href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener noopener noopener noopener noopener noopener nofollow noopener" target="_blank">source</a></figcaption></figure><p name="6e4c" id="6e4c" class="graf graf--p graf-after--figure">But there is a problem if we do not have the model. We do not know what action can take us to the target state.</p><figure name="f16a" id="f16a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 202px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*RseWZkNGpAdPsoaKzNxeFQ.png" data-width="1591" data-height="460" data-action="zoom" data-action-value="1*RseWZkNGpAdPsoaKzNxeFQ.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_RseWZkNGpAdPsoaKzNxeFQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="19"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*RseWZkNGpAdPsoaKzNxeFQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*RseWZkNGpAdPsoaKzNxeFQ.png"></noscript></div></div></figure><p name="fd8e" id="fd8e" class="graf graf--p graf-after--figure">Value function is not a model-free method. We need a model to make decisions. But as an important footnote, even when the model is unknown, value function is still helpful in complementing other RL methods that do not need a model.</p><p name="1a72" id="1a72" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Value iteration with Dynamic programming</strong></p><p name="914e" id="914e" class="graf graf--p graf-after--p">Other than the Monte Carlo method, we can use dynamic programming to compute <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">V</em></strong>. We take an action, observe the reward and compute it with the <em class="markup--em markup--p-em">V</em>-value of the next state:</p><figure name="2209" id="2209" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 44px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 6.3%;"></div><img class="graf-image" data-image-id="1*YjQoGPeaKHvLctl3WgCHqg.png" data-width="1200" data-height="75" data-action="zoom" data-action-value="1*YjQoGPeaKHvLctl3WgCHqg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_YjQoGPeaKHvLctl3WgCHqg.png"></div></figure><p name="f61b" id="f61b" class="graf graf--p graf-after--figure">The exact formula should be:</p><figure name="4e5e" id="4e5e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 55px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 7.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*_P5kRg0K5nx0NHw5OzKRkg.png" data-width="1700" data-height="134" data-action="zoom" data-action-value="1*_P5kRg0K5nx0NHw5OzKRkg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1__P5kRg0K5nx0NHw5OzKRkg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="3"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*_P5kRg0K5nx0NHw5OzKRkg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*_P5kRg0K5nx0NHw5OzKRkg.png"></noscript></div></div></figure><p name="c042" id="c042" class="graf graf--p graf-after--figure">If the model is unknown, we compute <em class="markup--em markup--p-em">V</em> by sampling. We execute the action and observe the reward and the next state instead.</p><h3 name="4cc3" id="4cc3" class="graf graf--h3 graf-after--p">Function fitting</h3><p name="2417" id="2417" class="graf graf--p graf-after--h3">However, maintain <em class="markup--em markup--p-em">V</em> for every state is not feasible for many problems. To solve that, we use supervised learning to train a deep network that approximates <em class="markup--em markup--p-em">V</em>.</p><figure name="83d9" id="83d9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*e5F37FBP-x-65z61jtJiXg.jpeg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_e5F37FBP-x-65z61jtJiXg.jpeg"></div></figure><p name="86b6" id="86b6" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">y</em></strong> above is the target value and we can use the Monte Carlo method to compute it.</p><figure name="d68d" id="d68d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 70px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*2CHTa8r4m5kFQ-Cpbz0O0A.png" data-width="1600" data-height="160" data-action="zoom" data-action-value="1*2CHTa8r4m5kFQ-Cpbz0O0A.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_2CHTa8r4m5kFQ-Cpbz0O0A.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="5"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*2CHTa8r4m5kFQ-Cpbz0O0A.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*2CHTa8r4m5kFQ-Cpbz0O0A.png"></noscript></div></div></figure><p name="a44f" id="a44f" class="graf graf--p graf-after--figure">Otherwise, we can apply the dynamic programming concept and use a one-step lookahead. This is called <strong class="markup--strong markup--p-strong">Temporal Difference</strong> TD.</p><figure name="0b8b" id="0b8b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 44px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 6.3%;"></div><img class="graf-image" data-image-id="1*YjQoGPeaKHvLctl3WgCHqg.png" data-width="1200" data-height="75" data-action="zoom" data-action-value="1*YjQoGPeaKHvLctl3WgCHqg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_YjQoGPeaKHvLctl3WgCHqg.png"></div></figure><p name="38d6" id="38d6" class="graf graf--p graf-after--figure">We take a single action and use the observed reward and the <em class="markup--em markup--p-em">V</em> value for the next state to compute <em class="markup--em markup--p-em">V(s)</em>.</p><figure name="51b4" id="51b4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 277px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*PYlL_2_2VGE2bl4I8SloQg.png" data-width="1600" data-height="634" data-action="zoom" data-action-value="1*PYlL_2_2VGE2bl4I8SloQg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_PYlL_2_2VGE2bl4I8SloQg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="29"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*PYlL_2_2VGE2bl4I8SloQg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*PYlL_2_2VGE2bl4I8SloQg.png"></noscript></div></div></figure><p name="8274" id="8274" class="graf graf--p graf-after--figure">The Monte Carlo method is accurate. But for a stochastic policy or a stochastic model, every run may have different results. So the variance is high. TD considers far fewer actions to update its value. So the variance is low. But at least in early training, the bias is very high. High bias gives wrong results but high variance makes the model very hard to converge. In practice, we can combine the Monte Carlo and TD with different <em class="markup--em markup--p-em">k</em>-step lookahead to form the target. This balances the bias and the variance which can stabilize the training.</p><h3 name="16cb" id="16cb" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Action-value function</strong></h3><p name="b495" id="b495" class="graf graf--p graf-after--h3">So can we use the value learning concept without a model? Yes, we can avoid the model by scoring an action instead of a state. Action-value function <em class="markup--em markup--p-em">Q(s, a)</em> measures the expected discounted rewards of taking an action. The tradeoff is we have more data to track. For each state, if we can take <em class="markup--em markup--p-em">k</em> actions, there will be <em class="markup--em markup--p-em">k Q</em>-values.</p><figure name="19a4" id="19a4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 252px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*k5JZbCNNJC4RFdtpj_wgFg.png" data-width="1486" data-height="536" data-action="zoom" data-action-value="1*k5JZbCNNJC4RFdtpj_wgFg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_k5JZbCNNJC4RFdtpj_wgFg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="25"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*k5JZbCNNJC4RFdtpj_wgFg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*k5JZbCNNJC4RFdtpj_wgFg.png"></noscript></div></div></figure><p name="98e2" id="98e2" class="graf graf--p graf-after--figure">For optimal result, we take the action with the highest <em class="markup--em markup--p-em">Q</em>-value.</p><figure name="3441" id="3441" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 5.6000000000000005%;"></div><img class="graf-image" data-image-id="1*iRUppfcGcHpa9W_pcD2uAw.png" data-width="1200" data-height="67" data-action="zoom" data-action-value="1*iRUppfcGcHpa9W_pcD2uAw.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_iRUppfcGcHpa9W_pcD2uAw.png"></div></figure><p name="81a0" id="81a0" class="graf graf--p graf-after--figure">As shown, we do not need a model to find the optimal action. Hence, Action-value learning is model-free. Which acton below has a higher Q-value? Intuitively, moving left at the state below should have a higher value than moving right.</p><figure name="6ad9" id="6ad9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 241px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*TuPcSwlIIelgv--DB6DKGg.png" data-width="1600" data-height="550" data-action="zoom" data-action-value="1*TuPcSwlIIelgv--DB6DKGg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_TuPcSwlIIelgv--DB6DKGg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="25"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*TuPcSwlIIelgv--DB6DKGg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*TuPcSwlIIelgv--DB6DKGg.png"></noscript></div></div></figure><p name="96ec" id="96ec" class="graf graf--p graf-after--figure">In deep learning, gradient descent works better when features are zero-centered. Intuitively, in RL, the absolute rewards may not be as important as how well an action does compare with the average action. That is the concept of the <strong class="markup--strong markup--p-strong">advantage function</strong> <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">A</em></strong>. In many RL methods, we use <em class="markup--em markup--p-em">A</em> instead of <em class="markup--em markup--p-em">Q</em>.</p><figure name="e304" id="e304" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 31px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.3999999999999995%;"></div><img class="graf-image" data-image-id="1*PLMQ5z0EwH4R0RJiIn7snw.png" data-width="1500" data-height="66" data-action="zoom" data-action-value="1*PLMQ5z0EwH4R0RJiIn7snw.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_PLMQ5z0EwH4R0RJiIn7snw.png"></div></figure><p name="3ab0" id="3ab0" class="graf graf--p graf-after--figure">where <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">A</em></strong> is the expected rewards over the average actions. To recap, here are all the definitions:</p><figure name="63c6" id="63c6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 33px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.7%;"></div><img class="graf-image" data-image-id="1*mEkv4hfGxRwv6kHGkMgQ5A.png" data-width="1400" data-height="66" data-action="zoom" data-action-value="1*mEkv4hfGxRwv6kHGkMgQ5A.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_mEkv4hfGxRwv6kHGkMgQ5A.png"></div></figure><figure name="9d2d" id="9d2d" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 36px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 5.1%;"></div><img class="graf-image" data-image-id="1*DB47b4EY0cVXZhWZ-D6gRg.png" data-width="1400" data-height="72" data-action="zoom" data-action-value="1*DB47b4EY0cVXZhWZ-D6gRg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_DB47b4EY0cVXZhWZ-D6gRg.png"></div></figure><figure name="cd02" id="cd02" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 35px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4.9%;"></div><img class="graf-image" data-image-id="1*xLyVqfy4yrPn9jPJcYrPbA.png" data-width="1500" data-height="74" data-action="zoom" data-action-value="1*xLyVqfy4yrPn9jPJcYrPbA.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_xLyVqfy4yrPn9jPJcYrPbA.png"></div></figure><h3 name="6d54" id="6d54" class="graf graf--h3 graf-after--figure">Q-learning</h3><p name="cba2" id="cba2" class="graf graf--p graf-after--h3">So how can we learn the Q-value? One of the most popular methods is the Q-learning with the following steps:</p><ol class="postList"><li name="f7f0" id="f7f0" class="graf graf--li graf-after--p">We sample an action.</li><li name="f9f9" id="f9f9" class="graf graf--li graf-after--li">We observed the reward and the next state.</li><li name="03e9" id="03e9" class="graf graf--li graf-after--li">We take the action with the highest Q.</li></ol><figure name="b6ad" id="b6ad" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 146px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*NUFoDSKWxRQm7uqhiq6Xcg.jpeg" data-width="1600" data-height="333" data-action="zoom" data-action-value="1*NUFoDSKWxRQm7uqhiq6Xcg.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_NUFoDSKWxRQm7uqhiq6Xcg.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="13"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*NUFoDSKWxRQm7uqhiq6Xcg.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*NUFoDSKWxRQm7uqhiq6Xcg.jpeg"></noscript></div></div></figure><p name="e56f" id="e56f" class="graf graf--p graf-after--figure">Then we apply the dynamic programming again to compute the <em class="markup--em markup--p-em">Q</em>-value function iteratively:</p><figure name="b2f8" id="b2f8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 47px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 6.7%;"></div><img class="graf-image" data-image-id="1*bJmVfEJ98XitkjW1z3TN7w.png" data-width="1450" data-height="97" data-action="zoom" data-action-value="1*bJmVfEJ98XitkjW1z3TN7w.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_bJmVfEJ98XitkjW1z3TN7w.png"></div></figure><p name="8d20" id="8d20" class="graf graf--p graf-after--figure">Here is the algorithm of <em class="markup--em markup--p-em">Q</em>-learning with function fitting. Step 2 below reduces the variance by using Temporal Difference. This also improves the <strong class="markup--strong markup--p-strong">sample efficiency</strong> comparing with the Monte Carlo method which takes samples until the end of the episode.</p><figure name="c55a" id="c55a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 232px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 33.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*-MarQ95RdLWkj8p1upYCDA.jpeg" data-width="1600" data-height="530" data-action="zoom" data-action-value="1*-MarQ95RdLWkj8p1upYCDA.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_-MarQ95RdLWkj8p1upYCDA.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="23"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*-MarQ95RdLWkj8p1upYCDA.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*-MarQ95RdLWkj8p1upYCDA.jpeg"></noscript></div></div><figcaption class="imageCaption">Modified from&nbsp;<a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html" data-href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html" class="markup--anchor markup--figure-anchor" rel="noopener nofollow noopener noopener noopener noopener nofollow noopener" target="_blank">source</a></figcaption></figure><p name="00ec" id="00ec" class="graf graf--p graf-after--figure">Exploration is very important in RL. Without exploration, you will never know what is better ahead. But if it is overdone, we are wasting time. In <em class="markup--em markup--p-em">Q</em>-learning, we have an exploration policy, like epsilon-greedy, to select the action taken in step 1. We pick the action with highest <em class="markup--em markup--p-em">Q</em> value but yet we allow a small chance of selecting other random actions. <em class="markup--em markup--p-em">Q</em> is initialized with zero. Hence, there is no specific action standing out in early training. As the training progress, more promising actions are selected and the training shift from exploration to exploitation.</p><h3 name="6a34" id="6a34" class="graf graf--h3 graf-after--p">Deep Q-network DQN</h3><p name="61fd" id="61fd" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">Q</em>-learning is unfortunately not very stable with deep learning. In this section, we will finally put all things together and introduce the DQN which beats the human in playing some of the Atari Games by accessing the image frames only.</p><figure name="cd5d" id="cd5d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 128px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*l9dL-vOijkkHclYR1tOftA.png" data-width="2140" data-height="392" data-action="zoom" data-action-value="1*l9dL-vOijkkHclYR1tOftA.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_l9dL-vOijkkHclYR1tOftA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="11"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*l9dL-vOijkkHclYR1tOftA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*l9dL-vOijkkHclYR1tOftA.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" data-href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Source</a></figcaption></figure><p name="58d9" id="58d9" class="graf graf--p graf-after--figure">DQN is the poster child for <em class="markup--em markup--p-em">Q</em>-learning using a deep network to approximate <em class="markup--em markup--p-em">Q.</em> We use supervised learning to fit the <em class="markup--em markup--p-em">Q</em>-value function. We want to duplicate the success of supervised learning but RL is different. In deep learning, we randomize the input samples so the input class is quite balanced and pretty stable across training batches. In RL, we search better as we explore more. So the input space and actions we searched are constantly changing. In addition, as we know better, we update the target value of <em class="markup--em markup--p-em">Q</em>. That is bad news. Both the input and output are under frequent changes.</p><figure name="d7a6" id="d7a6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 74px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*1sAPBjetPifL0F9b9ahSsQ.jpeg" data-width="1900" data-height="200" data-action="zoom" data-action-value="1*1sAPBjetPifL0F9b9ahSsQ.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_1sAPBjetPifL0F9b9ahSsQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="7"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*1sAPBjetPifL0F9b9ahSsQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*1sAPBjetPifL0F9b9ahSsQ.jpeg"></noscript></div></div></figure><p name="7d13" id="7d13" class="graf graf--p graf-after--figure">This makes it very hard to learn the <em class="markup--em markup--p-em">Q-</em>value approximator. DQN introduces experience replay and target network to slow down the changes so we can learn <em class="markup--em markup--p-em">Q</em> gradually. Experience replay stores the last million of state-action-reward in a replay buffer. We train <em class="markup--em markup--p-em">Q</em> with batches of random samples from this buffer. Therefore, the training samples are randomized and behave closer to the supervised learning in Deep Learning.</p><p name="fccf" id="fccf" class="graf graf--p graf-after--p">In addition, we have two networks for storing the values of <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Q</em></strong>. One is constantly updated while the second one, the target network, is synchronized from the first network once a while. We use the target network to retrieve the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Q</em></strong> value such that the changes for the target value are less volatile. Here is the objective for those interested. <em class="markup--em markup--p-em">D</em> is the replay buffer and <em class="markup--em markup--p-em">θ- </em>is the target network.</p><figure name="b43a" id="b43a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 110px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*3PNz2dU6Un5UW_YSmuvWvQ.jpeg" data-width="1400" data-height="219" data-action="zoom" data-action-value="1*3PNz2dU6Un5UW_YSmuvWvQ.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_3PNz2dU6Un5UW_YSmuvWvQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="9"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*3PNz2dU6Un5UW_YSmuvWvQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*3PNz2dU6Un5UW_YSmuvWvQ.jpeg"></noscript></div></div></figure><p name="46d4" id="46d4" class="graf graf--p graf-after--figure">DQN allows us to use value learning to solve RL methods in a more stable training environment.</p><h3 name="ebad" id="ebad" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong"><em class="markup--em markup--h3-em">Policy-Gradient</em></strong></h3><p name="f18c" id="f18c" class="graf graf--p graf-after--h3">So far we have covered two major RL methods: model-based and value learning. Model-based RL uses the model and the cost function to find the optimal path. Value learning uses <em class="markup--em markup--p-em">V</em> or <em class="markup--em markup--p-em">Q</em> value to derive the optimal policy. Next, we will cover the third major RL method, also one of the popular ones in RL. The Policy Gradient method focuses on the policy.</p><figure name="fbe5" id="fbe5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 180px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*eNkhTPvTUaiN6-QDklGJfw.jpeg" data-width="1424" data-height="367" data-action="zoom" data-action-value="1*eNkhTPvTUaiN6-QDklGJfw.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_eNkhTPvTUaiN6-QDklGJfw.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="17"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*eNkhTPvTUaiN6-QDklGJfw.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*eNkhTPvTUaiN6-QDklGJfw.jpeg"></noscript></div></div></figure><p name="17a4" id="17a4" class="graf graf--p graf-after--figure">Many of our actions, in particular with human motor controls, are very intuitive. We observe and act rather than plan it thoroughly or take samples for maximum returns.</p><figure name="620c" id="620c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 250px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*Dof34Ien0I0FOc-sOjcv3A.jpeg" data-width="1600" data-height="572" data-action="zoom" data-action-value="1*Dof34Ien0I0FOc-sOjcv3A.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_Dof34Ien0I0FOc-sOjcv3A.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="25"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*Dof34Ien0I0FOc-sOjcv3A.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*Dof34Ien0I0FOc-sOjcv3A.jpeg"></noscript></div></div></figure><p name="0f76" id="0f76" class="graf graf--p graf-after--figure">In the cart-pole example, we may not know the physics but when the pole falls to the left, our experience tells us to move left. Determining actions based on observations can be much easier than understanding a model.</p><figure name="c0dc" id="c0dc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 28px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 3.9%;"></div><img class="graf-image" data-image-id="1*5gL04Bfqf4iXBCK1fLB8tg.png" data-width="1400" data-height="55" data-action="zoom" data-action-value="1*5gL04Bfqf4iXBCK1fLB8tg.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_5gL04Bfqf4iXBCK1fLB8tg.png"></div></figure><p name="e22e" id="e22e" class="graf graf--p graf-after--figure">This type of RL methods is <strong class="markup--strong markup--p-strong">policy-based</strong> which we model a policy parameterized by <em class="markup--em markup--p-em">θ </em>directly.</p><p name="10d2" id="10d2" class="graf graf--p graf-after--p">The concept for Policy Gradient is very simple. For actions with better rewards, we make it more likely to happen (or vice versa). The policy gradient is computed as:</p><figure name="c855" id="c855" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 75px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*knNdtuW11Rv_U2Gfy7G9Ww.png" data-width="1600" data-height="171" data-action="zoom" data-action-value="1*knNdtuW11Rv_U2Gfy7G9Ww.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_knNdtuW11Rv_U2Gfy7G9Ww.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="7"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*knNdtuW11Rv_U2Gfy7G9Ww.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*knNdtuW11Rv_U2Gfy7G9Ww.png"></noscript></div></div></figure><p name="8ee3" id="8ee3" class="graf graf--p graf-after--figure">We use this gradient to update the policy using gradient ascent. i.e. we change the policy in the direction with the steepest reward increase.</p><figure name="838f" id="838f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 28px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 4%;"></div><img class="graf-image" data-image-id="1*bTd1OpnOSpFK6V6IFk-kJA.png" data-width="1400" data-height="56" data-action="zoom" data-action-value="1*bTd1OpnOSpFK6V6IFk-kJA.png" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_bTd1OpnOSpFK6V6IFk-kJA.png"></div></figure><p name="1fa2" id="1fa2" class="graf graf--p graf-after--figure">Let’s look at the policy gradient closely. The one underlines in red above is the maximum likelihood. It measures the likelihood of an action under the specific policy. As we multiply it with the advantage function, we change the policy to favor actions with rewards greater than the average action. Or vice versa, we reduce the chance if it is not better off.</p><p name="66a2" id="66a2" class="graf graf--p graf-after--p">But we need to be very careful in making such policy change. The gradient method is a first-order derivative method. It is not too accurate if the reward function has steep curvature. If our policy change is too aggressive, the estimate policy improvement may be too far off that the decision can be a disaster.</p><figure name="9b04" id="9b04" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 279px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.800000000000004%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*a13NrOnCClT6uLmGYiYSKw.jpeg" data-width="1400" data-height="557" data-action="zoom" data-action-value="1*a13NrOnCClT6uLmGYiYSKw.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_a13NrOnCClT6uLmGYiYSKw.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="29"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*a13NrOnCClT6uLmGYiYSKw.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*a13NrOnCClT6uLmGYiYSKw.jpeg"></noscript></div></div><figcaption class="imageCaption">Image <a href="https://pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/" data-href="https://pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/" class="markup--anchor markup--figure-anchor" rel="noopener nofollow noopener noopener noopener noopener noopener noopener nofollow noopener" target="_blank">source</a></figcaption></figure><p name="0d04" id="0d04" class="graf graf--p graf-after--figure">To address this issue, we impose a trust region. We pick the optimal control within this region only. By establishing an upper bound of the potential error, we know how far we can go before we get too optimistic and the potential error can kill us. Within the trust region, we have a reasonable guarantee that the new policy will be better off. Outside the trust region, the bet is off. If we force it, we may land in states that are much worse and destroy the training progress. TRPO and PPO are methods using the trust region concept to improve the convergence of the policy model.</p><h3 name="f819" id="f819" class="graf graf--h3 graf-after--p">Actor-critic method</h3><p name="d348" id="d348" class="graf graf--p graf-after--h3">Policy Gradient methods use a lot of samples to reach an optimal solution. Every time the policy is updated, we need to resample. Similar to other deep learning methods, it takes many iterations to compute the model. Its convergence is often a major concern. Can we use fewer samples to compute the policy gradient? Can we further reduce the variance of <em class="markup--em markup--p-em">A </em>to make the gradient less volatile?</p><figure name="82ef" id="82ef" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 129px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*al_zhSPHusgnd_IT8UY6Pg.png" data-width="1600" data-height="295" data-action="zoom" data-action-value="1*al_zhSPHusgnd_IT8UY6Pg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_al_zhSPHusgnd_IT8UY6Pg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="13"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*al_zhSPHusgnd_IT8UY6Pg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*al_zhSPHusgnd_IT8UY6Pg.png"></noscript></div></div></figure><p name="41d5" id="41d5" class="graf graf--p graf-after--figure">RL methods are rarely mutually exclusive. We mix different approaches to complement each other. Actor-critic combines the policy gradient with function fitting. In the Actor-critic method, we use the <strong class="markup--strong markup--p-strong">actor</strong> to model the policy and the <strong class="markup--strong markup--p-strong">critic</strong> to model <em class="markup--em markup--p-em">V</em>. By introduce a critic, we reduce the number of samples to collect for each policy update. We don’t collect all samples until the end of an episode. This Temporal Difference technique also reduce variance.</p><p name="ff87" id="ff87" class="graf graf--p graf-after--p">The algorithm of actor-critic is very similar to the policy gradient method. In step 2 below, we are fitting the <em class="markup--em markup--p-em">V</em>-value function, that is the critic. In step 3, we use TD to calculate <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">A</em></strong>. In step 5, we are updating our policy, the actor.</p><figure name="c2a9" id="c2a9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 166px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 23.799999999999997%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*JXFq4joINPlTANeLjymgGg.png" data-width="1456" data-height="346" data-action="zoom" data-action-value="1*JXFq4joINPlTANeLjymgGg.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_JXFq4joINPlTANeLjymgGg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="17"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*JXFq4joINPlTANeLjymgGg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*JXFq4joINPlTANeLjymgGg.png"></noscript></div></div><figcaption class="imageCaption"><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" data-href="http://rail.eecs.berkeley.edu/deeprlcourse/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Source</a></figcaption></figure><h3 name="d01e" id="d01e" class="graf graf--h3 graf-after--figure">Guide Policy&nbsp;Search</h3><p name="be46" id="be46" class="graf graf--p graf-after--h3">The actor-critic mixes the value-learning with policy gradient. Again, we can mix Model-based and Policy-based methods together. We use model-based RL to improve a controller and run the controller on a robot to make moves. A controller determines the best action based on the results of the trajectory optimization.</p><p name="9ee9" id="9ee9" class="graf graf--p graf-after--p">We observe the trajectories and in parallel, we use the generated trajectories to train a policy (the right figure below) using supervised learning.</p><figure name="186e" id="186e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 243px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*pypliLWLL9KOi0uSi9bDSw.jpeg" data-width="2103" data-height="729" data-action="zoom" data-action-value="1*pypliLWLL9KOi0uSi9bDSw.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_pypliLWLL9KOi0uSi9bDSw.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="25"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*pypliLWLL9KOi0uSi9bDSw.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*pypliLWLL9KOi0uSi9bDSw.jpeg"></noscript></div></div><figcaption class="imageCaption">Modified from&nbsp;<a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_10_imitating_optimal_control.pdf" data-href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_10_imitating_optimal_control.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">source</a></figcaption></figure><p name="3576" id="3576" class="graf graf--p graf-after--figure">Why we train a policy when we have a controller? Do they serve the same purpose in predicting the action from a state anyway? That comes to the question of whether the model or the policy is simpler.</p><p name="315c" id="315c" class="graf graf--p graf-after--p">Model-based learning can produce pretty accurate trajectories but may generate inconsistent results for areas where the model is complex and not well trained. Its accumulated errors can hurt also. If the policy is simpler, it should be easier to learn and to generalize. We can use supervised learning to eliminate the noise in the model-based trajectories and discover the fundamental rules behind them.</p><figure name="1694" id="1694" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 200px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.499999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*op6zYpZATTaaB2VaxtrwrA.jpeg" data-width="3000" data-height="856" data-action="zoom" data-action-value="1*op6zYpZATTaaB2VaxtrwrA.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_op6zYpZATTaaB2VaxtrwrA.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="19"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*op6zYpZATTaaB2VaxtrwrA.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*op6zYpZATTaaB2VaxtrwrA.jpeg"></noscript></div></div></figure><p name="c62e" id="c62e" class="graf graf--p graf-after--figure">However, policy-gradient is similar to a trial-and-error method with smarter and educated searches. The training usually has a long warm-up period before seeing any actions that make sense.</p><figure name="7ac4" id="7ac4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 277px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*dd9kDFfsbUePj8JzcyaAew.png" data-width="2000" data-height="790" data-action="zoom" data-action-value="1*dd9kDFfsbUePj8JzcyaAew.png" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_dd9kDFfsbUePj8JzcyaAew.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="29"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*dd9kDFfsbUePj8JzcyaAew.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*dd9kDFfsbUePj8JzcyaAew.png"></noscript></div></div><figcaption class="imageCaption"><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf" data-href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a></figcaption></figure><p name="b12b" id="b12b" class="graf graf--p graf-after--figure">So we combine both of their strength in the Guided Policy Search. We use model-based RL to guide the search better. Then we use the trajectories to train a policy that can generalize better (if the policy is simpler for the task).</p><p name="d713" id="d713" class="graf graf--p graf-after--p">We train both controller and policy in an alternate step. To avoid aggressive changes, we apply the trust region between the controller and the policy again. So the policy and controller are learned in close steps. This helps the training to converge better.</p><figure name="8ba5" id="8ba5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 240px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.300000000000004%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*KN1b1nCagEJl7Ew3_ciKag.jpeg" data-width="1600" data-height="549" data-action="zoom" data-action-value="1*KN1b1nCagEJl7Ew3_ciKag.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_KN1b1nCagEJl7Ew3_ciKag.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="25"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*KN1b1nCagEJl7Ew3_ciKag.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*KN1b1nCagEJl7Ew3_ciKag.jpeg"></noscript></div></div><figcaption class="imageCaption"><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" data-href="http://rail.eecs.berkeley.edu/deeprlcourse/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Source</a></figcaption></figure><h3 name="8ef9" id="8ef9" class="graf graf--h3 graf-after--figure">Deep Learning</h3><p name="2527" id="2527" class="graf graf--p graf-after--h3">What is the role of Deep Learning in reinforcement learning? As mentioned before, deep learning is the eye and the ear. We apply CNN to extract features from images and RNN for voices.</p><figure name="df95" id="df95" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 143px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*01ts7lX7lCigVEdUJhqQ7A.jpeg" data-width="1134" data-height="232" data-action="zoom" data-action-value="1*01ts7lX7lCigVEdUJhqQ7A.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_01ts7lX7lCigVEdUJhqQ7A.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="13"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*01ts7lX7lCigVEdUJhqQ7A.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*01ts7lX7lCigVEdUJhqQ7A.jpeg"></noscript></div></div></figure><p name="c1d9" id="c1d9" class="graf graf--p graf-after--figure">A deep network is also a great function approximator. We can use it to approximate any functions we needed in RL. This includes <em class="markup--em markup--p-em">V</em>-value, <em class="markup--em markup--p-em">Q</em>-value, policy, and model.</p><figure name="601f" id="601f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 125px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*cV6MFkLt1M9dv9DZ-TFNJQ.jpeg" data-width="1332" data-height="238" data-action="zoom" data-action-value="1*cV6MFkLt1M9dv9DZ-TFNJQ.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_cV6MFkLt1M9dv9DZ-TFNJQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="11"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*cV6MFkLt1M9dv9DZ-TFNJQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*cV6MFkLt1M9dv9DZ-TFNJQ.jpeg"></noscript></div></div></figure><p name="96bc" id="96bc" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Partially Observable MDP</strong></p><p name="6047" id="6047" class="graf graf--p graf-after--p">For many problems, objects can be temporarily obstructed by others. To construct the state of the environment, we need more than the current image. For a Partially Observable MDP, we construct states from the recent history of images. This can be done by applying RNN on a sequence of images.</p><figure name="0977" id="0977" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 389px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.60000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*hAExkREX6IGXQ1X1oOp34A.jpeg" data-width="1800" data-height="1000" data-action="zoom" data-action-value="1*hAExkREX6IGXQ1X1oOp34A.jpeg" data-scroll="native"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_hAExkREX6IGXQ1X1oOp34A.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="41"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*hAExkREX6IGXQ1X1oOp34A.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*hAExkREX6IGXQ1X1oOp34A.jpeg"></noscript></div></div></figure><h3 name="301b" id="301b" class="graf graf--h3 graf-after--figure">Which methods to&nbsp;use?</h3><p name="2d1d" id="2d1d" class="graf graf--p graf-after--h3">We have introduced three major groups of RL methods. We can mix and match methods to complement each other and there are many improvements made to each method. Which methods are the best?</p><p name="5b85" id="5b85" class="graf graf--p graf-after--p">This will be impossible to explain within a single section. Each method has its strength and weakness. Model-based RL has the best sample efficiency so far but Model-free RL may have better optimal solutions under the current state of the technology. Sometimes, we can view it more like fashion. What is the best will depend on the year you ask? Research makes progress and out-of-favor methods may have a new lifeline after some improvements. In reality, we mix and match for RL problems. The desired method is strongly restricted by constraints, the context of the task and the progress of the research. For example, robotic controls strongly favor methods with high sample efficient. Physical simulations cannot be replaced by computer simulations easily. But yet in some problem domains, we can now bridge the gap or introduce self-learning better. In short, we are still in a highly evolving field and therefore there is no golden guideline yet. We can only say at the current state, what method may be better under the constraints and the context of your task. Stay tuned and we will have more detail discussion on this.</p><h3 name="d419" id="d419" class="graf graf--h3 graf-after--p">Thoughts</h3><p name="616f" id="616f" class="graf graf--p graf-after--h3">Deep reinforcement learning is about how we make decisions. In this article, we cover three basic algorithm groups namely, model-based RL, value learning and policy gradients. This is just a start. How to learn as efficiently as the human remains challenging. The bad news is there is a lot of room to improve for commercial applications. The good news is the problem is so hard but important that don’t expect that the subject is boring. In this article, we explore the basic but hardly touch its challenge and many innovative solutions that have been proposed. It is one of the hardest areas in AI but probably one of the hardest parts of daily life also. For those want to explore more, here are the articles detailing different RL areas.</p><div name="6f66" id="6f66" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530" data-href="https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530"><strong class="markup--strong markup--mixtapeEmbed-strong">Deep Reinforcement Learning Series</strong><br><em class="markup--em markup--mixtapeEmbed-em">Deep Reinforcement Learning is about making the best decisions for what we see and what we hear. It sounds simple but…</em>medium.com</a><a href="https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="87dee65f484fc4931e718f4936d5e484" data-thumbnail-img-id="0*H5oS37AeAD6JYR93" style="background-image: url(https://cdn-images-1.medium.com/fit/c/200/200/0*H5oS37AeAD6JYR93);"></a></div></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://medium.com/tag/machine-learning?source=post" data-action-source="post">Machine Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/artificial-intelligence?source=post" data-action-source="post">Artificial Intelligence</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/data-science?source=post" data-action-source="post">Data Science</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/programming?source=post" data-action-source="post">Programming</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/deep-learning?source=post" data-action-source="post">Deep Learning</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="35c25e04c199" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----35c25e04c199---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboarding" data-action="multivote" data-action-value="35c25e04c199" data-action-type="long-press" data-action-source="post_actions_footer-----35c25e04c199---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="35c25e04c199"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker" data-action="show-recommends" data-action-value="35c25e04c199">1.5K claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/35c25e04c199/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/35c25e04c199/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-marginRight12" data-action="scroll-to-responses">10</button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="35c25e04c199" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="bd51f1a63813"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="toggle-block-user" data-action-value="bd51f1a63813" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="toggle-subscribe-user" data-action-value="bd51f1a63813" data-action-source="footer_card-bd51f1a63813-------------------------follow_footer" data-subscribe-source="footer_card" data-follow-context-entity-id="35c25e04c199"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://medium.com/@jonathan_hui?source=footer_card" title="Go to the profile of Jonathan Hui" aria-label="Go to the profile of Jonathan Hui" data-action-source="footer_card" data-user-id="bd51f1a63813" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_c3Z3aOPBooxEX4tx4RkzLw(1).jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Jonathan Hui"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://medium.com/@jonathan_hui" property="cc:attributionName" title="Go to the profile of Jonathan Hui" aria-label="Go to the profile of Jonathan Hui" rel="author cc:attributionUrl" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Deep Learning</p></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="35c25e04c199" data-scroll="native"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1032 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="be570a40f6de" data-source="placement_card_footer_grid---------0-44" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/@jonathan_hui/qc-simons-algorithm-be570a40f6de?source=placement_card_footer_grid---------0-44" data-action-source="placement_card_footer_grid---------0-44"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/0*vASC2neUi_NVFqYM&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://medium.com/@jonathan_hui/qc-simons-algorithm-be570a40f6de?source=placement_card_footer_grid---------0-44" data-action-source="placement_card_footer_grid---------0-44"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">More from Jonathan Hui</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">QC — Simon’s algorithm</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_c3Z3aOPBooxEX4tx4RkzLw(2).jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jonathan Hui"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://medium.com/@jonathan_hui?source=placement_card_footer_grid---------0-44" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-44" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@jonathan_hui/qc-simons-algorithm-be570a40f6de?source=placement_card_footer_grid---------0-44" data-action="open-post" data-action-value="https://medium.com/@jonathan_hui/qc-simons-algorithm-be570a40f6de?source=placement_card_footer_grid---------0-44" data-action-source="preview-listing"><time datetime="2018-12-11T23:31:00.962Z">Dec 12, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="5 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="be570a40f6de" data-is-label-padded="true" data-source="placement_card_footer_grid-----be570a40f6de----0-44----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="be570a40f6de" data-action-type="long-press" data-action-source="placement_card_footer_grid-----be570a40f6de----0-44----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="be570a40f6de">268</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="be570a40f6de" data-action-source="placement_card_footer_grid-----be570a40f6de----0-44----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="14ac0b4493cc" data-source="placement_card_footer_grid---------1-60" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/free-code-camp/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc?source=placement_card_footer_grid---------1-60" data-action-source="placement_card_footer_grid---------1-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/0*DX9ZRnzwmh2FImV-&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://medium.com/free-code-camp/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc?source=placement_card_footer_grid---------1-60" data-action-source="placement_card_footer_grid---------1-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Related reads</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">An introduction to Q-Learning: reinforcement learning</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@iamadl" data-action="show-user-card" data-action-value="a8ae39d4d401" data-action-type="hover" data-user-id="a8ae39d4d401" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_pMtFxlNwSZrCrxdGvTwcSA.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of ADL"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://medium.com/@iamadl?source=placement_card_footer_grid---------1-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-60" data-action-value="a8ae39d4d401" data-action-type="hover" data-user-id="a8ae39d4d401" dir="auto">ADL</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/free-code-camp/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc?source=placement_card_footer_grid---------1-60" data-action="open-post" data-action-value="https://medium.com/free-code-camp/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc?source=placement_card_footer_grid---------1-60" data-action-source="preview-listing"><time datetime="2018-09-03T21:31:39.029Z">Sep 4, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="6 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="14ac0b4493cc" data-is-label-padded="true" data-source="placement_card_footer_grid-----14ac0b4493cc----1-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="14ac0b4493cc" data-action-type="long-press" data-action-source="placement_card_footer_grid-----14ac0b4493cc----1-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="14ac0b4493cc">2.3K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="14ac0b4493cc" data-action-source="placement_card_footer_grid-----14ac0b4493cc----1-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="ed0eb5b58288" data-source="placement_card_footer_grid---------2-60" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/1*Q9gDKBugQeYNxA6ZBei1MQ.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Related reads</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Cartpole - Introduction to Reinforcement Learning (DQN - Deep Q-Learning)</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@gsurma" data-action="show-user-card" data-action-value="ef86714c1eec" data-action-type="hover" data-user-id="ef86714c1eec" data-collection-slug="towards-data-science" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_PgSTuTAlhHLwrpoTQum2xA.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Greg Surma"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@gsurma?source=placement_card_footer_grid---------2-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-60" data-action-value="ef86714c1eec" data-action-type="hover" data-user-id="ef86714c1eec" data-collection-slug="towards-data-science" dir="auto">Greg Surma</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288?source=placement_card_footer_grid---------2-60" data-action="open-post" data-action-value="https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288?source=placement_card_footer_grid---------2-60" data-action-source="preview-listing"><time datetime="2018-09-26T13:03:12.104Z">Sep 26, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="6 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="ed0eb5b58288" data-is-label-padded="true" data-source="placement_card_footer_grid-----ed0eb5b58288----2-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="ed0eb5b58288" data-action-type="long-press" data-action-source="placement_card_footer_grid-----ed0eb5b58288----2-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="ed0eb5b58288">1.5K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="ed0eb5b58288" data-action-source="placement_card_footer_grid-----ed0eb5b58288----2-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream-editor cardChromeless u-marginBottom20 u-paddingLeft20 u-paddingRight20 js-responsesStreamEditor"><div class="inlineNewPostControl js-inlineNewPostControl" data-action-scope="_actionscope_14"><div class="inlineEditor is-collapsed is-postEditMode js-inlineEditor" data-action="focus-editor"><div class="u-paddingTop20 js-block js-inlineEditorContent"><div class="inlineEditor-header"><div class="inlineEditor-avatar u-paddingRight20"><div class="avatar u-inline"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/0_toB60eKEH3klSlAD(1).jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Vivek Agrawal"></div></div><div class="inlineEditor-headerContent"><div class="inlineEditor-placeholder js-inlineEditorPrompt">Write a response…</div><div class="inlineEditor-author u-accentColor--textNormal">Vivek Agrawal</div></div></div></div></div></div></div><div class="responsesStream js-responsesStream"><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_6"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="2fd3b2c7d5ab" data-source="responses---------0-----------------------" data-action-scope="_actionscope_7" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@parekhvivek49" data-action="show-user-card" data-action-value="864a4c487658" data-action-type="hover" data-user-id="864a4c487658" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_dmbNkD5D-u45r44go_cf0g.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of parekh vivek"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@parekhvivek49?source=responses---------0-----------------------" data-action="show-user-card" data-action-source="responses---------0-----------------------" data-action-value="864a4c487658" data-action-type="hover" data-user-id="864a4c487658" dir="auto">parekh vivek</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@parekhvivek49/good-article-2fd3b2c7d5ab?source=responses---------0-----------------------" data-action="open-post" data-action-value="https://medium.com/@parekhvivek49/good-article-2fd3b2c7d5ab?source=responses---------0-----------------------" data-action-source="preview-listing"><time datetime="2018-10-30T14:29:21.886Z">Oct 30, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="1 min read"></span></div></div></div></div></div><div class="js-inlineExpandBody"><a class="" href="https://medium.com/@parekhvivek49/good-article-2fd3b2c7d5ab?source=responses---------0-----------------------" data-action="expand-inline"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="68f5" id="68f5" class="graf graf--p graf--leading">good article.</p><p name="d401" id="d401" class="graf graf--p graf-after--p">can you please explain bit more in this lines</p><p name="b01c" id="b01c" class="graf graf--p graf-after--p graf--trailing">In addition, we have two networks for storing the values of <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Q</em></strong>. One is constantly updated while the second one, the target network, is synchronized from the first network once a while. We use the target network to retrieve the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Q</em></strong> value such…</p></div></div></section></div></a></div><div class="postArticle-readMore"><button class="button button--smaller button--link u-baseColor--buttonNormal" data-action="expand-inline">Read more…</button></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="2fd3b2c7d5ab" data-is-flush-left="true" data-source="listing-----2fd3b2c7d5ab---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="2fd3b2c7d5ab" data-action-type="long-press" data-action-source="listing-----2fd3b2c7d5ab---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@parekhvivek49/good-article-2fd3b2c7d5ab?source=responses---------0-----------------------#--responses" data-action-source="responses---------0-----------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="2fd3b2c7d5ab" data-action-source="listing-----2fd3b2c7d5ab---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="2fd3b2c7d5ab"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="b5cd55863acf" data-source="responses---------0-----------------------" data-action-scope="_actionscope_8" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_c3Z3aOPBooxEX4tx4RkzLw(2).jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jonathan Hui"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@jonathan_hui?source=responses---------0-----------------------" data-action="show-user-card" data-action-source="responses---------0-----------------------" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@jonathan_hui/this-will-explain-it-b5cd55863acf?source=responses---------0-----------------------" data-action="open-post" data-action-value="https://medium.com/@jonathan_hui/this-will-explain-it-b5cd55863acf?source=responses---------0-----------------------" data-action-source="preview-listing"><time datetime="2018-10-30T15:29:05.373Z">Oct 30, 2018</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@jonathan_hui/this-will-explain-it-b5cd55863acf?source=responses---------0-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3864" id="3864" class="graf graf--p graf--leading">This will explain it.</p><p name="9596" id="9596" class="graf graf--p graf-after--p graf--trailing"><span class="markup--anchor markup--p-anchor" data-action="open-inner-link" data-action-value="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4">https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4</span></p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="b5cd55863acf" data-is-flush-left="true" data-source="listing-----b5cd55863acf---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="b5cd55863acf" data-action-type="long-press" data-action-source="listing-----b5cd55863acf---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="b5cd55863acf" data-action-source="listing-----b5cd55863acf---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="b5cd55863acf"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_9"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="348b51118e44" data-source="responses---------1-----------------------" data-action-scope="_actionscope_10" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@adi_63613" data-action="show-user-card" data-action-value="5a5de5d7b480" data-action-type="hover" data-user-id="5a5de5d7b480" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_dmbNkD5D-u45r44go_cf0g(1).png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Adì Lev-Tov"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@adi_63613?source=responses---------1-----------------------" data-action="show-user-card" data-action-source="responses---------1-----------------------" data-action-value="5a5de5d7b480" data-action-type="hover" data-user-id="5a5de5d7b480" dir="auto">Adì Lev-Tov</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@adi_63613/thanks-for-the-article-348b51118e44?source=responses---------1-----------------------" data-action="open-post" data-action-value="https://medium.com/@adi_63613/thanks-for-the-article-348b51118e44?source=responses---------1-----------------------" data-action-source="preview-listing"><time datetime="2019-01-07T08:44:01.508Z">Jan 7</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="1 min read"></span></div></div></div></div></div><div class="js-inlineExpandBody"><a class="" href="https://medium.com/@adi_63613/thanks-for-the-article-348b51118e44?source=responses---------1-----------------------" data-action="expand-inline"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="380d" id="380d" class="graf graf--pullquote graf--leading graf--trailing">Thanks for the article. I come from another field, so my question might look “strange”: can RL learn from a “non functioning well” physical model i.e. the physical system is defecate? the physical system is either&nbsp;with…</blockquote></div></div></section></div></a></div><div class="postArticle-readMore"><button class="button button--smaller button--link u-baseColor--buttonNormal" data-action="expand-inline">Read more…</button></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="348b51118e44" data-is-flush-left="true" data-source="listing-----348b51118e44---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="348b51118e44" data-action-type="long-press" data-action-source="listing-----348b51118e44---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@adi_63613/thanks-for-the-article-348b51118e44?source=responses---------1-----------------------#--responses" data-action-source="responses---------1-----------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="348b51118e44" data-action-source="listing-----348b51118e44---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="348b51118e44"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="ab60c482598b" data-source="responses---------1-----------------------" data-action-scope="_actionscope_11" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_c3Z3aOPBooxEX4tx4RkzLw(2).jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jonathan Hui"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@jonathan_hui?source=responses---------1-----------------------" data-action="show-user-card" data-action-source="responses---------1-----------------------" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@jonathan_hui/your-question-is-a-little-bit-vague-ab60c482598b?source=responses---------1-----------------------" data-action="open-post" data-action-value="https://medium.com/@jonathan_hui/your-question-is-a-little-bit-vague-ab60c482598b?source=responses---------1-----------------------" data-action-source="preview-listing"><time datetime="2019-01-07T15:50:39.585Z">Jan 7</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@jonathan_hui/your-question-is-a-little-bit-vague-ab60c482598b?source=responses---------1-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2b33" id="2b33" class="graf graf--p graf--leading graf--trailing">Your question is a little bit vague. So it is hard to answer. Deep learning is good at modeling complex function. DL is just a function estimator in some sense and learns from data. If it is predictable, you just need to know what features/data need to process. Some may argue stock is unpredictable, therefore it is hard to model. But people try to do it with DL anyway.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="ab60c482598b" data-is-flush-left="true" data-source="listing-----ab60c482598b---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="ab60c482598b" data-action-type="long-press" data-action-source="listing-----ab60c482598b---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@jonathan_hui/your-question-is-a-little-bit-vague-ab60c482598b?source=responses---------1-----------------------#--responses" data-action-source="responses---------1-----------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="ab60c482598b" data-action-source="listing-----ab60c482598b---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="ab60c482598b"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="425e05ada824" data-source="responses---------1-----------------------" data-action-scope="_actionscope_12" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@adi_63613" data-action="show-user-card" data-action-value="5a5de5d7b480" data-action-type="hover" data-user-id="5a5de5d7b480" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_dmbNkD5D-u45r44go_cf0g(1).png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Adì Lev-Tov"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@adi_63613?source=responses---------1-----------------------" data-action="show-user-card" data-action-source="responses---------1-----------------------" data-action-value="5a5de5d7b480" data-action-type="hover" data-user-id="5a5de5d7b480" dir="auto">Adì Lev-Tov</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@adi_63613/thanks-for-your-answer-425e05ada824?source=responses---------1-----------------------" data-action="open-post" data-action-value="https://medium.com/@adi_63613/thanks-for-your-answer-425e05ada824?source=responses---------1-----------------------" data-action-source="preview-listing"><time datetime="2019-01-08T08:22:53.263Z">Jan 8</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@adi_63613/thanks-for-your-answer-425e05ada824?source=responses---------1-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="c7f0" id="c7f0" class="graf graf--pullquote graf--leading graf--trailing">Thanks for your answer. My question is related to the data on which AI is trained i.e. If the data is not complete and/or is representing a non-functioning physical system, will the AI still is able to give a “good”&nbsp;answer?</blockquote></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="425e05ada824" data-is-flush-left="true" data-source="listing-----425e05ada824---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="425e05ada824" data-action-type="long-press" data-action-source="listing-----425e05ada824---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@adi_63613/thanks-for-your-answer-425e05ada824?source=responses---------1-----------------------#--responses" data-action-source="responses---------1-----------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="425e05ada824" data-action-source="listing-----425e05ada824---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="425e05ada824"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="3622893059e6" data-source="responses---------1-----------------------" data-action-scope="_actionscope_13" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/1_c3Z3aOPBooxEX4tx4RkzLw(2).jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jonathan Hui"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@jonathan_hui?source=responses---------1-----------------------" data-action="show-user-card" data-action-source="responses---------1-----------------------" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@jonathan_hui/dl-mainly-focuses-on-learning-from-data-3622893059e6?source=responses---------1-----------------------" data-action="open-post" data-action-value="https://medium.com/@jonathan_hui/dl-mainly-focuses-on-learning-from-data-3622893059e6?source=responses---------1-----------------------" data-action-source="preview-listing"><time datetime="2019-01-08T17:12:06.591Z">Jan 8</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="1 min read"></span></div></div></div></div></div><div class="js-inlineExpandBody"><a class="" href="https://medium.com/@jonathan_hui/dl-mainly-focuses-on-learning-from-data-3622893059e6?source=responses---------1-----------------------" data-action="expand-inline"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="56e7" id="56e7" class="graf graf--p graf--leading graf--trailing">DL mainly focuses on learning from data. So that will be very challenging for the current state of technology. But for RL, one objective is to learn effectively as human. So the first question is: does human perform well under this limit set of data? If not, then AI will have a tough time also. But being said, it is still under heavy research of what…</p></div></div></section></div></a></div><div class="postArticle-readMore"><button class="button button--smaller button--link u-baseColor--buttonNormal" data-action="expand-inline">Read more…</button></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="3622893059e6" data-is-flush-left="true" data-source="listing-----3622893059e6---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="3622893059e6" data-action-type="long-press" data-action-source="listing-----3622893059e6---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="3622893059e6" data-action-source="listing-----3622893059e6---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="3622893059e6"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div></div><div class="container js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-fixed u-sm-hide js-postShareWidget u-transition--fadeIn300" data-scroll="fixed" style="transform: translateY(150px);"><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="35c25e04c199" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----35c25e04c199---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="35c25e04c199" data-action-type="long-press" data-action-source="post_share_widget-----35c25e04c199---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="35c25e04c199">1.5K</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="35c25e04c199" data-action-source="post_share_widget-----35c25e04c199---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/35c25e04c199/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/35c25e04c199/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a></li></ul></div></aside><div class="highlightMenu" data-action-scope="_actionscope_3"><div class="highlightMenu-inner"><div class="buttonSet"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="quote" data-action-source="quote_menu--------------------------highlight_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="quote-respond" data-action-source="quote_menu--------------------------respond_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless button--highlightMenu js-highlightMenuTwitterShare" href="https://medium.com/p/35c25e04c199/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action="twitter"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="highlight" data-action-source="quote_menu--------------------------privatenote_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"38048-00ee67b","currentUser":{"userId":"be025283e717","username":"vivekagr12199","name":"Vivek Agrawal","email":"vivekagr12199@gmail.com","imageId":"0*toB60eKEH3klSlAD.jpg","createdAt":1555886025012,"lastPostCreatedAt":1562172632717,"isVerified":true,"subscriberEmail":"","onboardingStatus":1,"googleAccountId":"101203869735328059129","googleEmail":"vivekagr12199@gmail.com","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":true,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":true,"isWriterProgramOptedOut":false,"writerProgramVersion":5,"writerProgramEnrolledAt":1555886025012,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":true},"currentUserHasUnverifiedEmail":false,"isAuthenticated":true,"isCurrentUserVerified":true,"language":"en-in","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.yPFOKEJjWpZVGvh54bK7SQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.cUeGLoj7sHNpuw-bFOuTeQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.6FOGY6ouqNMySk5yHMmW3A.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.d2Ua48DeSXmhnx6BikN_Iw.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.lGxiZyGF1luZqFEkF5YgGw.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.RzL89au1QllN2FPu92pO7w.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.cg-IrTO8WysnMg0c__FJHA.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.b2XUkpbaZgVu0fuO48AaIw.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.wvhZeq_iZ2e-AecOAxjiBg.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.qf6Gl40snMZcEXwnIYeTVw.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.WhICKSRsJMorvucd3Nl7Jg.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1562635667256:c95d3d66b6ae","useragent":{"browser":"chrome","family":"chrome","os":"windows","version":75,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":1,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"isSeoBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_international_tax_withholding":true,"enable_international_tax_withholding_documentation":true,"enable_revised_first_partner_program_distro_on_email":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_primary_topic_for_mobile":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_embedding_based_diversification":true,"enable_pub_newsletters":true,"enable_lite_pub_header_menu":true,"enable_lite_claps":true,"enable_lite_post_manager_gear_menu":true,"enable_live_user_post_scoring":true,"enable_lite_post_highlights":true,"enable_lite_post_highlights_view_only":true,"enable_tick_landing_page":true,"enable_lite_private_notes":true,"enable_trumpland_landing_page":true,"enable_lite_email_sign_in_flow":true,"enable_daily_read_digest_promo":true,"enable_lite_paywall_alert":true,"enable_serve_recs_from_ml_rank_homepage":true,"enable_serve_recs_from_ml_rank_digest":true,"enable_serve_recs_from_ml_rank_app_highlights":true,"enable_lite_google_captcha":true,"enable_lite_branch_io":true,"enable_lite_notifications":true,"enable_ticks_digest_promo":true,"enable_lite_verify_email_butter_bar":true,"enable_lite_unread_notification_count":true},"xsrfToken":"KdR9CGheyWKQ","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"131a322a1c8b0d4f\",\"ot-tracer-traceid\":\"4d31cb2a52616044\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"li_post_page","type":0,"url":"www.calendly.com"},{"promptId":"li_home_page","type":1,"url":"mediumuserfeedback.typeform.com/to/GcFjEO"},{"promptId":"li_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"IN","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618"]}}
// ]]></script><script charset="UTF-8" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/main-base.bundle.yPFOKEJjWpZVGvh54bK7SQ.js.download" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"35c25e04c199","versionId":"7f7cbb82a772","creatorId":"bd51f1a63813","creator":{"userId":"bd51f1a63813","name":"Jonathan Hui","username":"jonathan_hui","createdAt":1518417490123,"imageId":"1*c3Z3aOPBooxEX4tx4RkzLw.jpeg","backgroundImageId":"","bio":"Deep Learning","twitterScreenName":"","socialStats":{"userId":"bd51f1a63813","usersFollowedCount":17,"usersFollowedByCount":8947,"type":"SocialStats"},"social":{"userId":"be025283e717","targetUserId":"bd51f1a63813","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollectionId":"","title":"RL— Introduction to Deep Reinforcement Learning","detectedLanguage":"en","latestVersion":"7f7cbb82a772","latestPublishedVersion":"7f7cbb82a772","hasUnpublishedEdits":false,"latestRev":12265,"createdAt":1535580176556,"updatedAt":1546819631183,"acceptedAt":0,"firstPublishedAt":1539547800410,"latestPublishedAt":1546819631183,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a…","bodyModel":{"paragraphs":[{"name":"c858","type":3,"text":"RL— Introduction to Deep Reinforcement Learning","markups":[]},{"name":"149c","type":4,"text":"Photo by Fab Lentz","markups":[{"type":3,"start":9,"end":18,"href":"https://unsplash.com/@fossy?utm_source=medium&utm_medium=referral","title":"","rel":"photo-creator","anchorType":0}],"layout":1,"metadata":{"id":"0*JjsLueA2KATdLD5Z","originalWidth":2048,"originalHeight":1524,"isFeatured":true}},{"name":"b093","type":1,"text":"Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a high barrier in learning the concepts and the lingos. In this article, we will cover deep RL with an overview of the general landscape. Yet, we will not shy away from equations and lingos. They provide the basics in understanding the concepts deeper. We will not appeal to you that it only takes 20 lines of code to tackle an RL problem. The official answer should be one! But we will try hard to make it approachable.","markups":[{"type":1,"start":126,"end":128}]},{"name":"8ae3","type":1,"text":"In most AI topics, we create mathematical frameworks to tackle problems. For RL, the answer is the Markov Decision Process (MDP). It sounds complicated but it produces an easy framework to model a complex problem. An agent (e.g. a human) observes the environment and takes actions. Rewards are given out but they may be infrequent and delayed. Very often, the long-delayed rewards make it extremely hard to untangle the information and traceback what sequence of actions contributed to the rewards.","markups":[{"type":1,"start":99,"end":128},{"type":1,"start":217,"end":222},{"type":1,"start":273,"end":280},{"type":1,"start":282,"end":289}]},{"name":"b7cf","type":1,"text":"Markov decision process (MDP) composes of:","markups":[]},{"name":"a662","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*sMZBhROM7Wt4lqHUc6VRqw.jpeg","originalWidth":1600,"originalHeight":73}},{"name":"a0e1","type":4,"text":"Source: left, right","markups":[{"type":3,"start":7,"end":13,"href":"https://drive.google.com/file/d/0BxXI_RttTZAhVXBlMUVkQ1BVVDQ/view","title":"","rel":"noopener","anchorType":0},{"type":3,"start":14,"end":19,"href":"https://mitpress.mit.edu/books/reinforcement-learning","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*X6h1TXf4QidZbu2NfVl1wg.png","originalWidth":1680,"originalHeight":700}},{"name":"d0c4","type":1,"text":"State in MDP can be represented as raw images.","markups":[{"type":1,"start":0,"end":5}]},{"name":"4487","type":4,"text":"AlphaGO & Atari Seaquest","markups":[],"layout":1,"metadata":{"id":"1*aosA682yS21d8lhhgL9AHA.jpeg","originalWidth":2911,"originalHeight":788}},{"name":"5e4d","type":1,"text":"Or for robotic controls, we use sensors to measure the joint angles, velocity, and the end-effector pose:","markups":[]},{"name":"115a","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*8snUNu8FgaAs6UMm-o69og.jpeg","originalWidth":2000,"originalHeight":497}},{"name":"9e62","type":9,"text":"An action can be a move in a chess game or moving a robotic arm or a joystick.","markups":[{"type":1,"start":3,"end":9}]},{"name":"be5e","type":9,"text":"For a GO game, the reward is very sparse: 1 if we win or -1 if we lose. Sometimes, we get rewards more frequently. In the Atari Seaquest game, we score whenever we hit the sharks.","markups":[]},{"name":"90bc","type":9,"text":"The discount factor discounts future rewards if it is smaller than one. Money earned in the future often has a smaller current value, and we may need it for a purely technical reason to converge the solution better.","markups":[]},{"name":"ce04","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*qR9uqk2c8LNB8LGQ6sU-qg.png","originalWidth":1500,"originalHeight":116}},{"name":"f283","type":9,"text":"We can rollout actions forever or limit the experience to N time steps. This is called the horizon.","markups":[{"type":2,"start":58,"end":59}]},{"name":"3537","type":1,"text":"The transition function is the system dynamics. It predicts the next state after taking action. It is called the model which plays a major role when we discuss Model-based RL later.","markups":[{"type":1,"start":113,"end":119}]},{"name":"bcd2","type":1,"text":"Convention","markups":[{"type":1,"start":0,"end":10}]},{"name":"e8d0","type":1,"text":"The concepts in RL come from many research fields including the control theory. Different notations may be used in a different context. Let’s get this out first before any confusion. The state can be written as s or x, and action as a or u. An action is the same as a control. We can maximize the rewards or minimizes the costs which are simply the negative of each other. Notations can be in upper or lower case.","markups":[{"type":1,"start":244,"end":250},{"type":1,"start":268,"end":275},{"type":2,"start":211,"end":212},{"type":2,"start":216,"end":217},{"type":2,"start":233,"end":234},{"type":2,"start":238,"end":239}]},{"name":"8ddf","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*SJXL97vgexqJkH6PT-VQQw.png","originalWidth":1600,"originalHeight":271}},{"name":"1b9e","type":3,"text":"Policy","markups":[{"type":1,"start":0,"end":6}]},{"name":"b952","type":1,"text":"In RL, our focus is finding an optimal policy. A policy tells us how to act from a particular state.","markups":[{"type":1,"start":39,"end":47},{"type":1,"start":48,"end":49}]},{"name":"7e8e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*3qVw9kj0Unbh2SjScCt8wg.png","originalWidth":1400,"originalHeight":55}},{"name":"326c","type":1,"text":"Like the weights in Deep Learning methods, this policy can be parameterized by θ,","markups":[{"type":2,"start":79,"end":81}]},{"name":"e118","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*XmXQfyvadrUd73ZOU1o95g.jpeg","originalWidth":1392,"originalHeight":327}},{"name":"53de","type":1,"text":"and we want to find a policy that makes the most rewarding decisions:","markups":[]},{"name":"fea8","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*7Yt8b1k0POIGR_cnqRvqPw.jpeg","originalWidth":1424,"originalHeight":244}},{"name":"43f0","type":1,"text":"In real life, nothing is absolute. So our policy can be deterministic or stochastic. For stochastic, the policy outputs a probability distribution instead.","markups":[]},{"name":"0d02","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*fPfloA0G8zrz89OqpD2oJA.png","originalWidth":1500,"originalHeight":86}},{"name":"5fa9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*37PNsTcH0U5fe1UaCx810A.png","originalWidth":2400,"originalHeight":458}},{"name":"cc38","type":1,"text":"Finally, let’s put our objective together. In RL, we want to find a sequence of actions that maximize expected rewards or minimize cost.","markups":[]},{"name":"a21f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*P4alpFJHhNtbbbXGK1UAUQ.jpeg","originalWidth":1424,"originalHeight":397}},{"name":"818d","type":1,"text":"But there are many ways to solve the problem. For example, we can","markups":[]},{"name":"10bb","type":9,"text":"Analyze how good to reach a certain state or take a specific action (i.e. Value-learning),","markups":[]},{"name":"3140","type":9,"text":"Use the model to find actions that have the maximum rewards (model-based learning), or","markups":[]},{"name":"0c26","type":9,"text":"Derive a policy directly to maximize rewards (policy gradient).","markups":[]},{"name":"a753","type":1,"text":"We will go through all these approaches shortly.","markups":[]},{"name":"87aa","type":3,"text":"Notation","markups":[]},{"name":"6513","type":1,"text":"But, in case you want further elaboration for the terms and notation in RL first, this table should help.","markups":[]},{"name":"5a88","type":4,"text":"Modified from source","markups":[{"type":3,"start":14,"end":20,"href":"https://arxiv.org/pdf/1504.00702.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*uC5gslmql53IxSRSS1oBsg.jpeg","originalWidth":1600,"originalHeight":982}},{"name":"827c","type":3,"text":"Model-based RL","markups":[{"type":1,"start":0,"end":14}]},{"name":"ad15","type":1,"text":"Intuitively, if we know the rule of the game and how much it costs for every move, we can find the actions that minimize the cost. The model p (the system dynamics) predicts the next state after taking an action. Mathematically, it is formulated as a probability distribution. In this article, the model can be written as p or f.","markups":[{"type":1,"start":135,"end":140},{"type":1,"start":141,"end":143},{"type":1,"start":322,"end":323},{"type":1,"start":327,"end":328},{"type":2,"start":141,"end":143},{"type":2,"start":322,"end":323},{"type":2,"start":327,"end":328}]},{"name":"6557","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*J2s5B7bu63hBAmhC-CbWlw.jpeg","originalWidth":1350,"originalHeight":52}},{"name":"9aae","type":1,"text":"Let’s demonstrate the idea of a model with a cart-pole example. p models the angle of the pole after taking action.","markups":[{"type":1,"start":64,"end":65},{"type":2,"start":64,"end":65}]},{"name":"9cdd","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Z9g2-AyfQbFHZMFDfwbnaQ.png","originalWidth":1600,"originalHeight":491}},{"name":"d49e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*wJ3vb0rN0ZmnkZuOQpUtaA.jpeg","originalWidth":1600,"originalHeight":150}},{"name":"3d80","type":1,"text":"Here is the probability distribution output for θ in the next time step for the example above.","markups":[{"type":2,"start":48,"end":50}]},{"name":"980d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*32zFcyhR5eqDClXDbSplJQ.png","originalWidth":2200,"originalHeight":648}},{"name":"6140","type":1,"text":"This model describes the law of Physics. But a model can be just the rule of a chess game. The core idea of Model-based RL is using the model and the cost function to locate the optimal path of actions (to be exact — a trajectory of states and actions).","markups":[{"type":1,"start":108,"end":123},{"type":1,"start":125,"end":126}]},{"name":"1d63","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*uUKlBZW8EMS4L8jlVMDaWQ.jpeg","originalWidth":1450,"originalHeight":66}},{"name":"c8f0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*oWcm1sR3mZw_iSmiQDUr-Q.jpeg"}},{"name":"3d7a","type":1,"text":"Let’s get into another example. In the GO game, the model is the rule of the game. According to this rule, we search the possible moves and find the actions to win the game. Of course, the search space is too large and we need to search smarter.","markups":[]},{"name":"d372","type":4,"text":"AlphaGO","markups":[],"layout":1,"metadata":{"id":"1*U_9cl3HV5o_8DUC7gmsDXg.jpeg"}},{"name":"3a7c","type":6,"text":"In model-based RL, we use the model and cost function to find an optimal trajectory of states and actions (optimal control).","markups":[{"type":1,"start":62,"end":83},{"type":1,"start":107,"end":122}]},{"name":"cfa4","type":1,"text":"Sometimes, we may not know the models. But this does not exclude us from learning them. Indeed, we can use deep learning to model complex motions from sample trajectories or approximate them locally.","markups":[]},{"name":"a791","type":1,"text":"The video below is a nice demonstration of performing tasks by a robot using Model-based RL. Instead of programming the robot arm directly, the robot is trained for 20 minutes to learn each task, mostly by itself. Once it is done, the robot should handle situations that have not trained before. We can move around the objects or change the grasp of the hammer, the robot should manage to complete the task successfully.","markups":[]},{"name":"52be","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"7e8b53ed7b0e11d6e928fc9c3f6a204f","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FQ4bMcUk6pcw%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"79ca","type":1,"text":"The tasks sound pretty simple. But they are not easy to solve. We often make approximations to make it easier. For example, we approximate the system dynamics to be linear and the cost function to be a quadratic equation.","markups":[]},{"name":"cbba","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*bRIOfdX3kSdX1FtPD8ChsA.jpeg","originalWidth":1680,"originalHeight":132}},{"name":"3271","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*tq4EeBxSl4XmqAQnsw2d8g.jpeg","originalWidth":1600,"originalHeight":136}},{"name":"9c7b","type":1,"text":"Then we find the actions that minimize the cost while obeying the model.","markups":[]},{"name":"c8b1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*fJXD75kQup0hYRYBQUy17g.jpeg","originalWidth":1390,"originalHeight":120}},{"name":"3a0b","type":1,"text":"There are known optimization methods like LQR to solve this kind of objective. To move to non-linear system dynamics, we can apply iLQR which use LQR iteratively to find the optimal solution similar to Newton’s optimization. All these methods are complex and computationally intense. You can find the details in here. But for our context, we just need to know that given a cost function and a model, we can find the corresponding optimal actions.","markups":[{"type":3,"start":312,"end":316,"href":"https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750","title":"","rel":"","anchorType":0}]},{"name":"e74d","type":1,"text":"Model-based RL has a strong competitive edge over other RL methods because it is sample efficiency. Many models can be approximated locally with fewer samples and trajectory planning requires no further samples. If physical simulation takes time, the saving is significant. Therefore, it is popular in robotic control. With other RL methods, the same training may take weeks.","markups":[]},{"name":"f73a","type":1,"text":"Let’s detail the process a little bit more. The following is the MPC (Model Predictive Control) which run a random or an educated policy to explore space to fit the model. Then, in step 3, we use iLQR to plan the optimal controls. But we only execute the first action in the plan. We observe the state again and replan the trajectory. This allows us to take corrective actions if needed.","markups":[]},{"name":"12f0","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf","title":"","rel":"nofollow noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*RSCF3Zd9Qykxg6V1w5ZRAQ.png","originalWidth":1600,"originalHeight":413}},{"name":"039c","type":1,"text":"The following figure summarizes the flow. We observe the environments and extract the states. We fit the model and use a trajectory optimization method to plan our path which composes of actions required at each time step.","markups":[]},{"name":"865d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*sQG2HXMcoU0_1TZcHAdNaA.jpeg","originalWidth":1312,"originalHeight":1268}},{"name":"96b0","type":3,"text":"Value learning","markups":[]},{"name":"1fbd","type":1,"text":"Next, we go to another major RL method called Value Learning. In playing a GO game, it is very hard to plan the next winning move even the rule of the game is well understood. The exponential growth of possibilities makes it too hard to be solved. When the GO champions play the GO game, they evaluate how good a move is and how good to reach a certain board position. Assume we have a cheat sheet scoring every state:","markups":[]},{"name":"cb9a","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*9MCyxA48Ibc2eLv4VkN0HQ.png","originalWidth":1400,"originalHeight":66}},{"name":"350b","type":1,"text":"We can simply look at the cheat sheet and find what is the next most rewarding state and take the corresponding action.","markups":[]},{"name":"25bd","type":1,"text":"Value function V(s) measures the expected discounted rewards for a state under a policy. Intuitively, it measures the total rewards that you get from a particular state following a specific policy.","markups":[{"type":1,"start":0,"end":14},{"type":2,"start":15,"end":19}]},{"name":"7324","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*mEkv4hfGxRwv6kHGkMgQ5A.png","originalWidth":1400,"originalHeight":66}},{"name":"294e","type":1,"text":"In our cart-pole example, we can use the pole stay-up time to measure the rewards. Below, there is a better chance to maintain the pole upright for the state s1 than s2 (better to be in the position on the left below than the right). For most policies, the state on the left is likely to have a higher value function.","markups":[{"type":2,"start":158,"end":161},{"type":2,"start":165,"end":169}]},{"name":"ebc0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*QsLkYHRrppgaqNiuI5GB-g.png","originalWidth":1600,"originalHeight":578}},{"name":"4f68","type":1,"text":"So how to find out V? One method is the Monte Carlo method. We run the policy and play out the whole episode until the end to observe the total rewards. For example, we time how long the pole stays up. Then we have multiple Monte Carlo rollouts and we average the results for V.","markups":[{"type":1,"start":19,"end":20},{"type":1,"start":40,"end":51},{"type":1,"start":276,"end":277},{"type":2,"start":19,"end":20},{"type":2,"start":276,"end":277}]},{"name":"30b6","type":1,"text":"There are a few ways to find the corresponding optimal policy. In policy evaluation, we can start with a random policy and evaluate how good each state is. After many iterations, we use V(s) to decide the next best state. Then, we use the model to determine the action that leads us there. For the GO game, this is simple since the rule of the game is known.","markups":[{"type":1,"start":66,"end":83},{"type":2,"start":186,"end":190}]},{"name":"2c21","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*RIBB9aho7mZZJYw31oVCTQ.jpeg","originalWidth":1700,"originalHeight":820}},{"name":"63d2","type":1,"text":"Alternatively, after each policy evaluation, we improve the policy based on the value function. We continue the evaluation and refinement. Eventually, we will reach the optimal policy. This is called policy iteration.","markups":[{"type":1,"start":200,"end":216}]},{"name":"f0d8","type":4,"text":"Modified from source","markups":[{"type":3,"start":14,"end":20,"href":"https://mitpress.mit.edu/books/reinforcement-learning-second-edition","title":"","rel":"nofollow noopener noopener noopener noopener noopener noopener noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*hdJrrav0zRMjzLsiMRFAjA.jpeg"}},{"name":"6e4c","type":1,"text":"But there is a problem if we do not have the model. We do not know what action can take us to the target state.","markups":[]},{"name":"f16a","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*RseWZkNGpAdPsoaKzNxeFQ.png","originalWidth":1591,"originalHeight":460}},{"name":"fd8e","type":1,"text":"Value function is not a model-free method. We need a model to make decisions. But as an important footnote, even when the model is unknown, value function is still helpful in complementing other RL methods that do not need a model.","markups":[]},{"name":"1a72","type":1,"text":"Value iteration with Dynamic programming","markups":[{"type":1,"start":0,"end":40}]},{"name":"914e","type":1,"text":"Other than the Monte Carlo method, we can use dynamic programming to compute V. We take an action, observe the reward and compute it with the V-value of the next state:","markups":[{"type":1,"start":77,"end":78},{"type":2,"start":77,"end":78},{"type":2,"start":142,"end":143}]},{"name":"2209","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*YjQoGPeaKHvLctl3WgCHqg.png","originalWidth":1200,"originalHeight":75}},{"name":"f61b","type":1,"text":"The exact formula should be:","markups":[]},{"name":"4e5e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_P5kRg0K5nx0NHw5OzKRkg.png","originalWidth":1700,"originalHeight":134}},{"name":"c042","type":1,"text":"If the model is unknown, we compute V by sampling. We execute the action and observe the reward and the next state instead.","markups":[{"type":2,"start":36,"end":37}]},{"name":"4cc3","type":3,"text":"Function fitting","markups":[]},{"name":"2417","type":1,"text":"However, maintain V for every state is not feasible for many problems. To solve that, we use supervised learning to train a deep network that approximates V.","markups":[{"type":2,"start":18,"end":19},{"type":2,"start":155,"end":156}]},{"name":"83d9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*e5F37FBP-x-65z61jtJiXg.jpeg"}},{"name":"86b6","type":1,"text":"y above is the target value and we can use the Monte Carlo method to compute it.","markups":[{"type":1,"start":0,"end":1},{"type":2,"start":0,"end":1}]},{"name":"d68d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2CHTa8r4m5kFQ-Cpbz0O0A.png","originalWidth":1600,"originalHeight":160}},{"name":"a44f","type":1,"text":"Otherwise, we can apply the dynamic programming concept and use a one-step lookahead. This is called Temporal Difference TD.","markups":[{"type":1,"start":101,"end":120}]},{"name":"0b8b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*YjQoGPeaKHvLctl3WgCHqg.png","originalWidth":1200,"originalHeight":75}},{"name":"38d6","type":1,"text":"We take a single action and use the observed reward and the V value for the next state to compute V(s).","markups":[{"type":2,"start":60,"end":61},{"type":2,"start":98,"end":102}]},{"name":"51b4","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*PYlL_2_2VGE2bl4I8SloQg.png","originalWidth":1600,"originalHeight":634}},{"name":"8274","type":1,"text":"The Monte Carlo method is accurate. But for a stochastic policy or a stochastic model, every run may have different results. So the variance is high. TD considers far fewer actions to update its value. So the variance is low. But at least in early training, the bias is very high. High bias gives wrong results but high variance makes the model very hard to converge. In practice, we can combine the Monte Carlo and TD with different k-step lookahead to form the target. This balances the bias and the variance which can stabilize the training.","markups":[{"type":2,"start":434,"end":435}]},{"name":"16cb","type":3,"text":"Action-value function","markups":[{"type":1,"start":0,"end":21}]},{"name":"b495","type":1,"text":"So can we use the value learning concept without a model? Yes, we can avoid the model by scoring an action instead of a state. Action-value function Q(s, a) measures the expected discounted rewards of taking an action. The tradeoff is we have more data to track. For each state, if we can take k actions, there will be k Q-values.","markups":[{"type":2,"start":149,"end":156},{"type":2,"start":294,"end":295},{"type":2,"start":319,"end":322}]},{"name":"19a4","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*k5JZbCNNJC4RFdtpj_wgFg.png","originalWidth":1486,"originalHeight":536}},{"name":"98e2","type":1,"text":"For optimal result, we take the action with the highest Q-value.","markups":[{"type":2,"start":56,"end":57}]},{"name":"3441","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*iRUppfcGcHpa9W_pcD2uAw.png","originalWidth":1200,"originalHeight":67}},{"name":"81a0","type":1,"text":"As shown, we do not need a model to find the optimal action. Hence, Action-value learning is model-free. Which acton below has a higher Q-value? Intuitively, moving left at the state below should have a higher value than moving right.","markups":[]},{"name":"6ad9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*TuPcSwlIIelgv--DB6DKGg.png","originalWidth":1600,"originalHeight":550}},{"name":"96ec","type":1,"text":"In deep learning, gradient descent works better when features are zero-centered. Intuitively, in RL, the absolute rewards may not be as important as how well an action does compare with the average action. That is the concept of the advantage function A. In many RL methods, we use A instead of Q.","markups":[{"type":1,"start":233,"end":251},{"type":1,"start":252,"end":253},{"type":2,"start":252,"end":253},{"type":2,"start":282,"end":283},{"type":2,"start":295,"end":296}]},{"name":"e304","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*PLMQ5z0EwH4R0RJiIn7snw.png","originalWidth":1500,"originalHeight":66}},{"name":"3ab0","type":1,"text":"where A is the expected rewards over the average actions. To recap, here are all the definitions:","markups":[{"type":1,"start":6,"end":7},{"type":2,"start":6,"end":7}]},{"name":"63c6","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*mEkv4hfGxRwv6kHGkMgQ5A.png","originalWidth":1400,"originalHeight":66}},{"name":"9d2d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*DB47b4EY0cVXZhWZ-D6gRg.png","originalWidth":1400,"originalHeight":72}},{"name":"cd02","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*xLyVqfy4yrPn9jPJcYrPbA.png","originalWidth":1500,"originalHeight":74}},{"name":"6d54","type":3,"text":"Q-learning","markups":[]},{"name":"cba2","type":1,"text":"So how can we learn the Q-value? One of the most popular methods is the Q-learning with the following steps:","markups":[]},{"name":"f7f0","type":10,"text":"We sample an action.","markups":[]},{"name":"f9f9","type":10,"text":"We observed the reward and the next state.","markups":[]},{"name":"03e9","type":10,"text":"We take the action with the highest Q.","markups":[]},{"name":"b6ad","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*NUFoDSKWxRQm7uqhiq6Xcg.jpeg","originalWidth":1600,"originalHeight":333}},{"name":"e56f","type":1,"text":"Then we apply the dynamic programming again to compute the Q-value function iteratively:","markups":[{"type":2,"start":59,"end":60}]},{"name":"b2f8","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*bJmVfEJ98XitkjW1z3TN7w.png","originalWidth":1450,"originalHeight":97}},{"name":"8d20","type":1,"text":"Here is the algorithm of Q-learning with function fitting. Step 2 below reduces the variance by using Temporal Difference. This also improves the sample efficiency comparing with the Monte Carlo method which takes samples until the end of the episode.","markups":[{"type":1,"start":146,"end":163},{"type":2,"start":25,"end":26}]},{"name":"c55a","type":4,"text":"Modified from source","markups":[{"type":3,"start":14,"end":20,"href":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html","title":"","rel":"noopener nofollow noopener noopener noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*-MarQ95RdLWkj8p1upYCDA.jpeg","originalWidth":1600,"originalHeight":530}},{"name":"00ec","type":1,"text":"Exploration is very important in RL. Without exploration, you will never know what is better ahead. But if it is overdone, we are wasting time. In Q-learning, we have an exploration policy, like epsilon-greedy, to select the action taken in step 1. We pick the action with highest Q value but yet we allow a small chance of selecting other random actions. Q is initialized with zero. Hence, there is no specific action standing out in early training. As the training progress, more promising actions are selected and the training shift from exploration to exploitation.","markups":[{"type":2,"start":147,"end":148},{"type":2,"start":281,"end":282},{"type":2,"start":356,"end":357}]},{"name":"6a34","type":3,"text":"Deep Q-network DQN","markups":[]},{"name":"61fd","type":1,"text":"Q-learning is unfortunately not very stable with deep learning. In this section, we will finally put all things together and introduce the DQN which beats the human in playing some of the Atari Games by accessing the image frames only.","markups":[{"type":2,"start":0,"end":1}]},{"name":"cd5d","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*l9dL-vOijkkHclYR1tOftA.png","originalWidth":2140,"originalHeight":392}},{"name":"58d9","type":1,"text":"DQN is the poster child for Q-learning using a deep network to approximate Q. We use supervised learning to fit the Q-value function. We want to duplicate the success of supervised learning but RL is different. In deep learning, we randomize the input samples so the input class is quite balanced and pretty stable across training batches. In RL, we search better as we explore more. So the input space and actions we searched are constantly changing. In addition, as we know better, we update the target value of Q. That is bad news. Both the input and output are under frequent changes.","markups":[{"type":2,"start":28,"end":29},{"type":2,"start":75,"end":77},{"type":2,"start":116,"end":117},{"type":2,"start":514,"end":515}]},{"name":"d7a6","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*1sAPBjetPifL0F9b9ahSsQ.jpeg","originalWidth":1900,"originalHeight":200}},{"name":"7d13","type":1,"text":"This makes it very hard to learn the Q-value approximator. DQN introduces experience replay and target network to slow down the changes so we can learn Q gradually. Experience replay stores the last million of state-action-reward in a replay buffer. We train Q with batches of random samples from this buffer. Therefore, the training samples are randomized and behave closer to the supervised learning in Deep Learning.","markups":[{"type":2,"start":37,"end":39},{"type":2,"start":152,"end":153},{"type":2,"start":259,"end":260}]},{"name":"fccf","type":1,"text":"In addition, we have two networks for storing the values of Q. One is constantly updated while the second one, the target network, is synchronized from the first network once a while. We use the target network to retrieve the Q value such that the changes for the target value are less volatile. Here is the objective for those interested. D is the replay buffer and θ- is the target network.","markups":[{"type":1,"start":60,"end":61},{"type":1,"start":226,"end":227},{"type":2,"start":60,"end":61},{"type":2,"start":226,"end":227},{"type":2,"start":340,"end":341},{"type":2,"start":367,"end":370}]},{"name":"b43a","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*3PNz2dU6Un5UW_YSmuvWvQ.jpeg","originalWidth":1400,"originalHeight":219}},{"name":"46d4","type":1,"text":"DQN allows us to use value learning to solve RL methods in a more stable training environment.","markups":[]},{"name":"ebad","type":3,"text":"Policy-Gradient","markups":[{"type":1,"start":0,"end":15},{"type":2,"start":0,"end":15}]},{"name":"f18c","type":1,"text":"So far we have covered two major RL methods: model-based and value learning. Model-based RL uses the model and the cost function to find the optimal path. Value learning uses V or Q value to derive the optimal policy. Next, we will cover the third major RL method, also one of the popular ones in RL. The Policy Gradient method focuses on the policy.","markups":[{"type":2,"start":175,"end":176},{"type":2,"start":180,"end":181}]},{"name":"fbe5","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*eNkhTPvTUaiN6-QDklGJfw.jpeg","originalWidth":1424,"originalHeight":367}},{"name":"17a4","type":1,"text":"Many of our actions, in particular with human motor controls, are very intuitive. We observe and act rather than plan it thoroughly or take samples for maximum returns.","markups":[]},{"name":"620c","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Dof34Ien0I0FOc-sOjcv3A.jpeg","originalWidth":1600,"originalHeight":572}},{"name":"0f76","type":1,"text":"In the cart-pole example, we may not know the physics but when the pole falls to the left, our experience tells us to move left. Determining actions based on observations can be much easier than understanding a model.","markups":[]},{"name":"c0dc","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*5gL04Bfqf4iXBCK1fLB8tg.png","originalWidth":1400,"originalHeight":55}},{"name":"e22e","type":1,"text":"This type of RL methods is policy-based which we model a policy parameterized by θ directly.","markups":[{"type":1,"start":27,"end":39},{"type":2,"start":81,"end":83}]},{"name":"10d2","type":1,"text":"The concept for Policy Gradient is very simple. For actions with better rewards, we make it more likely to happen (or vice versa). The policy gradient is computed as:","markups":[]},{"name":"c855","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*knNdtuW11Rv_U2Gfy7G9Ww.png","originalWidth":1600,"originalHeight":171}},{"name":"8ee3","type":1,"text":"We use this gradient to update the policy using gradient ascent. i.e. we change the policy in the direction with the steepest reward increase.","markups":[]},{"name":"838f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*bTd1OpnOSpFK6V6IFk-kJA.png","originalWidth":1400,"originalHeight":56}},{"name":"1fa2","type":1,"text":"Let’s look at the policy gradient closely. The one underlines in red above is the maximum likelihood. It measures the likelihood of an action under the specific policy. As we multiply it with the advantage function, we change the policy to favor actions with rewards greater than the average action. Or vice versa, we reduce the chance if it is not better off.","markups":[]},{"name":"66a2","type":1,"text":"But we need to be very careful in making such policy change. The gradient method is a first-order derivative method. It is not too accurate if the reward function has steep curvature. If our policy change is too aggressive, the estimate policy improvement may be too far off that the decision can be a disaster.","markups":[]},{"name":"9b04","type":4,"text":"Image source","markups":[{"type":3,"start":6,"end":12,"href":"https://pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/","title":"","rel":"noopener nofollow noopener noopener noopener noopener noopener noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*a13NrOnCClT6uLmGYiYSKw.jpeg","originalWidth":1400,"originalHeight":557}},{"name":"0d04","type":1,"text":"To address this issue, we impose a trust region. We pick the optimal control within this region only. By establishing an upper bound of the potential error, we know how far we can go before we get too optimistic and the potential error can kill us. Within the trust region, we have a reasonable guarantee that the new policy will be better off. Outside the trust region, the bet is off. If we force it, we may land in states that are much worse and destroy the training progress. TRPO and PPO are methods using the trust region concept to improve the convergence of the policy model.","markups":[]},{"name":"f819","type":3,"text":"Actor-critic method","markups":[]},{"name":"d348","type":1,"text":"Policy Gradient methods use a lot of samples to reach an optimal solution. Every time the policy is updated, we need to resample. Similar to other deep learning methods, it takes many iterations to compute the model. Its convergence is often a major concern. Can we use fewer samples to compute the policy gradient? Can we further reduce the variance of A to make the gradient less volatile?","markups":[{"type":2,"start":354,"end":356}]},{"name":"82ef","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*al_zhSPHusgnd_IT8UY6Pg.png","originalWidth":1600,"originalHeight":295}},{"name":"41d5","type":1,"text":"RL methods are rarely mutually exclusive. We mix different approaches to complement each other. Actor-critic combines the policy gradient with function fitting. In the Actor-critic method, we use the actor to model the policy and the critic to model V. By introduce a critic, we reduce the number of samples to collect for each policy update. We don’t collect all samples until the end of an episode. This Temporal Difference technique also reduce variance.","markups":[{"type":1,"start":200,"end":205},{"type":1,"start":234,"end":240},{"type":2,"start":250,"end":251}]},{"name":"ff87","type":1,"text":"The algorithm of actor-critic is very similar to the policy gradient method. In step 2 below, we are fitting the V-value function, that is the critic. In step 3, we use TD to calculate A. In step 5, we are updating our policy, the actor.","markups":[{"type":1,"start":185,"end":186},{"type":2,"start":113,"end":114},{"type":2,"start":185,"end":186}]},{"name":"c2a9","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"http://rail.eecs.berkeley.edu/deeprlcourse/","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*JXFq4joINPlTANeLjymgGg.png","originalWidth":1456,"originalHeight":346}},{"name":"d01e","type":3,"text":"Guide Policy Search","markups":[]},{"name":"be46","type":1,"text":"The actor-critic mixes the value-learning with policy gradient. Again, we can mix Model-based and Policy-based methods together. We use model-based RL to improve a controller and run the controller on a robot to make moves. A controller determines the best action based on the results of the trajectory optimization.","markups":[]},{"name":"9ee9","type":1,"text":"We observe the trajectories and in parallel, we use the generated trajectories to train a policy (the right figure below) using supervised learning.","markups":[]},{"name":"186e","type":4,"text":"Modified from source","markups":[{"type":3,"start":14,"end":20,"href":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_10_imitating_optimal_control.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*pypliLWLL9KOi0uSi9bDSw.jpeg","originalWidth":2103,"originalHeight":729}},{"name":"3576","type":1,"text":"Why we train a policy when we have a controller? Do they serve the same purpose in predicting the action from a state anyway? That comes to the question of whether the model or the policy is simpler.","markups":[]},{"name":"315c","type":1,"text":"Model-based learning can produce pretty accurate trajectories but may generate inconsistent results for areas where the model is complex and not well trained. Its accumulated errors can hurt also. If the policy is simpler, it should be easier to learn and to generalize. We can use supervised learning to eliminate the noise in the model-based trajectories and discover the fundamental rules behind them.","markups":[]},{"name":"1694","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*op6zYpZATTaaB2VaxtrwrA.jpeg","originalWidth":3000,"originalHeight":856}},{"name":"c62e","type":1,"text":"However, policy-gradient is similar to a trial-and-error method with smarter and educated searches. The training usually has a long warm-up period before seeing any actions that make sense.","markups":[]},{"name":"7ac4","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*dd9kDFfsbUePj8JzcyaAew.png","originalWidth":2000,"originalHeight":790}},{"name":"b12b","type":1,"text":"So we combine both of their strength in the Guided Policy Search. We use model-based RL to guide the search better. Then we use the trajectories to train a policy that can generalize better (if the policy is simpler for the task).","markups":[]},{"name":"d713","type":1,"text":"We train both controller and policy in an alternate step. To avoid aggressive changes, we apply the trust region between the controller and the policy again. So the policy and controller are learned in close steps. This helps the training to converge better.","markups":[]},{"name":"8ba5","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"http://rail.eecs.berkeley.edu/deeprlcourse/","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*KN1b1nCagEJl7Ew3_ciKag.jpeg","originalWidth":1600,"originalHeight":549}},{"name":"8ef9","type":3,"text":"Deep Learning","markups":[]},{"name":"2527","type":1,"text":"What is the role of Deep Learning in reinforcement learning? As mentioned before, deep learning is the eye and the ear. We apply CNN to extract features from images and RNN for voices.","markups":[]},{"name":"df95","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*01ts7lX7lCigVEdUJhqQ7A.jpeg","originalWidth":1134,"originalHeight":232}},{"name":"c1d9","type":1,"text":"A deep network is also a great function approximator. We can use it to approximate any functions we needed in RL. This includes V-value, Q-value, policy, and model.","markups":[{"type":2,"start":128,"end":129},{"type":2,"start":137,"end":138}]},{"name":"601f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cV6MFkLt1M9dv9DZ-TFNJQ.jpeg","originalWidth":1332,"originalHeight":238}},{"name":"96bc","type":1,"text":"Partially Observable MDP","markups":[{"type":1,"start":0,"end":24}]},{"name":"6047","type":1,"text":"For many problems, objects can be temporarily obstructed by others. To construct the state of the environment, we need more than the current image. For a Partially Observable MDP, we construct states from the recent history of images. This can be done by applying RNN on a sequence of images.","markups":[]},{"name":"0977","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*hAExkREX6IGXQ1X1oOp34A.jpeg","originalWidth":1800,"originalHeight":1000}},{"name":"301b","type":3,"text":"Which methods to use?","markups":[]},{"name":"2d1d","type":1,"text":"We have introduced three major groups of RL methods. We can mix and match methods to complement each other and there are many improvements made to each method. Which methods are the best?","markups":[]},{"name":"5b85","type":1,"text":"This will be impossible to explain within a single section. Each method has its strength and weakness. Model-based RL has the best sample efficiency so far but Model-free RL may have better optimal solutions under the current state of the technology. Sometimes, we can view it more like fashion. What is the best will depend on the year you ask? Research makes progress and out-of-favor methods may have a new lifeline after some improvements. In reality, we mix and match for RL problems. The desired method is strongly restricted by constraints, the context of the task and the progress of the research. For example, robotic controls strongly favor methods with high sample efficient. Physical simulations cannot be replaced by computer simulations easily. But yet in some problem domains, we can now bridge the gap or introduce self-learning better. In short, we are still in a highly evolving field and therefore there is no golden guideline yet. We can only say at the current state, what method may be better under the constraints and the context of your task. Stay tuned and we will have more detail discussion on this.","markups":[]},{"name":"d419","type":3,"text":"Thoughts","markups":[]},{"name":"616f","type":1,"text":"Deep reinforcement learning is about how we make decisions. In this article, we cover three basic algorithm groups namely, model-based RL, value learning and policy gradients. This is just a start. How to learn as efficiently as the human remains challenging. The bad news is there is a lot of room to improve for commercial applications. The good news is the problem is so hard but important that don’t expect that the subject is boring. In this article, we explore the basic but hardly touch its challenge and many innovative solutions that have been proposed. It is one of the hardest areas in AI but probably one of the hardest parts of daily life also. For those want to explore more, here are the articles detailing different RL areas.","markups":[]},{"name":"6f66","type":14,"text":"Deep Reinforcement Learning Series\nDeep Reinforcement Learning is about making the best decisions for what we see and what we hear. It sounds simple but…medium.com","markups":[{"type":3,"start":0,"end":163,"href":"https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530","title":"https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530","rel":"","anchorType":0},{"type":1,"start":0,"end":34},{"type":2,"start":35,"end":153}],"mixtapeMetadata":{"mediaResourceId":"87dee65f484fc4931e718f4936d5e484","thumbnailImageId":"0*H5oS37AeAD6JYR93","href":"https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530"}}],"sections":[{"name":"7278","startIndex":0}]},"postDisplay":{"coverless":true}},"virtuals":{"allowNotes":true,"previewImage":{"imageId":"0*JjsLueA2KATdLD5Z","filter":"","backgroundSize":"","originalWidth":2048,"originalHeight":1524,"strategy":"resample","height":0,"width":0},"wordCount":4034,"imageCount":66,"readingTime":19.272641509433964,"subtitle":"Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a…","usersBySocialRecommends":[],"noIndex":false,"recommends":316,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":76459,"metadata":{"postCount":76459,"coverImage":{"id":"1*Objn6iYe6g4-DLDV67JKWA.jpeg","originalWidth":1904,"originalHeight":1068,"isFeatured":true}},"type":"Tag"},{"slug":"artificial-intelligence","name":"Artificial Intelligence","postCount":87780,"metadata":{"postCount":87780,"coverImage":{"id":"1*gAn_BSffVBcwCIR6bDgK1g.jpeg"}},"type":"Tag"},{"slug":"data-science","name":"Data Science","postCount":50848,"metadata":{"postCount":50848,"coverImage":{"id":"1*Objn6iYe6g4-DLDV67JKWA.jpeg","originalWidth":1904,"originalHeight":1068,"isFeatured":true}},"type":"Tag"},{"slug":"programming","name":"Programming","postCount":103716,"metadata":{"postCount":103716,"coverImage":{"id":"1*AmeD38st98PaTMaxfFKc1A.png","originalWidth":1920,"originalHeight":1080,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":18762,"metadata":{"postCount":18762,"coverImage":{"id":"1*uyQ61XHkTY2qHhovclaLIQ.png","originalWidth":1073,"originalHeight":741,"isFeatured":true}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":10,"links":{"entries":[{"url":"https://mitpress.mit.edu/books/reinforcement-learning","alts":[],"httpStatus":200},{"url":"https://pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/","alts":[{"type":1,"url":"https://cdn.ampproject.org/c/s/pronetowanderfriends.wordpress.com/2014/02/23/angels-landing-in-zion-national-park-check/amp/"}],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750","alts":[{"type":2,"url":"medium://p/a5de5104c750"},{"type":3,"url":"medium://p/a5de5104c750"}],"httpStatus":200},{"url":"https://unsplash.com/@fossy?utm_source=medium&utm_medium=referral","alts":[],"httpStatus":200},{"url":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html","alts":[],"httpStatus":200},{"url":"http://rail.eecs.berkeley.edu/deeprlcourse/","alts":[],"httpStatus":200},{"url":"https://drive.google.com/file/d/0BxXI_RttTZAhVXBlMUVkQ1BVVDQ/view","alts":[],"httpStatus":200},{"url":"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf","alts":[],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530","alts":[{"type":2,"url":"medium://p/833319a95530"},{"type":3,"url":"medium://p/833319a95530"}],"httpStatus":200},{"url":"https://arxiv.org/pdf/1504.00702.pdf","alts":[],"httpStatus":200},{"url":"https://mitpress.mit.edu/books/reinforcement-learning-second-edition","alts":[],"httpStatus":200},{"url":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_10_imitating_optimal_control.pdf","alts":[],"httpStatus":200},{"url":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf","alts":[],"httpStatus":200},{"url":"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1546819640235},"isLockedPreviewOnly":false,"metaDescription":"","totalClapCount":1599,"sectionCount":1,"readingList":0,"topics":[{"topicId":"1eca0103fff3","slug":"machine-learning","createdAt":1534449726145,"deletedAt":0,"image":{"id":"1*gFJS3amhZEg_z39D5EErVg@2x.png","originalWidth":2800,"originalHeight":1750},"name":"Machine Learning","description":"Teaching the learners.","relatedTopics":[],"visibility":1,"relatedTags":[],"relatedTopicIds":[],"type":"Topic"},{"topicId":"ae5d4995e225","slug":"data-science","createdAt":1493923906289,"deletedAt":0,"image":{"id":"1*NHWOEki_ncCX-xzbKtkEWw@2x.jpeg","originalWidth":5760,"originalHeight":3840},"name":"Data Science","description":"Query this.","relatedTopics":[],"visibility":1,"relatedTags":[],"relatedTopicIds":[],"type":"Topic"}]},"coverless":true,"slug":"rl-introduction-to-deep-reinforcement-learning","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":false,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"rl-introduction-to-deep-reinforcement-learning-35c25e04c199","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"0*JjsLueA2KATdLD5Z","originalWidth":2048,"originalHeight":1524,"isFeatured":true}},{"name":"c858","type":3,"text":"RL— Introduction to Deep Reinforcement Learning","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"Deep reinforcement learning is about taking the best actions from what we see and hear. Unfortunately, reinforcement learning RL has a…"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199","approvedHomeCollectionId":"","newsletterId":"","webCanonicalUrl":"https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199","mediumUrl":"https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":3,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"primaryTopic":{"topicId":"1eca0103fff3","slug":"machine-learning","createdAt":1534449726145,"deletedAt":0,"image":{"id":"1*gFJS3amhZEg_z39D5EErVg@2x.png","originalWidth":2800,"originalHeight":1750},"name":"Machine Learning","description":"Teaching the learners.","relatedTopics":[],"visibility":1,"relatedTags":[],"relatedTopicIds":[],"type":"Topic"},"primaryTopicId":"1eca0103fff3","isProxyPost":false,"proxyPostFaviconUrl":"","proxyPostProviderName":"","proxyPostType":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"hideMeter":false,"collectionUserRelations":[],"mode":null,"references":{"User":{"bd51f1a63813":{"userId":"bd51f1a63813","name":"Jonathan Hui","username":"jonathan_hui","createdAt":1518417490123,"imageId":"1*c3Z3aOPBooxEX4tx4RkzLw.jpeg","backgroundImageId":"","bio":"Deep Learning","twitterScreenName":"","socialStats":{"userId":"bd51f1a63813","usersFollowedCount":17,"usersFollowedByCount":8947,"type":"SocialStats"},"social":{"userId":"be025283e717","targetUserId":"bd51f1a63813","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Social":{"bd51f1a63813":{"userId":"be025283e717","targetUserId":"bd51f1a63813","type":"Social"}},"SocialStats":{"bd51f1a63813":{"userId":"bd51f1a63813","usersFollowedCount":17,"usersFollowedByCount":8947,"type":"SocialStats"}}}})
// ]]></script><script>window.PARSELY = window.PARSELY || { autotrack: false }</script><script id="parsely-cfg" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/p.js.download"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script><div id="weava-permanent-marker" date="1562635683512"></div><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/main-common-async.bundle.cUeGLoj7sHNpuw-bFOuTeQ.js.download"></script><script charset="UTF-8" src="./RL— Introduction to Deep Reinforcement Learning – Jonathan Hui – Medium_files/main-notes.bundle.RzL89au1QllN2FPu92pO7w.js.download"></script></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>