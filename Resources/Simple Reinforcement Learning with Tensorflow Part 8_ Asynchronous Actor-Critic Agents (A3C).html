<!DOCTYPE html>
<!-- saved from url=(0137)https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2 -->
<html lang="en" data-rh="lang" class="gr__medium_com"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/branch-latest.min.js.download"></script><script async="" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/analytics.js.download"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2017-06-30T17:53:11.876Z"><meta data-rh="true" name="title" content="Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)"><meta data-rh="true" property="og:title" content="Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)"><meta data-rh="true" property="twitter:title" content="Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)"><meta data-rh="true" name="twitter:site" content="@emergentfuture"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/c88f72a5e9f2"><meta data-rh="true" property="al:android:url" content="medium://p/c88f72a5e9f2"><meta data-rh="true" property="al:ios:url" content="medium://p/c88f72a5e9f2"><meta data-rh="true" name="apple-itunes-app" content="app-id=828256236,app-argument=medium://p/c88f72a5e9f2"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom…"><meta data-rh="true" property="og:description" content="In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will…"><meta data-rh="true" property="twitter:description" content="In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will…"><meta data-rh="true" property="og:url" content="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"><meta data-rh="true" property="al:web:url" content="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*odZkJyxANWf6pofV9D17hw.jpeg"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*odZkJyxANWf6pofV9D17hw.jpeg"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="/@awjuliani"><meta data-rh="true" name="twitter:creator" content="@awjuliani"><meta data-rh="true" name="author" content="Arthur Juliani"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="8 min read"><link data-rh="true" rel="publisher" href="https://plus.google.com/103654360130207659246"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/m2.css"><link data-rh="true" rel="author" href="https://medium.com/@awjuliani"><link data-rh="true" rel="canonical" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><style type="text/css" data-fela-rehydration="333" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.n{display:block}.o{position:fixed}.p{top:0}.q{left:0}.r{right:0}.s{z-index:500}.t{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.u{transition:transform 300ms ease}.v{will-change:transform}.w{padding-left:24px}.x{padding-right:24px}.y{margin-left:auto}.z{margin-right:auto}.ab{height:65px}.ac{width:100%}.ae{max-width:1080px}.af{box-sizing:border-box}.ag{display:flex}.ah{align-items:center}.ak{flex:1 0 auto}.al{margin-left:-6px}.am{fill:rgba(0, 0, 0, 0.84)}.an{flex:0 0 auto}.ao{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.ap{font-style:normal}.aq{line-height:20px}.ar{font-size:15.8px}.as{letter-spacing:0px}.at{color:rgba(0, 0, 0, 0.54)}.au{fill:rgba(0, 0, 0, 0.54)}.av{color:inherit}.aw{fill:inherit}.ax{font-size:inherit}.ay{border:inherit}.az{font-family:inherit}.ba{letter-spacing:inherit}.bb{font-weight:inherit}.bc{padding:0}.bd{margin:0}.be:hover{cursor:pointer}.bf:hover{color:rgba(0, 0, 0, 0.9)}.bg:hover{fill:rgba(0, 0, 0, 0.9)}.bh:focus{outline:none}.bi:disabled{cursor:default}.bj:disabled{color:rgba(0, 0, 0, 0.54)}.bk:disabled{fill:rgba(0, 0, 0, 0.54)}.bl{margin-left:16px}.bm{margin-right:16px}.bp{padding:4px 12px}.bq{color:rgba(0, 0, 0, 0.84)}.br{background:0}.bs{border-color:rgba(0, 0, 0, 0.54)}.bt:hover{color:rgba(0, 0, 0, 0.97)}.bu:hover{fill:rgba(0, 0, 0, 0.97)}.bv:hover{border-color:rgba(0, 0, 0, 0.84)}.bw{border-radius:4px}.bx{border-width:1px}.by{border-style:solid}.bz{display:inline-block}.ca{text-decoration:none}.cb{padding-bottom:10px}.cc{padding-top:10px}.cd{border-radius:50%}.ce{height:32px}.cf{width:32px}.cg{border-top:1px solid rgba(0, 0, 0, 0.1)}.ci{height:54px}.cj{overflow:hidden}.ck{margin-right:40px}.cl{width:240px}.cm{height:36px}.cn{overflow:auto}.co{flex:0 1 auto}.cp{list-style-type:none}.cq{line-height:40px}.cr{white-space:nowrap}.cs{overflow-x:auto}.ct{align-items:flex-start}.cu{margin-top:20px}.cv{padding-top:20px}.cw{height:80px}.cx{height:20px}.cy{margin-right:15px}.cz{margin-left:15px}.da:first-child{margin-left:0}.db{min-width:1px}.dc{background-color:rgba(0, 0, 0, 0.34)}.dd{font-weight:300}.de{font-size:15px}.df{line-height:21px}.dg{text-transform:uppercase}.dh{letter-spacing:1px}.di{margin-bottom:0px}.dj{height:119px}.dm{padding-bottom:1px}.dn{margin-top:40px}.do{max-width:728px}.dp{opacity:0}.dq{pointer-events:none}.dr{will-change:opacity}.ds{transition:opacity 200ms}.dt{width:131px}.du{left:50%}.dv{transform:translateX(-516px)}.dw{top:calc(65px + 54px + 40px)}.dx{flex-direction:column}.dy{padding-bottom:28px}.dz{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.ea{font-weight:600}.eb{font-size:18px}.ec{padding-bottom:20px}.ed{padding-top:2px}.ee{font-size:16px}.ef{max-height:120px}.eg{text-overflow:ellipsis}.eh{display:-webkit-box}.ei{-webkit-line-clamp:6}.ej{-webkit-box-orient:vertical}.ek{color:rgba(109, 114, 124, 1)}.el{fill:rgba(127, 133, 145, 1)}.em{border-color:rgba(127, 133, 145, 1)}.en:hover{color:rgba(100, 105, 113, 1)}.eo:hover{fill:rgba(109, 114, 124, 1)}.ep:hover{border-color:rgba(109, 114, 124, 1)}.eq{padding-top:28px}.er{margin-bottom:19px}.es{margin-left:-5px}.et{margin-right:5px}.eu{position:relative}.ev{outline:0}.ew{border:0}.ex{user-select:none}.ey{cursor:pointer}.ez> svg{pointer-events:none}.fa:active{border-style:none}.fb{margin-top:5px}.fc button{text-align:left}.fd{fill:rgba(0, 0, 0, 0.76)}.fe{clear:both}.ff{justify-content:center}.fg{margin-right:8px}.fh{margin-bottom:8px}.fi{border-radius:3px}.fj{padding:5px 10px}.fk{background:rgba(0, 0, 0, 0.05)}.fl{line-height:22px}.fm{justify-content:space-between}.fn{margin-top:15px}.fo{border:1px solid rgba(0, 0, 0, 0.1)}.fp{height:60px}.fq{transition:border-color 150ms ease}.fr{width:60px}.fs::before{background:
      radial-gradient(circle, rgba(109, 114, 124, 1) 60%, transparent 70%)
    }.ft::before{border-radius:50%}.fu::before{content:""}.fv::before{display:block}.fw::before{z-index:0}.fx::before{left:0}.fy::before{height:100%}.fz::before{position:absolute}.ga::before{top:0}.gb::before{width:100%}.gc:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.gd:active{border-style:solid}.ge{background:rgba(255, 255, 255, 1)}.gf{z-index:2}.gg{height:100%}.gh{position:absolute}.gi{padding-right:8px}.gj{display:none}.gk{margin-top:25px}.gl{margin-bottom:25px}.gm{padding-top:32px}.gn{border-top:solid 1px rgba(0, 0, 0, 0.1)}.go{margin-bottom:32px}.gp{min-height:80px}.gu{width:80px}.gv{padding-left:102px}.gx{letter-spacing:0.05em}.gy{margin-bottom:6px}.gz{font-size:28px}.ha{line-height:36px}.hb{max-width:555px}.hc{max-width:450px}.hd{line-height:24px}.hf{max-width:550px}.hg{padding-top:25px}.hh{padding:20px}.hi{border:1px solid rgba(127, 133, 145, 1)}.hj{text-align:center}.hk{margin-top:64px}.hl{background-color:rgba(0, 0, 0, 0.02)}.hm{margin:15px 0}.hn{top:calc(100vh + 100px)}.ho{bottom:calc(100vh + 100px)}.hp{width:10px}.hq{word-break:break-word}.hr{word-wrap:break-word}.hs:after{display:block}.ht:after{content:""}.hu:after{clear:both}.hv{margin:0 auto}.hw{line-height:1.23}.hx{letter-spacing:0}.hy{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.hz{font-size:40px}.if{margin-bottom:-0.27em}.ig{line-height:48px}.ih{margin-top:32px}.ii{height:48px}.ij{width:48px}.ik{margin-left:12px}.il{margin-bottom:2px}.in{max-height:20px}.io{-webkit-line-clamp:1}.ip:hover{text-decoration:underline}.iq{margin-left:8px}.ir{padding:0px 8px}.is{line-height:18px}.iy{transition:opacity 100ms 400ms}.iz{transform:translateZ(0)}.ja{margin:auto}.jb{background-color:rgba(0, 0, 0, 0.05)}.jc{padding-bottom:55.15%}.jd{filter:blur(20px)}.je{transform:scale(1.1)}.jf{line-height:1.58}.jg{letter-spacing:-0.004em}.jh{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.js{margin-bottom:-0.46em}.jt{background-repeat:repeat-x}.ju{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.jv{background-size:1px 1px}.jw{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.jx{line-height:1.18}.jy{letter-spacing:-0.022em}.kj{margin-bottom:-0.31em}.kk{max-width:1392px}.kl{padding-bottom:89.36781609195401%}.km{line-height:1.4}.kn{margin-top:10px}.kp{font-weight:700}.kq{font-style:italic}.kr{box-shadow:inset 3px 0 0 0 rgba(0, 0, 0, 0.84)}.ks{padding-left:23px}.kt{margin-left:-20px}.ku{max-width:1316px}.kv{padding-bottom:86.93009118541033%}.kw{list-style-type:disc}.kx{margin-left:30px}.ky{padding-left:0px}.kz{padding:2px 4px}.la{font-size:75%}.lb{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.lh{padding-bottom:NaN%}.li{max-width:643px}.lj{padding-bottom:74.65007776049767%}.lk{max-width:811px}.ll{padding-bottom:48.45869297163995%}.lm{background:none}.ln{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.lo{border:none}.lp{margin-top:30px}.lq:before{content:"..."}.lr:before{letter-spacing:0.6em}.ls:before{text-indent:0.6em}.lt:before{font-style:italic}.lu:before{line-height:1.4}.lv{list-style-type:decimal}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (max-width: 1256px)">.d{display:none}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (max-width: 1080px)">.e{display:none}.ko{text-align:center}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (max-width: 904px)">.f{display:none}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (max-width: 728px)">.g{display:none}.ai{height:56px}.aj{display:flex}.bn{margin-left:10px}.bo{margin-right:10px}.ch{display:block}.dk{margin-bottom:0px}.dl{height:110px}.gq{margin-bottom:24px}.gr{align-items:center}.gs{width:102px}.gt{position:relative}.gw{padding-left:0}.he{margin-top:24px}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (max-width: 552px)">.h{display:none}.im{margin-bottom:0px}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (min-width: 1080px)">.i{display:none}.ie{margin-top:0.78em}.ix{margin-top:56px}.jq{font-size:21px}.jr{margin-top:2em}.kh{font-size:26px}.ki{margin-top:1.72em}.lg{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (min-width: 904px) and (max-width: 1079.98px)">.j{display:none}.id{margin-top:0.78em}.iw{margin-top:56px}.jo{font-size:21px}.jp{margin-top:2em}.kf{font-size:26px}.kg{margin-top:1.72em}.lf{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (min-width: 728px) and (max-width: 903.98px)">.k{display:none}.ic{margin-top:0.78em}.iv{margin-top:56px}.jm{font-size:21px}.jn{margin-top:2em}.kd{font-size:26px}.ke{margin-top:1.72em}.le{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (min-width: 552px) and (max-width: 727.98px)">.l{display:none}.ib{margin-top:0.39em}.iu{margin-top:40px}.jk{font-size:18px}.jl{margin-top:1.56em}.kb{font-size:24px}.kc{margin-top:1.23em}.ld{margin-top:1.34em}</style><style type="text/css" data-fela-rehydration="333" data-fela-type="RULE" media="screen and (max-width: 551.98px)">.m{display:none}.ia{margin-top:0.39em}.it{margin-top:40px}.ji{font-size:18px}.jj{margin-top:1.56em}.jz{font-size:24px}.ka{margin-top:1.23em}.lc{margin-top:1.34em}</style><script charset="utf-8" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/vendors_tracing.db265f32.chunk.js.download"></script><script charset="utf-8" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/tracing.48bfc3d4.chunk.js.download"></script><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":108,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F135\u002F1*odZkJyxANWf6pofV9D17hw.jpeg"},"thumbnailUrl":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F135\u002F1*odZkJyxANWf6pofV9D17hw.jpeg","url":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2","dateCreated":"2016-12-17T02:34:44.978Z","datePublished":"2016-12-17T02:34:44.978Z","dateModified":"2018-06-21T02:19:03.572Z","headline":"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)","name":"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)","articleId":"c88f72a5e9f2","keywords":["Lite:true","Tag:Machine Learning","Tag:Artificial Intelligence","Tag:Deep Learning","Tag:Neural Networks","Tag:Robotics","Publication:emergent-future","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Arthur Juliani","url":"https:\u002F\u002Fmedium.com\u002F@awjuliani"},"creator":["Arthur Juliani"],"publisher":{"@type":"Organization","name":"Emergent \u002F\u002F Future","url":"https:\u002F\u002Fmedium.com\u002Femergent-future","logo":{"@type":"ImageObject","width":398,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F498\u002F1*O0r7MC7Yw8UJc2TZoIPJxw.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
  branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body data-gr-c-s-loaded="true"><div id="root"><div class="a b c"><div class="d e f g h i j k l m"></div><nav class="n o p q r c s t u v" style=""><div class="branch-journeys-top"><div class="n c"><section class="w x y z ab ac ae af ag ah ai aj"><div class="ag ah ak s"><div class="al n"><a href="https://medium.com/?source=post_page---------------------------" aria-label="Homepage"><svg width="45" height="45" viewBox="0 0 45 45" class="am"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div></div><div class="n an s"><span class="ao b ap aq ar as n at au"><div class="ag ah"><a href="https://medium.com/search?source=post_page---------------------------" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25" class="bl bm n bn bo"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a><div class="bm ag bo"><a href="https://medium.com/me/activity?source=post_page---------------------------" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n"><svg width="25" height="25" viewBox="-293 409 25 25" class="hm n"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></a></div><div class="bm n g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="bp bq am br bs bt bu bv be bw ao b ap aq ar as bx by af bz ca bh">Upgrade</a></div></div><div class="ag"><div class="cb cc ag ah"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><img alt="Vivek Agrawal" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/0_toB60eKEH3klSlAD.jpg" class="n cd ce cf" width="32" height="32"></button></div></div></div></span></div></section></div><div class="cg n c ch"><section class="w x y z ci ac ae af cj ag ah"><div class="ck n an"><a href="https://medium.com/emergent-future?source=post_page---------------------------"><div class="cl cm"><img alt="Emergent // Future" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_O0r7MC7Yw8UJc2TZoIPJxw.png" class="" width="240" height="36"></div></a></div><div class="cn n co"><ul class="cp bd cq cr cs ag ct g cu cv cw"><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk" href="https://medium.com/emergent-future/machine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129?source=post_page---------------------------">Machine Learning Trends</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk" href="https://medium.com/emergent-future/how-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d?source=post_page---------------------------">Algorithm Economy</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk" href="https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4?source=post_page---------------------------">Why Deep Learning Matters</a></span></li><span class="cx db dc"></span><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://algorithmia.com/?source=post_page---------------------------" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">Algorithmia</a></span></li></ul></div></section></div></div></nav><div class="di dj n dk dl"></div><article><div class="dm dn n"><section class="w x y z ac do af n"></section></div><span class="n"></span><div><div class="gh q hn ho hp dq"></div><div class="y z do eu"><div class="n h g f e"><aside class="ly gh p" style="width: 380.4px;"><div class="mc md gh me cr"><h4 class="ao dd de aq at"><span class="bz md cr cj eg">You highlighted</span></h4></div><div class="mc md gh mf cr"><h4 class="ao dd de aq at"><span class="bz md cr cj eg">Top highlight</span></h4></div></aside></div></div><section class="hq hr hs ht hu"><div class="af hv ac do w x"><div><div id="9fc8" class="hw hx bq ap hy b hz ia ib ic id ie if"><h1 class="hy b hz ig bq">Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)</h1></div><div class="ih"><div class="ah ag"><div><a href="https://medium.com/@awjuliani?source=post_page---------------------------"><img alt="Arthur Juliani" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_kLlWemBAJPjdMTEh5hDtUg.jpeg" class="n cd ii ij" width="48" height="48"></a></div><div class="ik ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="il ag ah im" data-test-id="postByline"><span class="ao dd ee aq cj in eg eh io ej bq"><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/@awjuliani?source=post_page---------------------------">Arthur Juliani</a></span><div class="iq n an h"><button class="ir br ek el em en eo ep be bw ao b ap is de as bx by af bz ca bh">Follow</button></div></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj in eg eh io ej at"><div><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=post_page---------------------------">Dec 17, 2016</a> · 8 min read</div></span></span></div></div></div></div></div><div class="ac"><figure class="it iu iv iw ix fe ac paragraph-image"><div class="ja n eu jb"><div class="jc n"><div class="dp iy gh p q gg ac cj v iz"><img src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_odZkJyxANWf6pofV9D17hw.jpeg" class="gh p q gg ac jd je" width="2000" height="1103"></div><img class="lw lx gh p q gg ac ge" width="2000" height="1103" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_odZkJyxANWf6pofV9D17hw(1).jpeg"><noscript></noscript></div></div></figure></div><div class="af hv ac do w x"><p id="2165" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven’t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the <a class="av ca jt ju jv jw" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=post_page---------------------------">earlier entries</a> in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.</p><p id="bf4c" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">So what is A3C? The <a href="https://arxiv.org/pdf/1602.01783.pdf?source=post_page---------------------------" class="av ca jt ju jv jw">A3C algorithm</a> was released by Google’s DeepMind group earlier this year, and it made a splash by… essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. <mark class="mb ma ey">On top of all that it could work in continuous as well as discrete action spaces</mark>. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI <a href="https://openai.com/blog/universe/?source=post_page---------------------------" class="av ca jt ju jv jw">just released</a> a version of A3C as their “universal starter agent” for working with their new (and very diverse) set of Universe environments.</p><h2 id="ea50" class="jx jy bq ap ao ea jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The 3 As of A3C</h2><figure class="it iu iv iw ix fe kk y z paragraph-image"><div class="ja n eu jb"><div class="kl n"><div class="dp iy gh p q gg ac cj v iz"><img src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_YtnGhtSAMnnHSL8PvS7t_w.png" class="gh p q gg ac jd je" width="700" height="626"></div><img class="lw lx gh p q gg ac ge" width="700" height="626" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_YtnGhtSAMnnHSL8PvS7t_w(1).png"><noscript></noscript></div></div><figcaption class="at ee km kn hj do y z ko ao dd" data-selectable-paragraph="">Diagram of A3C high-level architecture.</figcaption></figure><p id="f76c" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Asynchronous Advantage Actor-Critic is quite a mouthful. Let’s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.</p><p id="de30" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><strong class="jh kp">Asynchronous</strong>: Unlike <a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=post_page---------------------------">DQN</a>, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.</p><p id="0151" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><strong class="jh kp">Actor-Critic</strong>: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function <strong class="jh kp"><em class="kq">V(s)</em></strong> (how good a certain state is to be in) and a policy <strong class="jh kp"><em class="kq">π(s)</em></strong> (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.</p><p id="ba19" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><strong class="jh kp">Advantage</strong>: If we think back to our <a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724?source=post_page---------------------------">implementation of Policy Gradient</a>, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were “good” and which were “bad.” The network was then updated in order to encourage and discourage actions appropriately.</p><blockquote class="kr ks kt"><p id="f881" class="jf jg bq kq jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Discounted Reward: R = γ(r)</p></blockquote><p id="188b" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><mark class="lz ma ey">The insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network’s predictions were lacking.</mark> If you recall from the <a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=post_page---------------------------">Dueling Q-Network architecture</a>, the advantage function is as follow:</p><blockquote class="kr ks kt"><p id="e08d" class="jf jg bq kq jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Advantage: A = Q(s,a) - V(s)</p></blockquote><p id="a856" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Since we won’t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.</p><blockquote class="kr ks kt"><p id="a7f7" class="jf jg bq kq jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Advantage Estimate: A = R - V(s)</p></blockquote><p id="388a" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">In this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as <a href="https://arxiv.org/pdf/1506.02438.pdf?source=post_page---------------------------" class="av ca jt ju jv jw">Generalized Advantage Estimation</a>.</p><h2 id="e225" class="jx jy bq ap ao ea jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Implementing the Algorithm</h2><figure class="it iu iv iw ix fe ku y z paragraph-image"><div class="ja n eu jb"><div class="kv n"><div class="dp iy gh p q gg ac cj v iz"><img src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_Hzql_1t0-wwDxiz0C97AcQ.png" class="gh p q gg ac jd je" width="700" height="609"></div><img class="lw lx gh p q gg ac ge" width="700" height="609" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_Hzql_1t0-wwDxiz0C97AcQ(1).png"><noscript></noscript></div></div><figcaption class="at ee km kn hj do y z ko ao dd" data-selectable-paragraph="">Training workflow of each worker agent in A3C.</figcaption></figure><p id="9288" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><em class="kq">In the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by </em><a href="https://github.com/dennybritz/reinforcement-learning?source=post_page---------------------------" class="av ca jt ju jv jw"><em class="kq">DennyBritz</em></a><em class="kq"> and </em><a href="https://github.com/openai/universe-starter-agent?source=post_page---------------------------" class="av ca jt ju jv jw"><em class="kq">OpenAI</em></a><em class="kq">. Both of which I highly recommend if you’d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won’t run on its own. To view and run the full, functional A3C implementation, see my </em><a href="https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb?source=post_page---------------------------" class="av ca jt ju jv jw"><em class="kq">Github repository</em></a><em class="kq">.</em></p><p id="19c3" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">The general outline of the code architecture is:</p><ul class=""><li id="24a5" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js kw kx ky" data-selectable-paragraph=""><code class="jb kz la lb b"><strong class="jh kp">AC_Network</strong></code><strong class="jh kp"> </strong>— This class contains all the Tensorflow ops to create the networks themselves.</li><li id="c95a" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js kw kx ky" data-selectable-paragraph=""><code class="jb kz la lb b"><strong class="jh kp">Worker</strong></code> — This class contains a copy of <code class="jb kz la lb b">AC_Network</code>, an environment class, as well as all the logic for interacting with the environment, and updating the global network.</li><li id="9f48" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js kw kx ky" data-selectable-paragraph="">High-level code for establishing the <code class="jb kz la lb b">Worker</code> instances and running them in parallel.</li></ul><p id="b66c" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">The A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.</p><figure class="it iu iv iw ix"><div class="ja n eu"><div class="mh n"><iframe src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1a5f68091ea4bd0853b6aaa7f508b056.html" frameborder="0" height="942" width="680" title="a3c.py" class="gh p q gg ac"></iframe></div></div></figure><p id="9226" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Next, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.</p><figure class="it iu iv iw ix"><div class="ja n eu"><div class="mk n"><iframe src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/28c80bc5d3c9808c1e8b87b5480f7d5b.html" frameborder="0" height="601" width="680" title="make_worker.py" class="gh p q gg ac"></iframe></div></div></figure><p id="9113" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><em class="kq">~ From here we go asynchronous ~</em></p><p id="4bcd" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Each worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.</p><figure class="it iu iv iw ix"><div class="ja n eu"><div class="ml n"><iframe src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/27d14957538011cabd7a9a117365b863.html" frameborder="0" height="466" width="680" title="set_parameters.py" class="gh p q gg ac"></iframe></div></div></figure><p id="d267" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Each worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples <em class="kq">(observation, action, reward, done, value)</em> that is constantly added to from interactions with the environment.</p><figure class="it iu iv iw ix"><div class="ja n eu"><div class="mi n"><iframe src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/0ac37f6cb07055fa52505d65b5ae2482.html" frameborder="0" height="1676" width="680" title="work.py" class="gh p q gg ac"></iframe></div></div></figure><p id="835d" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Once the worker’s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (<em class="kq">H</em>) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.</p><blockquote class="kr ks kt"><p id="4b5d" class="jf jg bq kq jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Value Loss: L = Σ(R - V(s))²</p><p id="0924" class="jf jg bq kq jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Policy Loss: L = -log(<strong class="jh kp">π</strong>(s)) * A(s) - β*H(<strong class="jh kp">π</strong>)</p></blockquote><p id="d245" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">A worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.</p><p id="2bdd" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">A worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.</p><figure class="it iu iv iw ix"><div class="ja n eu"><div class="mj n"><iframe src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/920870ee2a4504c11b643835489ea22e.html" frameborder="0" height="1503" width="680" title="train.py" class="gh p q gg ac"></iframe></div></div></figure><p id="fbb0" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Once a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.</p><p id="e345" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">To view the full and functional code, see the Github repository <a href="https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb?source=post_page---------------------------" class="av ca jt ju jv jw">here</a>.</p><h2 id="e1e2" class="jx jy bq ap ao ea jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Playing Doom</h2><figure class="it iu iv iw ix fe li y z paragraph-image"><div class="ja n eu jb"><div class="lj n"><div class="lw lx gh p q gg ac cj v iz"><img src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_yjPVxRywI8U9SKfCMCKR1A.png" class="gh p q gg ac jd je" width="643" height="480"></div><img class="dp iy gh p q gg ac ge" width="643" height="480"><noscript></noscript></div></div></figure><p id="eef1" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">The robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first <a href="http://vizdoom.cs.put.edu.pl/?source=post_page---------------------------" class="av ca jt ju jv jw">VizDoom</a> challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:</p><p id="b5e8" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><code class="jb kz la lb b">pip install vizdoom</code></p><p id="5625" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">Once it is installed, we will be using the <code class="jb kz la lb b">basic.wad</code> environment, which is provided in the <a href="https://github.com/awjuliani/DeepRL-Agents/blob/master/basic.wad?source=post_page---------------------------" class="av ca jt ju jv jw">Github repository</a>, and needs to be placed in the working directory.</p><p id="e421" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">The challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.</p><figure class="it iu iv iw ix fe lk y z paragraph-image"><div class="ja n eu jb"><div class="ll n"><div class="lw lx gh p q gg ac cj v iz"><img src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_DVaQXq6aVWYsU3S3INkV_w.png" class="gh p q gg ac jd je" width="700" height="339"></div><img class="dp iy gh p q gg ac ge" width="700" height="339"><noscript></noscript></div></div><figcaption class="at ee km kn hj do y z ko ao dd" data-selectable-paragraph="">Average reward over time for three workers on Doom task. 0.5 reward corresponds to optimal performance. X-axis represents number of training episodes per worker.</figcaption></figure><p id="a654" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">I hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.</p><p id="30ca" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><em class="kq">(There are a lot of moving parts in A3C, so if you discover a bug, or find a better way to do something, please don’t hesitate to bring it up here or in the Github. I am more than happy to incorporate changes and feedback to improve the algorithm.)</em></p><p id="d774" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">If you’d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @<a href="https://medium.com/u/18dfe63fa7f0?source=post_page---------------------------" class="lm ek ca">Arthur Juliani</a>, or on twitter <a href="https://twitter.com/awjuliani?source=post_page---------------------------" class="av ca jt ju jv jw">@awjuliani</a>.</p><p id="f902" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph="">If this post has been valuable to you, please consider <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;business=V2R22DV4XSR5Y&amp;lc=US&amp;item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&amp;currency_code=USD&amp;bn=PP-DonationsBF%3Abtn_donateCC_LG.gif%3ANonHosted&amp;source=post_page---------------------------" class="av ca jt ju jv jw"><em class="kq">donating</em></a> to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!</p></div></section><hr class="ln dd gz lo lp hj lq lr ls lt lu"><section class="hq hr hs ht hu"><div class="af hv ac do w x"><p id="8fd9" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js" data-selectable-paragraph=""><strong class="jh kp"><em class="kq">More from my Simple Reinforcement Learning with Tensorflow series:</em></strong></p><ol class=""><li id="e174" class="jf jg bq ap jh b ji jj jk jl jm jn jo jp jq jr js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=post_page---------------------------"><em class="kq">Part 0 — Q-Learning Agents</em></a></li><li id="bdb3" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149?source=post_page---------------------------"><em class="kq">Part 1 — Two-Armed Bandit</em></a></li><li id="96da" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c?source=post_page---------------------------"><em class="kq">Part 1.5 — Contextual Bandits</em></a></li><li id="ace9" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724?source=post_page---------------------------"><em class="kq">Part 2 — Policy-Based Agents</em></a></li><li id="76f1" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99?source=post_page---------------------------"><em class="kq">Part 3 — Model-Based RL</em></a></li><li id="30bc" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=post_page---------------------------"><em class="kq">Part 4 — Deep Q-Networks and Beyond</em></a></li><li id="42b2" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a?source=post_page---------------------------"><em class="kq">Part 5 — Visualizing an Agent’s Thoughts and Actions</em></a></li><li id="fb27" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc?source=post_page---------------------------"><em class="kq">Part 6 — Partial Observability and Deep Recurrent Q-Networks</em></a></li><li id="d290" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><a class="av ca jt ju jv jw" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf?source=post_page---------------------------"><em class="kq">Part 7 — Action-Selection Strategies for Exploration</em></a></li><li id="062f" class="jf jg bq ap jh b ji lc jk ld jm le jo lf jq lg js lv kx ky" data-selectable-paragraph=""><strong class="jh kp">Part 8 — Asynchronous Actor-Critic Agents (A3C)</strong></li></ol></div></section></div></article><div class="dp dq dr o ds dt du dv dw e" data-test-id="post-sidebar"><div class="ag dx"><div class="dy dz n"><a href="https://medium.com/emergent-future?source=post_sidebar--------------------------post_sidebar-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><h2 class="ao ea eb aq bq">Emergent // Future</h2></a><div class="ec ed n"><h4 class="ao dd ee aq cj ef eg eh ei ej at">Exploring frontier technology through the lens of artificial intelligence, data science, and the shape of things to come</h4></div><div class="bz"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div><div class="eq er es ag"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa el"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">4.94K </button></h4></div></div></div></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div><div class="dn fe ag dx ff"><section class="w x y z ac do af n"><ul class="bc bd"><li class="bz cp fg fh"><a href="https://medium.com/tag/machine-learning" class="fi fj ca at n fk fl a b de">Machine Learning</a></li><li class="bz cp fg fh"><a href="https://medium.com/tag/artificial-intelligence" class="fi fj ca at n fk fl a b de">Artificial Intelligence</a></li><li class="bz cp fg fh"><a href="https://medium.com/tag/deep-learning" class="fi fj ca at n fk fl a b de">Deep Learning</a></li><li class="bz cp fg fh"><a href="https://medium.com/tag/neural-networks" class="fi fj ca at n fk fl a b de">Neural Networks</a></li><li class="bz cp fg fh"><a href="https://medium.com/tag/robotics" class="fi fj ca at n fk fl a b de">Robotics</a></li></ul><div class="ag fm fn"><div class="ag ah"><div class="bm n eu"><div class=""><div class="c fo cd ag ah fp eu fq fr ep fs ft fu fv fw fx fy fz ga gb gc"><button class="bc ev ew ex ey ez gd ah ge cd ag el ff gf q gg gh p ac"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq bq"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">4.94K claps</button></h4></div></div></div><div class="ag ah"><div class="gi n an g"><a href="https://medium.com/p/c88f72a5e9f2/share/twitter?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="29" height="29" class="am"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gi n an g"><a href="https://medium.com/p/c88f72a5e9f2/share/facebook?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="29" height="29" class="am"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gi gj ch"><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" class="am"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div><div class="gi n an"><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="bz"><div class="n an"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="am"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div><div class="gk gl gm gn"><div class="go gp n eu"><span class="n gq aj gr"><div class="n gh gs gt"><a href="https://medium.com/@awjuliani?source=follow_footer--------------------------follow_footer-"><img alt="Arthur Juliani" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_kLlWemBAJPjdMTEh5hDtUg(1).jpeg" class="n cd cw gu" width="80" height="80"></a></div><span class="n"><div class="gv n gw"><p class="ao dd de aq at dg gx">Written by</p></div><div class="gv gy ag gw"><div class="ac ag ah fm"><h2 class="ao ea gz ha bq"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk" href="https://medium.com/@awjuliani?source=follow_footer--------------------------follow_footer-">Arthur Juliani</a></h2><div class="n g"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div></span></span><div class="gv hb n gw ch"><div class="hc n"><h4 class="ao dd eb hd at">Deep Learning @Unity3D</h4></div><div class="gj he ch"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div><div class="gm n"></div><div class="go gp n eu"><span class="n gq aj gr"><div class="n gh gs gt"><a href="https://medium.com/emergent-future?source=follow_footer--------------------------follow_footer-"><img alt="Emergent // Future" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_3iC1ql6KrdtqcsHchkWeSQ.png" class="bw gu cw" width="80" height="80"></a></div><span class="n"><div class="gv gy ag gw"><div class="ac ag ah fm"><h2 class="ao ea gz ha bq"><a href="https://medium.com/emergent-future?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">Emergent // Future</a></h2><div class="n g"><div class="bz"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div></div></span></span><div class="gv hf n gw ch"><div class="hc n"><h4 class="ao dd eb hd at">Exploring frontier technology through the lens of artificial intelligence, data science, and the shape of things to come</h4></div><div class="gj he ch"><div class="bz"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div></div></div><div class="hg cg n"><a href="https://medium.com/p/c88f72a5e9f2/responses/show?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><div class="hh hi fi n hj ch"><span class="ek">See responses (85)</span></div></a></div></section><div class="hk n hl"><section class="w x y z ac ae af n"><section class="y ih z ac mn af ag fm"><section class="go y z ac mn af n"><div class="mo dz go n"><h2 class="ao ea mp mq bq">More From Medium</h2></div><div class="ag mr"><div class="ms mt n ak mu"><div class="ac gg"><div class="go ag dx"><div class="mv n"><h4 class="ao dd ee aq at">More from Arthur Juliani</h4></div><div class="mw n"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n" href="https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3?source=post_recirc---------0------------------"><div class="mx eu"><div class="gg gh ac"><div class="my n mz na gg ac nb md"></div></div></div></a></div><div class="mw n"><a href="https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3?source=post_recirc---------0------------------"><h3 class="bq am hy nc ap nd ne nf">On “solving” Montezuma’s Revenge</h3></a></div><div class="ag ah fm"><div class="ng n co"><div class="ah ag"><div><a href="https://medium.com/@awjuliani?source=post_recirc---------0------------------"><img alt="Arthur Juliani" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_kLlWemBAJPjdMTEh5hDtUg(2).jpeg" class="n cd nh ni" width="40" height="40"></a></div><div class="ik ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="di ag ah im" data-test-id="postByline"><span class="ao dd ee aq cj in eg eh io ej bq"><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/@awjuliani?source=post_recirc---------0------------------">Arthur Juliani</a></span></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj in eg eh io ej at"><div><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3?source=post_recirc---------0------------------">Jul 13, 2018</a> · 10 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa fd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at">2.6K </h4></div></div></div><div class="nj ik ng cx nk n"></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div><div class="ms mt n ak mu"><div class="ac gg"><div class="go ag dx"><div class="mv n"><h4 class="ao dd ee aq at">Related reads</h4></div><div class="mw n"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n" href="https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323?source=post_recirc---------1------------------"><div class="mx eu"><div class="gg gh ac"><div class="nl n mz na gg ac nb md"></div></div></div></a></div><div class="mw n"><a href="https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323?source=post_recirc---------1------------------"><h3 class="bq am hy nc ap nd ne nf">RL — Model-based Reinforcement Learning</h3></a></div><div class="ag ah fm"><div class="ng n co"><div class="ah ag"><div><a href="https://medium.com/@jonathan_hui?source=post_recirc---------1------------------"><img alt="Jonathan Hui" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/1_c3Z3aOPBooxEX4tx4RkzLw.jpeg" class="n cd nh ni" width="40" height="40"></a></div><div class="ik ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="di ag ah im" data-test-id="postByline"><span class="ao dd ee aq cj in eg eh io ej bq"><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/@jonathan_hui?source=post_recirc---------1------------------">Jonathan Hui</a></span></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj in eg eh io ej at"><div><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323?source=post_recirc---------1------------------">Sep 25, 2018</a> · 12 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa fd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at">968 </h4></div></div></div><div class="nj ik ng cx nk n"></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div><div class="nm mt n ak mu"><div class="ac gg"><div class="go ag dx"><div class="mv n"><h4 class="ao dd ee aq at">Also tagged Neural Networks</h4></div><div class="mw n"><a href="https://towardsdatascience.com/working-with-tfrecords-and-tf-train-example-36d111b3ff4d?source=post_recirc---------2------------------" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n"><div class="mx eu"><div class="gg gh ac"><div class="nn n mz na gg ac nb md"></div></div></div></a></div><div class="mw n"><a href="https://towardsdatascience.com/working-with-tfrecords-and-tf-train-example-36d111b3ff4d?source=post_recirc---------2------------------"><h3 class="bq am hy nc ap nd ne nf">Working with TFRecords and tf.train.Example</h3></a></div><div class="ag ah fm"><div class="ng n co"><div class="ah ag"><div><a href="https://medium.com/@cihansoylu?source=post_recirc---------2------------------"><div class="eu ni nh"><svg width="46" height="50" viewBox="0 0 46 50" class="no gh np nq nr ns dq"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="Cihan Soylu" src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/2_cNam8xSwa8mnSNjV-VQONA.jpeg" class="n cd nh ni" width="40" height="40"></div></a></div><div class="ik ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="di ag ah im" data-test-id="postByline"><span class="ao dd ee aq cj in eg eh io ej bq"><a class="av aw ax ay az ba bb bc bd be ip bh bi bj bk" href="https://medium.com/@cihansoylu?source=post_recirc---------2------------------">Cihan Soylu</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------2------------------" class="av aw ax ay az ba bb bc bd be ip bh bi bj bk">Towards Data Science</a></span></span></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj in eg eh io ej at"><div><a href="https://towardsdatascience.com/working-with-tfrecords-and-tf-train-example-36d111b3ff4d?source=post_recirc---------2------------------" class="av aw ax ay az ba bb bc bd be ip bh bi bj bk">Jul 18</a> · 8 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa fd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at">1 </h4></div></div></div><div class="nj ik ng cx nk n"></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></section></section></section></div></div></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script></div></div><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20190718-173259-c281c804a2","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","iTunesAppId":"828256236","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20190718-173259-c281c804a2"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"]},"debug":{"requestId":"734b3446-3ac9-4367-9fb6-5f8ffce9d26e","originalSpanCarrier":{"ot-tracer-spanid":"5d0b5e4667bce0bc","ot-tracer-traceid":"1ced6aa627ef8c4a","ot-tracer-sampled":"true"}},"session":{"user":{"id":"be025283e717"},"xsrf":"GqzC6QU5GjE7"},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hideGoogleOneTap":false,"referrerSource":"--------------------------post_free-","currentLocation":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2","host":"medium.com","hostname":"medium.com","referrer":"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle"},"client":{"isBot":false,"isDnt":false,"isEu":false,"isNativeMedium":false,"isCustomDomain":false},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"User:be025283e717":{"id":"be025283e717","username":"vivekagr12199","name":"Vivek Agrawal","imageId":"0*toB60eKEH3klSlAD.jpg","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"vivekagr12199@gmail.com","unverifiedEmail":"","__typename":"User"},"ROOT_QUERY":{"viewer":{"type":"id","generated":false,"id":"User:be025283e717","typename":"User"},"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"}],"meterPost({\"postId\":\"c88f72a5e9f2\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"c88f72a5e9f2\"})":{"type":"id","generated":false,"id":"Post:c88f72a5e9f2","typename":"Post"},"notificationsConnection({})":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({})","typename":"NotificationsConnection"},"notificationStatus":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationStatus","typename":"NotificationStatus"}},"ROOT_QUERY.variantFlags.0":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.3":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.4":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.5":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.6":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.7":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.10":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.11":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.14":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.28":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_quarantine_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_pub_newsletters","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_claps","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_lite_post_manager_gear_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_live_user_post_scoring","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_private_notes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_private_notes_li_100","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_email_sign_in_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_daily_read_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_lite_paywall_alert","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_serve_recs_from_ml_rank_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_serve_recs_from_ml_rank_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_serve_recs_from_ml_rank_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_lite_thanks_to","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_google_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lite_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_lite_audio_upsells","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_ticks_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_lite_verify_email_butter_bar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_lite_unread_notification_count","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:c88f72a5e9f2":{"__typename":"Post","creator":{"type":"id","generated":false,"id":"User:18dfe63fa7f0","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","id":"c88f72a5e9f2","collection":{"type":"id","generated":false,"id":"Collection:d7fff85024c6","typename":"Collection"},"sequence":null,"firstPublishedAt":1481942084978,"isPublished":true,"title":"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)","canonicalUrl":"","layerCake":0,"primaryTopic":null,"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}})":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}})","typename":"PostContent"},"highlights":[{"type":"id","generated":false,"id":"Quote:9585d3234c26","typename":"Quote"},{"type":"id","generated":false,"id":"Quote:anon_8e9c92c8956d","typename":"Quote"}],"latestPublishedVersion":"2ba594da41c0","mediumUrl":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2","readingTime":7.252201257861635,"statusForCollection":"APPROVED","visibility":"PUBLIC","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:neural-networks","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:robotics","typename":"Tag"}],"viewerClapCount":0,"readingList":"READING_LIST_NONE","clapCount":4940,"voterCount":743,"recommenders":[],"responsesCount":85,"collaborators":[],"inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"audioVersionUrl":null,"socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1498845191876,"previewContent":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*odZkJyxANWf6pofV9D17hw.jpeg","typename":"ImageMetadata"},"updatedAt":1529547543572,"topics":[],"shareKey":null},"User:18dfe63fa7f0":{"id":"18dfe63fa7f0","__typename":"User","allowNotes":true,"name":"Arthur Juliani","isFollowing":false,"username":"awjuliani","bio":"Deep Learning @Unity3D","imageId":"1*kLlWemBAJPjdMTEh5hDtUg.jpeg","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"awjuliani"},"Collection:d7fff85024c6":{"id":"d7fff85024c6","__typename":"Collection","slug":"emergent-future","domain":null,"colorBehavior":"ACCENT_COLOR","name":"Emergent \u002F\u002F Future","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*O0r7MC7Yw8UJc2TZoIPJxw.png","typename":"ImageMetadata"},"creator":{"type":"id","generated":false,"id":"User:dada28c46df7","typename":"User"},"viewerCanManage":false,"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*3iC1ql6KrdtqcsHchkWeSQ.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"navItems":[{"type":"id","generated":true,"id":"Collection:d7fff85024c6.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:d7fff85024c6.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:d7fff85024c6.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:d7fff85024c6.navItems.3","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"Exploring frontier technology through the lens of artificial intelligence, data science, and the shape of things to come","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"mediumNewsletterId":"","isUserSubscribedToMediumNewsletter":false,"ampEnabled":false,"twitterUsername":"emergentfuture","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:","typename":"ImageMetadata"}},"ImageMetadata:1*O0r7MC7Yw8UJc2TZoIPJxw.png":{"id":"1*O0r7MC7Yw8UJc2TZoIPJxw.png","originalWidth":2656,"originalHeight":400,"__typename":"ImageMetadata"},"User:dada28c46df7":{"id":"dada28c46df7","__typename":"User"},"ImageMetadata:1*3iC1ql6KrdtqcsHchkWeSQ.png":{"id":"1*3iC1ql6KrdtqcsHchkWeSQ.png","__typename":"ImageMetadata"},"Collection:d7fff85024c6.navItems.0":{"title":"Machine Learning Trends","url":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fmachine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129","type":"POST_NAV_ITEM","__typename":"NavItem"},"Collection:d7fff85024c6.navItems.1":{"title":"Algorithm Economy","url":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fhow-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d","type":"POST_NAV_ITEM","__typename":"NavItem"},"Collection:d7fff85024c6.navItems.2":{"title":"Why Deep Learning Matters","url":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fwhy-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4","type":"POST_NAV_ITEM","__typename":"NavItem"},"Collection:d7fff85024c6.navItems.3":{"title":"Algorithmia","url":"https:\u002F\u002Falgorithmia.com","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF444A55","colorPoints":[{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF444A55","point":0,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF5A606A","point":0.1,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF6F747E","point":0.2,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF838790","point":0.3,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF9699A2","point":0.4,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FFA9ABB3","point":0.5,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFBBBDC3","point":0.6,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFCDCED3","point":0.7,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFDEDEE3","point":0.8,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFEFEFF2","point":0.9,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFFFEFF","point":1,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF7F8591","point":0,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF767C87","point":0.1,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF6D727C","point":0.2,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF646971","point":0.3,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF5B5F66","point":0.4,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF51545B","point":0.5,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF474A4F","point":0.6,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF3D3F43","point":0.7,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF323337","point":0.8,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF26272A","point":0.9,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF191A1C","point":1,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFF3F2F5","point":0,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFF1F0F4","point":0.1,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFEEEEF3","point":0.2,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFECEDF2","point":0.3,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFEAEBF1","point":0.4,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFE7E9F0","point":0.5,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFE5E7EF","point":0.6,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFE3E5EE","point":0.7,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFE0E4ED","point":0.8,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFDEE2EB","point":0.9,"__typename":"ColorPoint"},"$Collection:d7fff85024c6.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFDCE0EA","point":1,"__typename":"ColorPoint"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}})":{"isLockedPreviewOnly":false,"__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel","typename":"RichText"}},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.sections.0":{"name":"35c9","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.sections.1":{"name":"2573","startIndex":51,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.sections.1","typename":"Section"}],"paragraphs":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.0","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.1","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.2","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.3","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.4","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.5","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.6","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.7","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.9","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.10","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.11","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.12","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.13","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.14","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.15","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.16","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.17","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.19","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.20","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.22","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.23","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.24","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.25","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.26","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.27","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.28","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.29","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.30","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.31","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.32","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.33","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.34","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.35","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.36","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.37","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.38","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.39","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.40","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.41","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.42","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.43","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.44","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.45","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.46","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.47","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.48","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.49","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.50","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.51","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.52","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.53","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.54","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.55","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.56","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.57","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.58","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.59","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.60","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.61","typename":"Paragraph"}],"__typename":"RichText"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.0":{"name":"9fc8","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.1":{"name":"9c1e","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*odZkJyxANWf6pofV9D17hw.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*odZkJyxANWf6pofV9D17hw.jpeg":{"id":"1*odZkJyxANWf6pofV9D17hw.jpeg","originalHeight":1103,"originalWidth":2000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.2":{"name":"2165","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven’t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.2.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.2.markups.0":{"type":"A","start":472,"end":487,"href":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.3":{"name":"bf4c","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"So what is A3C? The A3C algorithm was released by Google’s DeepMind group earlier this year, and it made a splash by… essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their “universal starter agent” for working with their new (and very diverse) set of Universe environments.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.3.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.3.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.3.markups.0":{"type":"A","start":20,"end":33,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1602.01783.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.3.markups.1":{"type":"A","start":483,"end":496,"href":"https:\u002F\u002Fopenai.com\u002Fblog\u002Funiverse\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.4":{"name":"ea50","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"The 3 As of A3C","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.5":{"name":"ee7c","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*YtnGhtSAMnnHSL8PvS7t_w.png","typename":"ImageMetadata"},"text":"Diagram of A3C high-level architecture.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*YtnGhtSAMnnHSL8PvS7t_w.png":{"id":"1*YtnGhtSAMnnHSL8PvS7t_w.png","originalHeight":1244,"originalWidth":1392,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.6":{"name":"f76c","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Asynchronous Advantage Actor-Critic is quite a mouthful. Let’s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.7":{"name":"de30","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Asynchronous: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.7.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.7.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.7.markups.0":{"type":"A","start":21,"end":24,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.ut59y2t80","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.7.markups.1":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8":{"name":"0151","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Actor-Critic: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.1":{"type":"STRONG","start":268,"end":272,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.2":{"type":"STRONG","start":325,"end":329,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.3":{"type":"EM","start":268,"end":272,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.8.markups.4":{"type":"EM","start":325,"end":329,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.9":{"name":"ba19","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Advantage: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were “good” and which were “bad.” The network was then updated in order to encourage and discourage actions appropriately.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.9.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.9.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.9.markups.0":{"type":"A","start":35,"end":68,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsuper-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.nac3dxoyy","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.9.markups.1":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.10":{"name":"f881","__typename":"Paragraph","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Discounted Reward: R = γ(r)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.11":{"name":"188b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network’s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.11.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.11.markups.0":{"type":"A","start":325,"end":355,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.snohiu2y2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.12":{"name":"e08d","__typename":"Paragraph","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Advantage: A = Q(s,a) - V(s)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.13":{"name":"a856","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Since we won’t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.14":{"name":"a7f7","__typename":"Paragraph","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Advantage Estimate: A = R - V(s)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.15":{"name":"388a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.15.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.15.markups.0":{"type":"A","start":143,"end":175,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02438.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.16":{"name":"e225","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Implementing the Algorithm","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.17":{"name":"28da","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Hzql_1t0-wwDxiz0C97AcQ.png","typename":"ImageMetadata"},"text":"Training workflow of each worker agent in A3C.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Hzql_1t0-wwDxiz0C97AcQ.png":{"id":"1*Hzql_1t0-wwDxiz0C97AcQ.png","originalHeight":1144,"originalWidth":1316,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18":{"name":"9288","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by DennyBritz and OpenAI. Both of which I highly recommend if you’d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won’t run on its own. To view and run the full, functional A3C implementation, see my Github repository.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.0":{"type":"A","start":120,"end":130,"href":"https:\u002F\u002Fgithub.com\u002Fdennybritz\u002Freinforcement-learning","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.1":{"type":"A","start":135,"end":141,"href":"https:\u002F\u002Fgithub.com\u002Fopenai\u002Funiverse-starter-agent","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.2":{"type":"A","start":396,"end":413,"href":"https:\u002F\u002Fgithub.com\u002Fawjuliani\u002FDeepRL-Agents\u002Fblob\u002Fmaster\u002FA3C-Doom.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.18.markups.3":{"type":"EM","start":0,"end":414,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.19":{"name":"19c3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The general outline of the code architecture is:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.20":{"name":"24a5","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"AC_Network — This class contains all the Tensorflow ops to create the networks themselves.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.20.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.20.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.20.markups.0":{"type":"CODE","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.20.markups.1":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21":{"name":"c95a","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Worker — This class contains a copy of AC_Network, an environment class, as well as all the logic for interacting with the environment, and updating the global network.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21.markups.0":{"type":"CODE","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21.markups.1":{"type":"CODE","start":39,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.21.markups.2":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.22":{"name":"9f48","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"High-level code for establishing the Worker instances and running them in parallel.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.22.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.22.markups.0":{"type":"CODE","start":37,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.23":{"name":"b66c","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.24":{"name":"4f9b","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.24.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:1a5f68091ea4bd0853b6aaa7f508b056":{"id":"1a5f68091ea4bd0853b6aaa7f508b056","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"a3c.py","__typename":"MediaResource"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.24.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:1a5f68091ea4bd0853b6aaa7f508b056","typename":"MediaResource"},"__typename":"Iframe"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.25":{"name":"9226","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Next, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.26":{"name":"4996","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.26.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:28c80bc5d3c9808c1e8b87b5480f7d5b":{"id":"28c80bc5d3c9808c1e8b87b5480f7d5b","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"make_worker.py","__typename":"MediaResource"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.26.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:28c80bc5d3c9808c1e8b87b5480f7d5b","typename":"MediaResource"},"__typename":"Iframe"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.27":{"name":"9113","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"~ From here we go asynchronous ~","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.27.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.27.markups.0":{"type":"EM","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.28":{"name":"4bcd","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Each worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.29":{"name":"c8e9","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.29.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:27d14957538011cabd7a9a117365b863":{"id":"27d14957538011cabd7a9a117365b863","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"set_parameters.py","__typename":"MediaResource"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.29.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:27d14957538011cabd7a9a117365b863","typename":"MediaResource"},"__typename":"Iframe"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.30":{"name":"d267","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Each worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.30.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.30.markups.0":{"type":"EM","start":128,"end":170,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.31":{"name":"5020","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.31.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:0ac37f6cb07055fa52505d65b5ae2482":{"id":"0ac37f6cb07055fa52505d65b5ae2482","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"work.py","__typename":"MediaResource"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.31.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:0ac37f6cb07055fa52505d65b5ae2482","typename":"MediaResource"},"__typename":"Iframe"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.32":{"name":"835d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once the worker’s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.32.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.32.markups.0":{"type":"EM","start":192,"end":193,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.33":{"name":"4b5d","__typename":"Paragraph","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Value Loss: L = Σ(R - V(s))²","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.34":{"name":"0924","__typename":"Paragraph","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Policy Loss: L = -log(π(s)) * A(s) - β*H(π)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.34.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.34.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.34.markups.0":{"type":"STRONG","start":22,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.34.markups.1":{"type":"STRONG","start":41,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.35":{"name":"d245","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"A worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.36":{"name":"2bdd","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"A worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.37":{"name":"b7e0","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.37.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:920870ee2a4504c11b643835489ea22e":{"id":"920870ee2a4504c11b643835489ea22e","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"train.py","__typename":"MediaResource"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.37.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:920870ee2a4504c11b643835489ea22e","typename":"MediaResource"},"__typename":"Iframe"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.38":{"name":"fbb0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.39":{"name":"e345","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"To view the full and functional code, see the Github repository here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.39.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.39.markups.0":{"type":"A","start":64,"end":68,"href":"https:\u002F\u002Fgithub.com\u002Fawjuliani\u002FDeepRL-Agents\u002Fblob\u002Fmaster\u002FA3C-Doom.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.40":{"name":"e1e2","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Playing Doom","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.41":{"name":"0445","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*yjPVxRywI8U9SKfCMCKR1A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*yjPVxRywI8U9SKfCMCKR1A.png":{"id":"1*yjPVxRywI8U9SKfCMCKR1A.png","originalHeight":480,"originalWidth":643,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.42":{"name":"eef1","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.42.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.42.markups.0":{"type":"A","start":280,"end":287,"href":"http:\u002F\u002Fvizdoom.cs.put.edu.pl\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.43":{"name":"b5e8","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"pip install vizdoom","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.43.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.43.markups.0":{"type":"CODE","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.44":{"name":"5625","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once it is installed, we will be using the basic.wad environment, which is provided in the Github repository, and needs to be placed in the working directory.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.44.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.44.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.44.markups.0":{"type":"CODE","start":43,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.44.markups.1":{"type":"A","start":91,"end":108,"href":"https:\u002F\u002Fgithub.com\u002Fawjuliani\u002FDeepRL-Agents\u002Fblob\u002Fmaster\u002Fbasic.wad","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.45":{"name":"e421","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.46":{"name":"84d9","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*DVaQXq6aVWYsU3S3INkV_w.png","typename":"ImageMetadata"},"text":"Average reward over time for three workers on Doom task. 0.5 reward corresponds to optimal performance. X-axis represents number of training episodes per worker.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*DVaQXq6aVWYsU3S3INkV_w.png":{"id":"1*DVaQXq6aVWYsU3S3INkV_w.png","originalHeight":393,"originalWidth":811,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.47":{"name":"a654","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"I hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.48":{"name":"30ca","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"(There are a lot of moving parts in A3C, so if you discover a bug, or find a better way to do something, please don’t hesitate to bring it up here or in the Github. I am more than happy to incorporate changes and feedback to improve the algorithm.)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.48.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.48.markups.0":{"type":"EM","start":0,"end":248,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.49":{"name":"d774","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"If you’d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjuliani.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.49.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.49.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.49.markups.0":{"type":"A","start":101,"end":115,"href":null,"anchorType":"USER","userId":"18dfe63fa7f0","linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.49.markups.1":{"type":"A","start":131,"end":141,"href":"https:\u002F\u002Ftwitter.com\u002Fawjuliani","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.50":{"name":"f902","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.50.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.50.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.50.markups.0":{"type":"A","start":55,"end":63,"href":"https:\u002F\u002Fwww.paypal.com\u002Fcgi-bin\u002Fwebscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.50.markups.1":{"type":"EM","start":55,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.51":{"name":"8fd9","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"More from my Simple Reinforcement Learning with Tensorflow series:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.51.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.51.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.51.markups.0":{"type":"STRONG","start":0,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.51.markups.1":{"type":"EM","start":0,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.52":{"name":"e174","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 0 — Q-Learning Agents","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.52.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.52.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.52.markups.0":{"type":"A","start":0,"end":26,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.52.markups.1":{"type":"EM","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.53":{"name":"bdb3","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 1 — Two-Armed Bandit","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.53.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.53.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.53.markups.0":{"type":"A","start":0,"end":25,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsuper-simple-reinforcement-learning-tutorial-part-1-fd544fab149","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.53.markups.1":{"type":"EM","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.54":{"name":"96da","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 1.5 — Contextual Bandits","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.54.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.54.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.54.markups.0":{"type":"A","start":0,"end":29,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c#.uzs1axw0s","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.54.markups.1":{"type":"EM","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.55":{"name":"ace9","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 2 — Policy-Based Agents","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.55.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.55.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.55.markups.0":{"type":"A","start":0,"end":28,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsuper-simple-reinforcement-learning-tutorial-part-2-ded33892c724","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.55.markups.1":{"type":"EM","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.56":{"name":"76f1","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 3 — Model-Based RL","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.56.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.56.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.56.markups.0":{"type":"A","start":0,"end":23,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.56.markups.1":{"type":"EM","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.57":{"name":"30bc","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 4 — Deep Q-Networks and Beyond","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.57.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.57.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.57.markups.0":{"type":"A","start":0,"end":35,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.57.markups.1":{"type":"EM","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.58":{"name":"42b2","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 5 — Visualizing an Agent’s Thoughts and Actions","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.58.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.58.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.58.markups.0":{"type":"A","start":0,"end":52,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani\u002Fsimple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.58.markups.1":{"type":"EM","start":0,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.59":{"name":"fb27","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 6 — Partial Observability and Deep Recurrent Q-Networks","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.59.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.59.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.59.markups.0":{"type":"A","start":0,"end":60,"href":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.9djtshpqo","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.59.markups.1":{"type":"EM","start":0,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.60":{"name":"d290","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 7 — Action-Selection Strategies for Exploration","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.60.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.60.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.60.markups.0":{"type":"A","start":0,"end":52,"href":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.qfg7lqxpr","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.60.markups.1":{"type":"EM","start":0,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.61":{"name":"062f","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Part 8 — Asynchronous Actor-Critic Agents (A3C)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.61.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:c88f72a5e9f2.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"--------------------------post_free-\"}}).bodyModel.paragraphs.61.markups.0":{"type":"STRONG","start":0,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Quote:9585d3234c26":{"id":"9585d3234c26","paragraphs":[{"type":"id","generated":true,"id":"Quote:9585d3234c26.paragraphs.0","typename":"Paragraph"}],"__typename":"Quote","userId":"be025283e717","startOffset":264,"endOffset":344,"user":{"type":"id","generated":true,"id":"$Quote:9585d3234c26.user","typename":"User"}},"Quote:9585d3234c26.paragraphs.0":{"name":"bf4c","__typename":"Paragraph"},"$Quote:9585d3234c26.user":{"name":"Vivek Agrawal","__typename":"User"},"Quote:anon_8e9c92c8956d":{"id":"anon_8e9c92c8956d","paragraphs":[{"type":"id","generated":true,"id":"Quote:anon_8e9c92c8956d.paragraphs.0","typename":"Paragraph"}],"__typename":"Quote","userId":"anon","startOffset":0,"endOffset":301,"user":null},"Quote:anon_8e9c92c8956d.paragraphs.0":{"name":"188b","__typename":"Paragraph"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:neural-networks":{"id":"neural-networks","displayTitle":"Neural Networks","__typename":"Tag"},"Tag:robotics":{"id":"robotics","displayTitle":"Robotics","__typename":"Tag"},"ImageMetadata:":{"id":"","__typename":"ImageMetadata"},"$Post:c88f72a5e9f2.previewContent":{"subtitle":"In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will…","__typename":"PreviewContent"},"$ROOT_QUERY.notificationsConnection({})":{"notifications":[],"pagingInfo":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).pagingInfo","typename":"Paging"},"__typename":"NotificationsConnection"},"$ROOT_QUERY.notificationsConnection({}).pagingInfo":{"next":null,"__typename":"Paging"},"$ROOT_QUERY.notificationStatus":{"unreadNotificationCount":0,"__typename":"NotificationStatus"}}</script><script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/manifest.7eff00fa.js.download"></script><script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/vendors_main.85c53847.chunk.js.download"></script><script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/main.edf544ea.chunk.js.download"></script><script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/vendors_screen.landingpages.trumpland_screen.post_screen.post.amp_screen.post.series.download"></script>
<script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/screen.post_screen.post.amp_screen.post.series_screen.profile_screen.sequence.librar.download"></script>
<script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/screen.landingpages.trumpland_screen.post_screen.post.amp_screen.post.series_screen..download"></script>
<script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/screen.post_screen.post.amp_screen.sequence.post.333c1d1b.chunk.js.download"></script>
<script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/screen.post.a415bf7b.chunk.js.download"></script><script>window.main();</script><script src="./Simple Reinforcement Learning with Tensorflow Part 8_ Asynchronous Actor-Critic Agents (A3C)_files/p.js.download" async="" id="parsely-cf"></script><div id="weava-permanent-marker" date="1563473312987"></div><div id="weava-ui-wrapper"><div class="weava-drop-area-wrapper"><div class="weava-drop-area"></div>
<div class="weava-drop-area-text">Drop here!</div></div></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>