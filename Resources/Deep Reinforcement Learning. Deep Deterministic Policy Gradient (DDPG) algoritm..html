<!DOCTYPE html>
<!-- saved from url=(0127)https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43 -->
<html xmlns:cc="http://creativecommons.org/ns#" class="gr__medium_com"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm.</title><link rel="canonical" href="https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43"><meta name="title" content="Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm."><meta name="referrer" content="always"><meta name="description" content="In this project, the goal was to train the 20 Agents (each Agent controls a double — joint robot arm) to maintain its position at the target location for as many time steps as possible. 
 The…"><meta name="theme-color" content="#000000"><meta property="og:title" content="Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm."><meta property="twitter:title" content="Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm."><meta property="og:url" content="https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*_SHO6We5laYHoqOtAR-IEw.png"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="GOAL"><meta name="twitter:description" content="GOAL"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*_SHO6We5laYHoqOtAR-IEw.png"><link rel="author" href="https://medium.com/@markus.x.buchholz"><meta name="author" content="Markus Buchholz"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="Markus Buchholz"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2019-03-16T17:28:49.005Z"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="17 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/5a823da91b43"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/5a823da91b43"><meta property="al:android:url" content="medium://p/5a823da91b43"><meta property="al:web:url" content="https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/5a823da91b43"><script async="" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/branch-latest.min.js.download"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":1073,"url":"https://cdn-images-1.medium.com/max/2400/1*_SHO6We5laYHoqOtAR-IEw.png"},"url":"https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43","dateCreated":"2019-03-16T17:28:49.005Z","datePublished":"2019-03-16T17:28:49.005Z","dateModified":"2019-06-02T15:31:58.450Z","headline":"Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm.","name":"Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm.","articleId":"5a823da91b43","thumbnailUrl":"https://cdn-images-1.medium.com/max/2400/1*_SHO6We5laYHoqOtAR-IEw.png","keywords":["Tag:Reinforcement Learning","Tag:Artificial Intelligence","Tag:Programming","Tag:Robotics","LockedPostSource:0","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"Markus Buchholz","url":"https://medium.com/@markus.x.buchholz"},"creator":["Markus Buchholz"],"publisher":{"@type":"Organization","name":"Medium","url":"https://medium.com/","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/385/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43"}</script><meta name="parsely-link" content="https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/m2.css"><link rel="stylesheet" href="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/main-branding-base.DUpq82k2YI6OvEW6173IfA.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/analytics.js.download"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/190/190/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/150/150/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/95/95/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/75/75/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope="" class="postShowScreen browser-chrome os-windows is-withMagicUnderlines v-glyph v-glyph--m2 is-js" data-gr-c-s-loaded="true" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1562635569930" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="1519" height="674"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"></div></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLightest js-metabar is-hiddenWhenMinimized is-maximized"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div></div><div class="metabar-block u-flex0"><div class="buttonSet buttonSet--wide"><label class="button button--small button--chromeless button--withIcon button--withSvgIcon inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--buttonNormal u-baseColor--placeholderNormal" title="Search Medium"><span class="svgIcon svgIcon--search svgIcon--25px u-baseColor--iconLight"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent" type="search" placeholder="Search Medium" required="true"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless u-xs-top1" href="https://medium.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a><button class="button button--small button--chromeless is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--activity js-notificationsButton u-marginRight16 u-xs-marginRight10 u-lineHeight0 u-size25x25" title="Notifications" aria-label="Notifications" data-action="open-notifications"><span class="svgIcon svgIcon--bell svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-293 409 25 25"><path d="M-273.327 423.67l-1.673-1.52v-3.646a5.5 5.5 0 0 0-6.04-5.474c-2.86.273-4.96 2.838-4.96 5.71v3.41l-1.68 1.553c-.204.19-.32.456-.32.734V427a1 1 0 0 0 1 1h3.49a3.079 3.079 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59c0-.28-.12-.55-.327-.74zm-7.173 5.63c-.842 0-1.55-.546-1.812-1.3h3.624a1.92 1.92 0 0 1-1.812 1.3zm6.35-2.45h-12.7v-2.347l1.63-1.51c.236-.216.37-.522.37-.843v-3.41c0-2.35 1.72-4.356 3.92-4.565a4.353 4.353 0 0 1 4.78 4.33v3.645c0 .324.137.633.376.85l1.624 1.477v2.373z"></path></svg></span></button><a class="button button--small button--upsellNav button--withChrome u-baseColor--buttonNormal u-xs-hide js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---nav_full" data-disable-client-nav="true" data-scroll="native">Upgrade</a><button class="button button--chromeless u-baseColor--buttonNormal is-inSiteNavBar js-userActions" aria-haspopup="true" data-action="open-userActions"><div class="avatar"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/0_toB60eKEH3klSlAD.jpg" class="avatar-image avatar-image--icon" alt="Vivek Agrawal"></div></button></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full" lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls" data-post-id="5a823da91b43" data-source="post_page" data-tracking-context="postPage" data-scroll="native"><section name="c996" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="05c8" id="05c8" class="graf graf--h3 graf--leading graf--title">Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algorithm.</h1><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@markus.x.buchholz?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_12k3P8bjeeTvt1EoxSTDEg.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Markus Buchholz"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 70 70" xmlns="http://www.w3.org/2000/svg"><path d="M5.53538374,19.9430227 C11.180401,8.78497536 22.6271155,1.6 35.3571429,1.6 C48.0871702,1.6 59.5338847,8.78497536 65.178902,19.9430227 L66.2496695,19.401306 C60.4023065,7.84329843 48.5440457,0.4 35.3571429,0.4 C22.17024,0.4 10.3119792,7.84329843 4.46461626,19.401306 L5.53538374,19.9430227 Z"></path><path d="M65.178902,49.9077131 C59.5338847,61.0657604 48.0871702,68.2507358 35.3571429,68.2507358 C22.6271155,68.2507358 11.180401,61.0657604 5.53538374,49.9077131 L4.46461626,50.4494298 C10.3119792,62.0074373 22.17024,69.4507358 35.3571429,69.4507358 C48.5440457,69.4507358 60.4023065,62.0074373 66.2496695,50.4494298 L65.178902,49.9077131 Z"></path></svg></div></div></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://medium.com/@markus.x.buchholz" data-action="show-user-card" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto">Markus Buchholz</a><span class="followState js-followState" data-user-id="c9076eed918c"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="toggle-block-user" data-action-value="c9076eed918c" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="toggle-subscribe-user" data-action-value="c9076eed918c" data-action-source="post_header_lockup-c9076eed918c-------------------------follow_byline" data-subscribe-source="post_header_lockup" data-follow-context-entity-id="5a823da91b43"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2019-03-16T17:28:49.005Z">Mar 16</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="17 min read"></span></div></div></div><figure name="fe0e" id="fe0e" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 391px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.900000000000006%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*_SHO6We5laYHoqOtAR-IEw.png" data-width="1951" data-height="1091" data-is-featured="true" data-action="zoom" data-action-value="1*_SHO6We5laYHoqOtAR-IEw.png" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1__SHO6We5laYHoqOtAR-IEw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="41"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*_SHO6We5laYHoqOtAR-IEw.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1__SHO6We5laYHoqOtAR-IEw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*_SHO6We5laYHoqOtAR-IEw.png"></noscript></div></div><figcaption class="imageCaption">Credit: Maxuser</figcaption></figure><ol class="postList"><li name="4ef1" id="4ef1" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">GOAL</strong></li></ol><p name="4dbd" id="4dbd" class="graf graf--p graf-after--li">In this project, the goal was to train the 20 Agents (each Agent controls a double — joint robot arm) to maintain its position at the target location for as many time steps as possible.&nbsp;<br>&nbsp;The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1. Project requirement is to get average score of +30 over 100 consecutive episodes.</p><figure name="354b" id="354b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 329px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*3RJDOwDKWpMn3omvA3zL5Q.png" data-width="602" data-height="329" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_3RJDOwDKWpMn3omvA3zL5Q.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="39"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*3RJDOwDKWpMn3omvA3zL5Q.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_3RJDOwDKWpMn3omvA3zL5Q(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*3RJDOwDKWpMn3omvA3zL5Q.png"></noscript></div></div></figure><p name="6600" id="6600" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2.</strong> <strong class="markup--strong markup--p-strong">INTRODUCTION</strong></p><p name="3d30" id="3d30" class="graf graf--p graf-after--p">In previous <a href="https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p1_navigation" data-href="https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p1_navigation" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">project</a> we use Q function to find the optimal policy (as the Q function) which approximates (in use of Q table and later deep neural network) selection of the best action to perform in every state. Formally, this could be formulated as,</p><figure name="ddf1" id="ddf1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 262px; max-height: 39px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.899999999999999%;"></div><img class="graf-image" data-image-id="1*jwwqpivvnWlwyJWJ6vcJ8w.png" data-width="262" data-height="39" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_jwwqpivvnWlwyJWJ6vcJ8w.png"></div></figure><p name="2e34" id="2e34" class="graf graf--p graf-after--figure">which means that the result of our policy <em class="markup--em markup--p-em">π </em>at every state <em class="markup--em markup--p-em">s </em>is the action with the largest Q (value function).</p><p name="e153" id="e153" class="graf graf--p graf-after--p">However, in both cases, whether we used a table for small state spaces or a neural network for much larger state spaces, we had to first estimate the optimal action value function before we could tackle the optimal policy. As we can depicted (above equation), when we apply Q-learning we endeavour for the optimal policy by looking for the largest value function. The recent idea here will be to find optimal policy directly without worrying about a value function at all.</p><p name="f86f" id="f86f" class="graf graf--p graf-after--p">There are two main reasons why policy approach is more sufficient attitude to explore.</p><p name="a5b3" id="a5b3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">First of all</strong>, we care more about the total reward but not always about the highest values function (like DQN) at every state. To obtain these values (action and state), we have used the Bellman equation, which expresses the value on the current step via the values on the next step. Normally, when we must solve real, complex problems and the agent obtains the observation from the environment and needs to decide about what to do next, we need policy, not the value of the state or particular action. We need to know what to do next at each step (the action taken based on the highest value cannot be always optimal).</p><p name="5030" id="5030" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Another reason</strong> why policies may be more attractive than values is the environments with extreme number of actions or with a continuous action space.</p><p name="3b44" id="3b44" class="graf graf--p graf-after--p">To be able to decide on the best action, we need to solve an optimization problem finding action a, which maximizes Q(s, a). In the case of an Atari game with several discrete actions, such attitude wasn’t a problem: we just approximated values of all actions and took the action with the largest <em class="markup--em markup--p-em">Q </em>(as argmax ). If our action space is not a small discrete set, but has a scalar value attached to it, such as the steering wheel angle (continuous value which should be regulated), this optimization problem becomes hard, as <em class="markup--em markup--p-em">Q </em>is usually represented by a highly nonlinear neural network (NN), so finding the argument which maximizes the function’s values can be treacherous. In such cases, it’s much more feasible to avoid values function and pay more attention on policy approach.</p><p name="a3c8" id="a3c8" class="graf graf--p graf-after--p">The consecutive question comes straight forward. We need to answer how technically can we approach this idea of estimating an optimal policy?</p><p name="4bf8" id="4bf8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3.</strong> <strong class="markup--strong markup--p-strong">POLICY GRADIENTS. Reinforce algorithm</strong></p><p name="8e84" id="8e84" class="graf graf--p graf-after--p">For our learning reason we can consider cart pole environment. In this case, the Agent base on the feedback from the environment (car position, car velocity, pole angle and the pole speed at the tip) decides about the one of the possible action. The Agent can, at each time step push the cart either left or right. In order to approximate the policy, as we made that in DQN we construct a neural network, that accepts the state as input.</p><figure name="857b" id="857b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 385px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*-NHXgB50gmUGwgBJEIJuJw.png" data-width="602" data-height="385" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_-NHXgB50gmUGwgBJEIJuJw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="47"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*-NHXgB50gmUGwgBJEIJuJw.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_-NHXgB50gmUGwgBJEIJuJw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*-NHXgB50gmUGwgBJEIJuJw.png"></noscript></div></div><figcaption class="imageCaption"><em class="markup--em markup--figure-em">Neural network approximates the&nbsp;policy</em></figcaption></figure><p name="5fec" id="5fec" class="graf graf--p graf-after--figure">This time however, as output, the neural network <strong class="markup--strong markup--p-strong">returns the probability</strong> that the agent selects each possible action. The agent follows this policy and interacts with the environment by just passing the most recent state to the network. Network generates the action probabilities (probability distribution) and then the Agent samples from those probabilities to select an action as response (left or right).</p><p name="53cc" id="53cc" class="graf graf--p graf-after--p">We need to remember that when we worked with DQN, the output of the network were Q-values, so if one of the action was converged to value of 0.3 and another action converged to value of 0.5, so due to our “strategy” argmax the action with higher value was preferred 100% of the time. Actually, we consider the <strong class="markup--strong markup--p-strong">probability distribution</strong>, so if the first action has a probability of 0.3 and the second 0.5, so the Agent can take the first action with 30% chance and the second with 50% chance.</p><p name="b6a0" id="b6a0" class="graf graf--p graf-after--p">Our objective then is to establish appropriate values for the network weights so that for each state that we pass into the neural network it returns action probabilities where the optimal action is most likely to be selected (has the highest probability). As the Agent interacts with the environment and learns more about which action is best for maximizing reward, it changes neural network weights. Changes of neural network weights happen in the rhythm of gradient update in a such a way that actions yielding high reward in a state will have a high probability and actions yielding low reward will have a low probability (transition of weights will increase the probability for the actions where the episode finishes with success or decrease the action probability where the episode finish with lost).</p><p name="2bb0" id="2bb0" class="graf graf--p graf-after--p">Approaching our solution in finding the optimal policy we need to subsequently define <strong class="markup--strong markup--p-strong">trajectory </strong>(τ), which can be expressed as a sequence of the length H (as Horizon) of the states and actions. As R(τ) we can define the reward for the trajectories we are considering.</p><figure name="a991" id="a991" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 407px; max-height: 66px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.2%;"></div><img class="graf-image" data-image-id="1*AR1L9XM7vSCkf8bwvKUzwg.png" data-width="407" data-height="66" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_AR1L9XM7vSCkf8bwvKUzwg.png"></div></figure><p name="10db" id="10db" class="graf graf--p graf-after--figure">The goal here is still the same, we need to find the weights θ of the neural network that maximize expected return — U. As we discussed, we adjust the network weights.</p><p name="08cf" id="08cf" class="graf graf--p graf-after--p">We can express U as following function,</p><figure name="b18c" id="b18c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 311px; max-height: 83px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.700000000000003%;"></div><img class="graf-image" data-image-id="1*w-W2w0_y-ZUc7U4HmJjxsg.png" data-width="311" data-height="83" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_w-W2w0_y-ZUc7U4HmJjxsg.png"></div></figure><p name="6742" id="6742" class="graf graf--p graf-after--figure">where R(τ) is a just the return corresponding to an arbitrary Trajectory τ. The other component in above formula P(τ<strong class="markup--strong markup--p-strong">;</strong> θ) is<strong class="markup--strong markup--p-strong"> </strong>the probability of each possible Trajectory.</p><p name="6208" id="6208" class="graf graf--p graf-after--p">We can see that probability depends on the weights θ in the neural network (θ defines the policy</p><p name="bc90" id="bc90" class="graf graf--p graf-after--p">which is used to select the actions in the Trajectory). As we discussed before, our goal is to find the value of θ that maximizes expected return. To do so we can compute the Gradient Ascent (opposite to gradient descent which we use to find the minimum of the function).</p><p name="cda0" id="cda0" class="graf graf--p graf-after--p">Here, we need to perceive that in computation of the real value of gradient ascent stipulates that we need to evaluate all possible trajectories. It seems that this process can be very computationally expensive, so our approach here will be to just sample a few trajectories (m) using the policy and then use those m — trajectories only to estimate the gradient.</p><p name="b500" id="b500" class="graf graf--p graf-after--p">Finally, the gradient can be express as follows,</p><figure name="d0a7" id="d0a7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 105px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*HCQUckMaM2CwKp2B9gyyRQ.png" data-width="602" data-height="105" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_HCQUckMaM2CwKp2B9gyyRQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="11"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*HCQUckMaM2CwKp2B9gyyRQ.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_HCQUckMaM2CwKp2B9gyyRQ(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*HCQUckMaM2CwKp2B9gyyRQ.png"></noscript></div></div></figure><p name="7726" id="7726" class="graf graf--p graf-after--figure">Once we have an estimate for the gradient, we can use it to update the weights of the policy. Then, we repeatedly loop over these steps to converge to the weights of the optimal policy.</p><p name="43d6" id="43d6" class="graf graf--p graf-after--p">Here we need to understand/recall one more thing. The output from the neural network is just the number (when 2 nodes like in the our Cart Pole example, we have two numbers). Computation of probability distribution involves the need of softmax function exertion (function that takes as input a vector of <em class="markup--em markup--p-em">K</em> real numbers, and normalizes it into a probability distribution consisting of <em class="markup--em markup--p-em">K</em> probabilities).</p><p name="9b83" id="9b83" class="graf graf--p graf-after--p">Other thing is the log of probability π, which refers to the policy, which is parameterized by theta. This mathematical operation can be understood as the standard cross entropy operation, which is used to to quantify the difference between two probability distributions. Finally, we can estimate how we should change the weights of the policy theta, if we want to change the log probability (remember what we specified before. The agent changes the weights to increase the probability for the actions where the episode finishes with success or decrease the probability of the actions, where the episode finish with lost).</p><p name="6c17" id="6c17" class="graf graf--p graf-after--p">The pseudo code of <strong class="markup--strong markup--p-strong">Reinforce algorithm</strong> can be summarize as follows.</p><ol class="postList"><li name="77ee" id="77ee" class="graf graf--li graf-after--p">Use the policy <em class="markup--em markup--li-em">πθ</em>​ to collect M — trajectories with horizon H.</li></ol><figure name="8638" id="8638" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 416px; max-height: 96px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 23.1%;"></div><img class="graf-image" data-image-id="1*lJlxNpXhs_g0GDKR2r-R8g.png" data-width="416" data-height="96" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_lJlxNpXhs_g0GDKR2r-R8g.png"></div></figure><p name="d58e" id="d58e" class="graf graf--p graf-after--figure">2. Use the trajectories to estimate the policy gradient:</p><figure name="caad" id="caad" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 105px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*HCQUckMaM2CwKp2B9gyyRQ.png" data-width="602" data-height="105" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_HCQUckMaM2CwKp2B9gyyRQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="11"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*HCQUckMaM2CwKp2B9gyyRQ.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_HCQUckMaM2CwKp2B9gyyRQ(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*HCQUckMaM2CwKp2B9gyyRQ.png"></noscript></div></div></figure><p name="c2c1" id="c2c1" class="graf graf--p graf-after--figure">3. Update the neural network weights of the policy ( α — is a hyper parameter of neural network, step — learning rate):</p><figure name="d311" id="d311" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 170px; max-height: 77px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.300000000000004%;"></div><img class="graf-image" data-image-id="1*nzpUjjIGF4QG2K_QD7nU4w.png" data-width="170" data-height="77" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_nzpUjjIGF4QG2K_QD7nU4w.png"></div></figure><p name="83d3" id="83d3" class="graf graf--p graf-after--figure">4. Loop over steps 1–3.</p><p name="b3c6" id="b3c6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4.</strong> <strong class="markup--strong markup--p-strong">ACTOR — CRITIC METODS</strong></p><p name="0018" id="0018" class="graf graf--p graf-after--p">Our goal is to apply DDPG algorithm but due to complexity we should first approach our design throughout understanding new (discussed below) concepts.</p><figure name="a922" id="a922" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 377px; max-height: 266px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 70.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*SwQBktXlTXFxvzDyeFQp0g.png" data-width="377" data-height="266" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_SwQBktXlTXFxvzDyeFQp0g.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="51"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*SwQBktXlTXFxvzDyeFQp0g.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_SwQBktXlTXFxvzDyeFQp0g(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*SwQBktXlTXFxvzDyeFQp0g.png"></noscript></div></div></figure><p name="6fcf" id="6fcf" class="graf graf--p graf-after--figure">Above picture depicts the main concept of Actor -Critic methods. Here Actor — Critic methods combine the value-based methods such as DQN and policy-based methods such as Reinforce.</p><p name="1e96" id="1e96" class="graf graf--p graf-after--p">Previously we defined DQN Agent (in project 1) which learns to approximate the optimal action value function. If the Agent learns sufficiently well so deriving a good policy for the Agent it is straightforward.</p><p name="ee3b" id="ee3b" class="graf graf--p graf-after--p">On the other side the Reinforce Agent parameterizes the policy and learns to optimize it directly.</p><p name="d0c7" id="d0c7" class="graf graf--p graf-after--p">Here, the policy is usually stochastic, as we receive the distribution probability.</p><p name="1e9f" id="1e9f" class="graf graf--p graf-after--p">Right now, we will investigate <strong class="markup--strong markup--p-strong">deterministic policies</strong>, which take a state and return the single action (no stochasticity, the policy will be deterministic).</p><figure name="3da7" id="3da7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 396px; max-height: 289px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 73%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*uq7VvWI97IzIxP9UCem8gw.png" data-width="396" data-height="289" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_uq7VvWI97IzIxP9UCem8gw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="53"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*uq7VvWI97IzIxP9UCem8gw.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_uq7VvWI97IzIxP9UCem8gw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*uq7VvWI97IzIxP9UCem8gw.png"></noscript></div></div></figure><p name="66e4" id="66e4" class="graf graf--p graf-after--figure">In previous section we presented the Reinforce algorithm, which has to complete the episode before we can start training. For the environments, where every episode can last hundreds or even thousands of frames (like Atari games). It can be wasteful for the training perspective, where we have to interact with the environment in long perspective, only just to perform a single training step (in order to estimate Q as accurate as possible). It this case, the training batch becomes very large.</p><p name="5346" id="5346" class="graf graf--p graf-after--p">For the DQN however, it is possible to replace the exact value for a discounted reward with our estimation using the one-step Bellman equation:</p><figure name="15e0" id="15e0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 316px; max-height: 72px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.8%;"></div><img class="graf-image" data-image-id="1*aZ2lrgHL8AHSk4i6X5Ra1A.png" data-width="316" data-height="72" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_aZ2lrgHL8AHSk4i6X5Ra1A.png"></div></figure><p name="0160" id="0160" class="graf graf--p graf-after--figure">When we consider the Policy Gradient method (as we discussed above), we contemplated that the values V(s) or Q(s, a) exist anymore. In this case we apply the Actor — Critic method instead, where we use neural network to estimate V(s) and use this estimation to obtain Q.</p><p name="5bd8" id="5bd8" class="graf graf--p graf-after--p">Estimated gradient in Policy Gradient method is proportional to the discounted reward from the given state. However, the range of this reward is highly environment — dependent (it can happen that the Agent plays only short game — the Agent lose very quick (low value of reward) or the Agent is smart enough and plays the game for the longer time, while collecting the rewards). Large difference between rewards collection can seriously affect the training dynamics, as one lucky episode will dominate in the final gradient. In such occurrences, the policy gradient method has high <strong class="markup--strong markup--p-strong">variance</strong>, which can influence the training process can become unstable).</p><p name="207a" id="207a" class="graf graf--p graf-after--p">In reinforcement learning we look for the bias — variance trade — off (consider below figure), when the Agent tries to estimate value functions or policies from returns (we need to remember that the Agent samples only environment, so we are able to only estimate these expectation). Generally, the main effort in our domain (reinforcement learning) is an attempt to reduce the variance of algorithms while keeping bias to a minimum. The task is hard to achieve, but we will approach to some techniques that are designed to accomplish this.</p><figure name="3667" id="3667" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 264px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*nKs4yrMn9CbaSZ7uaqDnXg.png" data-width="602" data-height="264" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_nKs4yrMn9CbaSZ7uaqDnXg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="31"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*nKs4yrMn9CbaSZ7uaqDnXg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*nKs4yrMn9CbaSZ7uaqDnXg.png"></noscript></div></div></figure><p name="ca33" id="ca33" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">5.</strong> <strong class="markup--strong markup--p-strong">DEEP DETERMINISTIC POLICY GRADIENT (DDPG) algorithm</strong></p><p name="f46e" id="f46e" class="graf graf--p graf-after--p">As it was discussed in Udacity Deep Reinforcement Learning nanoprogram there exist two complimentary ways for estimating expected returns.</p><p name="3d9c" id="3d9c" class="graf graf--p graf-after--p">First is the <strong class="markup--strong markup--p-strong">Monte — Carlo estimate</strong>, which roles out an episode in calculating the discounter total reward from the rewards sequence.</p><p name="3dd5" id="3dd5" class="graf graf--p graf-after--p">In Dynamic Programming, the <strong class="markup--strong markup--p-strong">Markov Decision Process </strong>(<strong class="markup--strong markup--p-strong">MDP</strong>) is solved by using value iteration and policy iteration. Both techniques require transition and reward probabilities to find the optimal policy. When the transition and reward probabilities are unknown, we use the Monte Carlo method to solve MDP. The Monte Carlo method requires only sample sequences of states, actions, and rewards. Monte Carlo methods are applied only to the episodic tasks.</p><p name="99cf" id="99cf" class="graf graf--p graf-after--p">We can approach the Monte — Carlo estimate by considering that the Agent play in episode A. We start in state St and take action At. Based on the process the Agent transits to state St+1. From environment, the Agent receives the reward Rt+1. This process can be continued until the Agent reaches the end of the episode. The Agent can take part also in other episodes like B, C, and D. Some of those episodes will have trajectories that go through the same states, which influences that the value function is computed as average of estimates. Estimates for a state can vary across episodes so the Monte — Carlo estimates will have high variance.</p><p name="8f9a" id="8f9a" class="graf graf--p graf-after--p">On the other side, we can apply the <strong class="markup--strong markup--p-strong">Temporal Difference estimate</strong>. Here, the TD approximates the current estimate based on the previously learned estimate, which is also called <strong class="markup--strong markup--p-strong">bootstrapping</strong> (we try to predict the state values).</p><figure name="78e6" id="78e6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 505px; max-height: 121px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*6QCHeMdt3kYPmnNwYqbeyw.png" data-width="505" data-height="121" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_6QCHeMdt3kYPmnNwYqbeyw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="17"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*6QCHeMdt3kYPmnNwYqbeyw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*6QCHeMdt3kYPmnNwYqbeyw.png"></noscript></div></div></figure><p name="7d33" id="7d33" class="graf graf--p graf-after--figure">Above equation is the difference (<strong class="markup--strong markup--p-strong">TD error</strong>) between the actual reward and the expected reward multiplied by the learning rate alpha (the learning rate, also called step size, used for convergence reason).</p><p name="06e5" id="06e5" class="graf graf--p graf-after--p">TD estimates are low variance because you’re only compounding a single time step of randomness instead of a full rollout like in Monte — Carlo estimate. However, due to applying a bootstrapping (dynamic programming) the next state is only estimated. Estimated values introduce bias into our calculations. The agent will learn faster, but the converging problems can occur.</p><p name="cb7e" id="cb7e" class="graf graf--p graf-after--p">Deriving the Actor — Critic concept requires to consider first the <strong class="markup--strong markup--p-strong">policy — based approach</strong> (performed by <strong class="markup--strong markup--p-strong">AGENT</strong>). As we discussed before the Agent playing the game increases the probability of actions that lead to a win, and decrease the probability of actions that lead to losses. However, such process is cumbersome due to lot of data to approach the optimal policy.</p><p name="d95b" id="d95b" class="graf graf--p graf-after--p">On the other hand, we can evaluate the <strong class="markup--strong markup--p-strong">value — based approach </strong>(performed by<strong class="markup--strong markup--p-strong"> CRITIC</strong>),<strong class="markup--strong markup--p-strong"> </strong>where the guesses are performed on-the-fly, throughout all the episode. At the beginning our guesses will be misaligned (not correct). But over time, when we capture more experience, we will be able to make solid guesses. Though, this is not a perfect approach either, guesses introduce a bias because they’ll sometimes be wrong, particularly because of a lack of experience.</p><p name="58d2" id="58d2" class="graf graf--p graf-after--p">Based on this short analysis we can summarize that the Agent using policy — based approach is learning to act (agent learns by interacting with environment and adjusts the probabilities of good and bad actions, while in a value-based approach, the agent is learning to estimate states and actions.). In parallel we use a Critic, which is to be able to evaluate the quality of actions more quickly (proper action or not) and speed up learning. Actor-critic method is more stable than value — based agents, while requiring fewer training samples than policy-based agents.</p><p name="3b30" id="3b30" class="graf graf--p graf-after--p">As a result of merge Actor — Critic we utilize two separate neural networks. The role of the Actor network is to determine the best actions (from probability distribution) in the state by tuning the parameter θ (weights). The Critic by computing the temporal difference error TD (estimating expected returns), evaluates the action generated by the Actor.</p><p name="070a" id="070a" class="graf graf--p graf-after--p">In previous project (DQN) we discussed about discrete environments where the number of action which could be performed by the Agent was limited. However, very often we operate with continuous environment, securing continuous motion. The number of action then can be unlimited (huge). This is one of the problems DDPG solves. DDPG algorithm uses Agent — Critic concept, where we use two deep neural networks.</p><p name="21f8" id="21f8" class="graf graf--p graf-after--p">In DDPG, the Actor is used to approximate the optimal policy deterministically. That means we want always to generate the best believed action for any given state.</p><p name="cfbf" id="cfbf" class="graf graf--p graf-after--p">The Actor follows the policy-based approach, and learns how to act by directly estimating the optimal policy and maximizing reward through gradient ascent. The Critic however, utilizes the value-based approach and learns how to estimate the value of different state — action pairs.</p><figure name="daff" id="daff" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 621px; max-height: 272px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*ZsfURoPWKsME4XKvhGkC9Q.png" data-width="621" data-height="272" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_ZsfURoPWKsME4XKvhGkC9Q.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="31"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*ZsfURoPWKsME4XKvhGkC9Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*ZsfURoPWKsME4XKvhGkC9Q.png"></noscript></div></div></figure><p name="925a" id="925a" class="graf graf--p graf-after--figure">The DDPG algorithm can be presented as follows (Reinforcement Learning with Python by Sudharsan Ravichandiran):</p><p name="5abc" id="5abc" class="graf graf--p graf-after--p">1. The Actor and the Critic use separate neural network.</p><p name="f1ce" id="f1ce" class="graf graf--p graf-after--p">2. The Actor network with:</p><figure name="f8c2" id="f8c2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 253px; max-height: 64px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.3%;"></div><img class="graf-image" data-image-id="1*rMf74wXA1I43coTyMeWnrA.png" data-width="253" data-height="64" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_rMf74wXA1I43coTyMeWnrA.png"></div></figure><p name="d402" id="d402" class="graf graf--p graf-after--figure">which takes input as a <strong class="markup--strong markup--p-strong">state s</strong> and results in the <strong class="markup--strong markup--p-strong">action a</strong> where</p><figure name="a758" id="a758" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 61px; max-height: 55px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.2%;"></div><img class="graf-image" data-image-id="1*SgD0lAHLajtF5YmIjHEsNg.png" data-width="61" data-height="55" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_SgD0lAHLajtF5YmIjHEsNg.png"></div></figure><p name="4e5c" id="4e5c" class="graf graf--p graf-after--figure">is the Actor network learning weights. The actor here is used to approximate the optimal policy deterministically. That means that the output is the best believed action for any given state. This is unlike a stochastic policy (probability distribution) in which we want the policy to learn a probability distribution over the actions. In DDPG, we want the believed best action every single time we query the actor network. The actor is basically learning the argmax a Q(S, a), which is the best action.</p><p name="11de" id="11de" class="graf graf--p graf-after--p">3. The Critic network</p><figure name="d813" id="d813" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 75px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.8%;"></div><img class="graf-image" data-image-id="1*DLQzEM576I61ep3YFK7IPg.png" data-width="706" data-height="76" data-action="zoom" data-action-value="1*DLQzEM576I61ep3YFK7IPg.png" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_DLQzEM576I61ep3YFK7IPg.png"></div></figure><p name="0b7f" id="0b7f" class="graf graf--p graf-after--figure">which takes an input as a <strong class="markup--strong markup--p-strong">state s</strong> and <strong class="markup--strong markup--p-strong">action a</strong> and returns the <em class="markup--em markup--p-em">Q </em>value where is the Critic network</p><figure name="5c18" id="5c18" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 69px; max-height: 59px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 85.5%;"></div><img class="graf-image" data-image-id="1*w3xvae_LR7l2_Bf4lNoS4w.png" data-width="69" data-height="59" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_w3xvae_LR7l2_Bf4lNoS4w.png"></div></figure><p name="67d5" id="67d5" class="graf graf--p graf-after--figure">weights. The critic learns to evaluate the optimal action value function by using the actors best believed action.</p><p name="1023" id="1023" class="graf graf--p graf-after--p">4. We define a target network for both the Actor network</p><figure name="9293" id="9293" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 196px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.300000000000004%;"></div><img class="graf-image" data-image-id="1*FsAMxzgq0kr1a_MDT6snxw.png" data-width="196" data-height="79" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_FsAMxzgq0kr1a_MDT6snxw.png"></div></figure><p name="2162" id="2162" class="graf graf--p graf-after--figure">and Critic network respectively,</p><figure name="8279" id="8279" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 253px; max-height: 81px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 32%;"></div><img class="graf-image" data-image-id="1*xadJJKgb67WWweCriNmc2g.png" data-width="253" data-height="81" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_xadJJKgb67WWweCriNmc2g.png"></div></figure><p name="849d" id="849d" class="graf graf--p graf-after--figure">where</p><figure name="b933" id="b933" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 178px; max-height: 83px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 46.6%;"></div><img class="graf-image" data-image-id="1*Nb1hks8St5aU5oMJch8sjg.png" data-width="178" data-height="83" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_Nb1hks8St5aU5oMJch8sjg.png"></div></figure><p name="8b0c" id="8b0c" class="graf graf--p graf-after--figure">are the weights of the target Actor and Critic network.</p><p name="a2fa" id="a2fa" class="graf graf--p graf-after--p">5. Next, we perform the update of Actor network weights with policy gradients and the Critic network weight with the gradients calculated from the TD error.</p><p name="b6fe" id="b6fe" class="graf graf--p graf-after--p">6. In order, to select correct action, first we have to add an exploration <strong class="markup--strong markup--p-strong">noise <em class="markup--em markup--p-em">N</em></strong><em class="markup--em markup--p-em"> </em>to the action produced by Actor:</p><figure name="e529" id="e529" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 284px; max-height: 54px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19%;"></div><img class="graf-image" data-image-id="1*Jk2mUgL-rBGLa08kN-gHoQ.png" data-width="284" data-height="54" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_Jk2mUgL-rBGLa08kN-gHoQ.png"></div></figure><p name="7349" id="7349" class="graf graf--p graf-after--figure">(add noise to encourage exploration since policy is deterministic).</p><p name="09b9" id="09b9" class="graf graf--p graf-after--p">7. Selected action in a state, <em class="markup--em markup--p-em">s</em>, receive a reward, <em class="markup--em markup--p-em">r </em>and move to a new state, <em class="markup--em markup--p-em">s’</em>.</p><p name="8922" id="8922" class="graf graf--p graf-after--p">8. We store this transition information in an experience replay buffer.</p><p name="96ac" id="96ac" class="graf graf--p graf-after--p">9. As it is performed while we use DQN algorithm, we sample transitions from the replay buffer and train the network, and then we calculate the target <em class="markup--em markup--p-em">Q </em>value:</p><figure name="0cfc" id="0cfc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 631px; max-height: 58px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 9.2%;"></div><img class="graf-image" data-image-id="1*MgHdRUg85E28y4AQULDgMw.png" data-width="631" data-height="58" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_MgHdRUg85E28y4AQULDgMw.png"></div></figure><p name="2443" id="2443" class="graf graf--p graf-after--figure">10. Then, we can compute the TD error as:</p><figure name="1080" id="1080" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 378px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.2%;"></div><img class="graf-image" data-image-id="1*2dv67i3u1vPYTPyOpkB46w.png" data-width="378" data-height="80" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_2dv67i3u1vPYTPyOpkB46w.png"></div></figure><p name="aa5e" id="aa5e" class="graf graf--p graf-after--figure">11. Subsequently, we perform the update of the Critic networks weights with gradients calculated from this loss <em class="markup--em markup--p-em">L</em>.</p><p name="a57f" id="a57f" class="graf graf--p graf-after--p">12. Then, we update our policy network weights using a policy gradient.</p><p name="726c" id="726c" class="graf graf--p graf-after--p">13. Next, we update the weights of Actor and Critic network in the target network. In DDPG algorithm topology consist of two copies of network weights for each network, (Actor: regular and target) and (Critic: regular and target). In DDPG, the target networks are updated using a <strong class="markup--strong markup--p-strong">soft updates strategy</strong>. A soft update strategy consists of slowly blending regular network weights with target network weights. In means that every time step we make our target network be 99.99 percent of target network weights and only a 0.01 percent of regular network weights (slowly mix of regular network weights into target network weights).</p><p name="faaa" id="faaa" class="graf graf--p graf-after--p">14. We update the weights of the target networks (Agent, Critic) slowly, which promotes greater stability (soft updates strategy):</p><figure name="a83f" id="a83f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 314px; max-height: 116px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*O90_mr594IfWJTb4Bd_MBA.png" data-width="314" data-height="116" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_O90_mr594IfWJTb4Bd_MBA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="27"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*O90_mr594IfWJTb4Bd_MBA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*O90_mr594IfWJTb4Bd_MBA.png"></noscript></div></div></figure><p name="b5e4" id="b5e4" class="graf graf--p graf-after--figure">DDPG algorithm can be expressed in conscience shape as a pseudocode:</p><figure name="f968" id="f968" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 650px; max-height: 488px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*k20x517z-mh_54amzThOHA.png" data-width="650" data-height="488" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_k20x517z-mh_54amzThOHA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="55"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*k20x517z-mh_54amzThOHA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*k20x517z-mh_54amzThOHA.png"></noscript></div></div></figure><p name="ebd1" id="ebd1" class="graf graf--p graf-after--figure">General overview of DDPG algorithm flow was portrayed on underneath chart,</p><figure name="8618" id="8618" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 408px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 58.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*x6HIECcvr60kzSHdDU0zZw.png" data-width="754" data-height="440" data-action="zoom" data-action-value="1*x6HIECcvr60kzSHdDU0zZw.png" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_x6HIECcvr60kzSHdDU0zZw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="43"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*x6HIECcvr60kzSHdDU0zZw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*x6HIECcvr60kzSHdDU0zZw.png"></noscript></div></div></figure><p name="e52a" id="e52a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">6.</strong> <strong class="markup--strong markup--p-strong">PROJECT SETUP. RESULTS.</strong></p><p name="662b" id="662b" class="graf graf--p graf-after--p">In Continuous Control project following setup of neural networks (for Agent and Critic) were involved.</p><p name="dc2f" id="dc2f" class="graf graf--p graf-after--p">Depicted below plot of rewards illustrates that the agent is able to receive an average reward (over 100 episodes, and over all 20 Agents) while playing 206 episodes.</p><figure name="2132" id="2132" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 602px; max-height: 381px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 63.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*30KmcuCecdEQo-aYyqqynQ.png" data-width="602" data-height="381" data-scroll="native"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_30KmcuCecdEQo-aYyqqynQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="47"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*30KmcuCecdEQo-aYyqqynQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*30KmcuCecdEQo-aYyqqynQ.png"></noscript></div></div></figure><p name="ae30" id="ae30" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Applied Deep Neural Network architecture:</strong></p><p name="d95c" id="d95c" class="graf graf--p graf-after--p">Input layer FC1: 37 nodes in, 64 nodes out<br>Hidden layer FC2: 64 nodes in, 64 nodes out<br>Hidden layer FC3: 64 nodes in, 64 nodes out<br>Output layer: 64 nodes in, 4 out — action size</p><p name="aa36" id="aa36" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Applied hyperparameters:</strong></p><p name="62ae" id="62ae" class="graf graf--p graf-after--p">BUFFER_SIZE = int(1e5) # replay buffer size<br>BATCH_SIZE = 64 # minibatch size<br>GAMMA = 0.99 # discount factor<br>TAU = 1e-3 # for soft update of target parameters<br>LR = 5e-4 # learning rate<br>UPDATE_EVERY = 4 # how often to update the network<br>Epsilon start = 1.0<br>Epsilon start = 0.01<br>Epsilon decay = 0.999</p><p name="b1d1" id="b1d1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">7.</strong> <strong class="markup--strong markup--p-strong">IDEAS OF FUTURE WORK</strong></p><p name="7992" id="7992" class="graf graf--p graf-after--p">The discussed work compounds contemporary advances in deep learning and reinforcement learning. Modern combination yields exceptional results in solving challenging AI issues across a variety of domains. However, in this project<strong class="markup--strong markup--p-strong"> </strong>a few limitations to presented approach remain. Future work should concentrate mainly on following tasks. Most notably, which was treated as a minor case was tuning of the <strong class="markup--strong markup--p-strong">hyper parameters</strong>. Future work can include more parameter choice and checks in order to limit number of episodes exhausting the project goal. This can also comprise the deep neural <strong class="markup--strong markup--p-strong">network architecture design</strong>. The other thing which will be reasonable to verify is the choice of applied algorithm. Here, it is suggest to deploy the A3C (<strong class="markup--strong markup--p-strong">Asynchronous Advantage Actor — Critic</strong>) algorithm which exploits multiple agents. Each Agent has its own set of network parameters. Additionally, the Agent interacts (in parallel to the other Agents) with its own copy of the environment. Agents (workers) follows a different exploration strategy to learn an optimal policy. Following this, the Agents compute value and policy loss so the derived gradient can be applied the global network. Proceeding process is continued and the advantage function is estimated. Finally, the global value loss and policy loss are computed. The policy loss function includes the entropy component, which reveals the spread of action probabilities. Low entropy secures that one of the actions has a higher probability then the other actions, so the Agent has a good opportunity for convenient action selection. Although, the high entropy displays that every action’s probability is the same, so the Agent will be unsure as to which action to perform. We use the entropy to improve exploration, by encouraging the Agent to be reliable of excavation of perfect (optimal) action. A3C algorithm can sufficiently speed up the training process.</p><p name="c89b" id="c89b" class="graf graf--p graf-after--p graf--trailing">Please, find the full code for this project on my <a href="https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p2_continuous-control" data-href="https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p2_continuous-control" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Github</a>.</p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://medium.com/tag/reinforcement-learning?source=post" data-action-source="post">Reinforcement Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/artificial-intelligence?source=post" data-action-source="post">Artificial Intelligence</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/programming?source=post" data-action-source="post">Programming</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/robotics?source=post" data-action-source="post">Robotics</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5a823da91b43" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----5a823da91b43---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboarding" data-action="multivote" data-action-value="5a823da91b43" data-action-type="long-press" data-action-source="post_actions_footer-----5a823da91b43---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="5a823da91b43"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker" data-action="show-recommends" data-action-value="5a823da91b43">65 claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/5a823da91b43/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/5a823da91b43/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-marginRight12" data-action="scroll-to-responses">2</button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5a823da91b43" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="c9076eed918c"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="toggle-block-user" data-action-value="c9076eed918c" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="toggle-subscribe-user" data-action-value="c9076eed918c" data-action-source="footer_card-c9076eed918c-------------------------follow_footer" data-subscribe-source="footer_card" data-follow-context-entity-id="5a823da91b43"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://medium.com/@markus.x.buchholz?source=footer_card" title="Go to the profile of Markus Buchholz" aria-label="Go to the profile of Markus Buchholz" data-action-source="footer_card" data-user-id="c9076eed918c" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_12k3P8bjeeTvt1EoxSTDEg(1).jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Markus Buchholz"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 12px); height: calc(100% + 12px); top:-6px; left:-6px"><svg viewBox="0 0 114 114" xmlns="http://www.w3.org/2000/svg"><path d="M7.66922967,32.092726 C17.0070768,13.6353618 35.9421928,1.75 57,1.75 C78.0578072,1.75 96.9929232,13.6353618 106.33077,32.092726 L107.66923,31.4155801 C98.0784505,12.4582656 78.6289015,0.25 57,0.25 C35.3710985,0.25 15.9215495,12.4582656 6.33077033,31.4155801 L7.66922967,32.092726 Z"></path><path d="M106.33077,81.661427 C96.9929232,100.118791 78.0578072,112.004153 57,112.004153 C35.9421928,112.004153 17.0070768,100.118791 7.66922967,81.661427 L6.33077033,82.338573 C15.9215495,101.295887 35.3710985,113.504153 57,113.504153 C78.6289015,113.504153 98.0784505,101.295887 107.66923,82.338573 L106.33077,81.661427 Z"></path></svg></div></div></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://medium.com/@markus.x.buchholz" property="cc:attributionName" title="Go to the profile of Markus Buchholz" aria-label="Go to the profile of Markus Buchholz" rel="author cc:attributionUrl" data-user-id="c9076eed918c" dir="auto">Markus Buchholz</a></h3><div class="ui-caption u-textColorGreenNormal u-fontSize13 u-tintSpectrum u-accentColor--textNormal u-marginBottom7">Medium member since Jul 2018</div></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="5a823da91b43" data-scroll="native"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1032 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="5e99cae0abb8" data-source="placement_card_footer_grid---------0-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/1*Kbl-s1V9jOHXm4-37_rAJg.jpeg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Also tagged Reinforcement Learning</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Reinforcement Learning — Model Based Planning Methods</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@zhangyue9306" data-action="show-user-card" data-action-value="f37783fc8c26" data-action-type="hover" data-user-id="f37783fc8c26" data-collection-slug="towards-data-science" dir="auto"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/0_OHP4EDyFQhdpx4uk_" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jeremy Zhang"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@zhangyue9306?source=placement_card_footer_grid---------0-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-43" data-action-value="f37783fc8c26" data-action-type="hover" data-user-id="f37783fc8c26" data-collection-slug="towards-data-science" dir="auto">Jeremy Zhang</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8?source=placement_card_footer_grid---------0-43" data-action="open-post" data-action-value="https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8?source=placement_card_footer_grid---------0-43" data-action-source="preview-listing"><time datetime="2019-07-06T08:18:36.750Z">Jul 6</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="7 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5e99cae0abb8" data-is-label-padded="true" data-source="placement_card_footer_grid-----5e99cae0abb8----0-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="5e99cae0abb8" data-action-type="long-press" data-action-source="placement_card_footer_grid-----5e99cae0abb8----0-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="5e99cae0abb8">90</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5e99cae0abb8" data-action-source="placement_card_footer_grid-----5e99cae0abb8----0-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="7400cc67b8d9" data-source="placement_card_footer_grid---------1-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------1-43" data-action-source="placement_card_footer_grid---------1-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/0*KfHijq1bO1nDV5Dl.jpg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------1-43" data-action-source="placement_card_footer_grid---------1-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Also tagged Programming</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Predicting Who’s going to Survive on Titanic Dataset</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@vincentkernn" data-action="show-user-card" data-action-value="9848578f8495" data-action-type="hover" data-user-id="9848578f8495" data-collection-slug="towards-data-science" dir="auto"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/2_5VLESjo09eI1gMxevfyehg.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Vincent Tatan"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@vincentkernn?source=placement_card_footer_grid---------1-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-43" data-action-value="9848578f8495" data-action-type="hover" data-user-id="9848578f8495" data-collection-slug="towards-data-science" dir="auto">Vincent Tatan</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------1-43" data-action="open-post" data-action-value="https://towardsdatascience.com/predicting-whos-going-to-survive-on-titanic-dataset-7400cc67b8d9?source=placement_card_footer_grid---------1-43" data-action-source="preview-listing"><time datetime="2019-07-09T01:05:39.793Z">Jul 9</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="11 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="7400cc67b8d9" data-is-label-padded="true" data-source="placement_card_footer_grid-----7400cc67b8d9----1-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="7400cc67b8d9" data-action-type="long-press" data-action-source="placement_card_footer_grid-----7400cc67b8d9----1-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="7400cc67b8d9">3</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="7400cc67b8d9" data-action-source="placement_card_footer_grid-----7400cc67b8d9----1-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="cb9e41ff1f0d" data-source="placement_card_footer_grid---------2-60" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/model-based-reinforcement-learning-cb9e41ff1f0d?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/500/150/0*wkLXhdoIrI1Lv8ml&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/model-based-reinforcement-learning-cb9e41ff1f0d?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Related reads</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Model Based Reinforcement Learning</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@zsalloum" data-action="show-user-card" data-action-value="1f2b933522e2" data-action-type="hover" data-user-id="1f2b933522e2" data-collection-slug="towards-data-science" dir="auto"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_kAENWVwzuw04FVoOuN1ZSw.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Ziad SALLOUM"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@zsalloum?source=placement_card_footer_grid---------2-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-60" data-action-value="1f2b933522e2" data-action-type="hover" data-user-id="1f2b933522e2" data-collection-slug="towards-data-science" dir="auto">Ziad SALLOUM</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/model-based-reinforcement-learning-cb9e41ff1f0d?source=placement_card_footer_grid---------2-60" data-action="open-post" data-action-value="https://towardsdatascience.com/model-based-reinforcement-learning-cb9e41ff1f0d?source=placement_card_footer_grid---------2-60" data-action-source="preview-listing"><time datetime="2019-03-20T23:30:30.366Z">Mar 21</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="8 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="cb9e41ff1f0d" data-is-label-padded="true" data-source="placement_card_footer_grid-----cb9e41ff1f0d----2-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="cb9e41ff1f0d" data-action-type="long-press" data-action-source="placement_card_footer_grid-----cb9e41ff1f0d----2-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="cb9e41ff1f0d">51</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="cb9e41ff1f0d" data-action-source="placement_card_footer_grid-----cb9e41ff1f0d----2-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream-editor cardChromeless u-marginBottom20 u-paddingLeft20 u-paddingRight20 js-responsesStreamEditor"><div class="inlineNewPostControl js-inlineNewPostControl" data-action-scope="_actionscope_12"><div class="inlineEditor is-collapsed is-postEditMode js-inlineEditor" data-action="focus-editor"><div class="u-paddingTop20 js-block js-inlineEditorContent"><div class="inlineEditor-header"><div class="inlineEditor-avatar u-paddingRight20"><div class="avatar u-inline"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/0_toB60eKEH3klSlAD(1).jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Vivek Agrawal"></div></div><div class="inlineEditor-headerContent"><div class="inlineEditor-placeholder js-inlineEditorPrompt">Write a response…</div><div class="inlineEditor-author u-accentColor--textNormal">Vivek Agrawal</div></div></div></div></div></div></div><div class="responsesStream js-responsesStream"><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_6"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation between <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@alextorex" data-action="show-user-card" data-action-value="55b53e322e17" data-action-type="hover" data-user-id="55b53e322e17" dir="auto">Alex Torex</a> and <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@markus.x.buchholz" data-action="show-user-card" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto">Markus Buchholz</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="9aa2a2aea850" data-source="responses---------0-----------------------" data-action-scope="_actionscope_7" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@alextorex" data-action="show-user-card" data-action-value="55b53e322e17" data-action-type="hover" data-user-id="55b53e322e17" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_dmbNkD5D-u45r44go_cf0g.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Alex Torex"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@alextorex?source=responses---------0-----------------------" data-action="show-user-card" data-action-source="responses---------0-----------------------" data-action-value="55b53e322e17" data-action-type="hover" data-user-id="55b53e322e17" dir="auto">Alex Torex</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@alextorex/however-such-process-is-cumbersome-due-to-lot-of-data-to-approach-the-optimal-policy-9aa2a2aea850?source=responses---------0-----------------------" data-action="open-post" data-action-value="https://medium.com/@alextorex/however-such-process-is-cumbersome-due-to-lot-of-data-to-approach-the-optimal-policy-9aa2a2aea850?source=responses---------0-----------------------" data-action-source="preview-listing"><time datetime="2019-03-17T06:20:58.598Z">Mar 17</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@alextorex/however-such-process-is-cumbersome-due-to-lot-of-data-to-approach-the-optimal-policy-9aa2a2aea850?source=responses---------0-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="fc72" id="fc72" class="graf graf--p graf--startsWithDoubleQuote graf--leading">“However, such process is cumbersome due to lot of data to approach the optimal policy.”</p><p name="46e2" id="46e2" class="graf graf--p graf-after--p graf--trailing">What does this mean? Seems a mistake in formulation.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="9aa2a2aea850" data-is-flush-left="true" data-source="listing-----9aa2a2aea850---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="9aa2a2aea850" data-action-type="long-press" data-action-source="listing-----9aa2a2aea850---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents" data-action="show-recommends" data-action-value="9aa2a2aea850">1</button></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@alextorex/however-such-process-is-cumbersome-due-to-lot-of-data-to-approach-the-optimal-policy-9aa2a2aea850?source=responses---------0-----------------------#--responses" data-action-source="responses---------0-----------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="9aa2a2aea850" data-action-source="listing-----9aa2a2aea850---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="9aa2a2aea850"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="cd308023f398" data-source="responses---------0-----------------------" data-action-scope="_actionscope_8" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@markus.x.buchholz" data-action="show-user-card" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_12k3P8bjeeTvt1EoxSTDEg(2).jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Markus Buchholz"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@markus.x.buchholz?source=responses---------0-----------------------" data-action="show-user-card" data-action-source="responses---------0-----------------------" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto">Markus Buchholz</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@markus.x.buchholz/i-revised-this-sentence-thanks-cd308023f398?source=responses---------0-----------------------" data-action="open-post" data-action-value="https://medium.com/@markus.x.buchholz/i-revised-this-sentence-thanks-cd308023f398?source=responses---------0-----------------------" data-action-source="preview-listing"><time datetime="2019-03-17T09:26:21.334Z">Mar 17</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@markus.x.buchholz/i-revised-this-sentence-thanks-cd308023f398?source=responses---------0-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9723" id="9723" class="graf graf--p graf--leading graf--trailing">I revised this sentence. Thanks!</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="cd308023f398" data-is-flush-left="true" data-source="listing-----cd308023f398---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="cd308023f398" data-action-type="long-press" data-action-source="listing-----cd308023f398---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="cd308023f398" data-action-source="listing-----cd308023f398---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="cd308023f398"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_9"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation between <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@alextorex" data-action="show-user-card" data-action-value="55b53e322e17" data-action-type="hover" data-user-id="55b53e322e17" dir="auto">Alex Torex</a> and <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@markus.x.buchholz" data-action="show-user-card" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto">Markus Buchholz</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="5e30ed97cb00" data-source="responses---------1-----------------------" data-action-scope="_actionscope_10" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@alextorex" data-action="show-user-card" data-action-value="55b53e322e17" data-action-type="hover" data-user-id="55b53e322e17" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_dmbNkD5D-u45r44go_cf0g.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Alex Torex"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@alextorex?source=responses---------1-----------------------" data-action="show-user-card" data-action-source="responses---------1-----------------------" data-action-value="55b53e322e17" data-action-type="hover" data-user-id="55b53e322e17" dir="auto">Alex Torex</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@alextorex/12-then-we-update-our-policy-network-weights-using-a-policy-gradient-5e30ed97cb00?source=responses---------1-----------------------" data-action="open-post" data-action-value="https://medium.com/@alextorex/12-then-we-update-our-policy-network-weights-using-a-policy-gradient-5e30ed97cb00?source=responses---------1-----------------------" data-action-source="preview-listing"><time datetime="2019-03-17T06:47:02.782Z">Mar 17</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@alextorex/12-then-we-update-our-policy-network-weights-using-a-policy-gradient-5e30ed97cb00?source=responses---------1-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3508" id="3508" class="graf graf--p graf--leading">12. Then, we update our policy network weights using a policy gradient.</p><p name="715f" id="715f" class="graf graf--p graf-after--p graf--trailing">Equation for this step is not explained. This is the second article I read that does not explain this equation. Seems a big conspiration.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5e30ed97cb00" data-is-flush-left="true" data-source="listing-----5e30ed97cb00---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="5e30ed97cb00" data-action-type="long-press" data-action-source="listing-----5e30ed97cb00---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents" data-action="show-recommends" data-action-value="5e30ed97cb00">1</button></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@alextorex/12-then-we-update-our-policy-network-weights-using-a-policy-gradient-5e30ed97cb00?source=responses---------1-----------------------#--responses" data-action-source="responses---------1-----------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5e30ed97cb00" data-action-source="listing-----5e30ed97cb00---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="5e30ed97cb00"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackPostPresentation js-trackPostScrolls postArticle--short" data-post-id="1fe298659042" data-source="responses---------1-----------------------" data-action-scope="_actionscope_11" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@markus.x.buchholz" data-action="show-user-card" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/1_12k3P8bjeeTvt1EoxSTDEg(2).jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Markus Buchholz"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@markus.x.buchholz?source=responses---------1-----------------------" data-action="show-user-card" data-action-source="responses---------1-----------------------" data-action-value="c9076eed918c" data-action-type="hover" data-user-id="c9076eed918c" dir="auto">Markus Buchholz</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@markus.x.buchholz/the-policy-gradient-equation-is-shown-in-pseudo-code-1fe298659042?source=responses---------1-----------------------" data-action="open-post" data-action-value="https://medium.com/@markus.x.buchholz/the-policy-gradient-equation-is-shown-in-pseudo-code-1fe298659042?source=responses---------1-----------------------" data-action-source="preview-listing"><time datetime="2019-03-17T09:54:40.114Z">Mar 17</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@markus.x.buchholz/the-policy-gradient-equation-is-shown-in-pseudo-code-1fe298659042?source=responses---------1-----------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9f70" id="9f70" class="graf graf--p graf--leading graf--trailing">The policy gradient equation is shown in pseudo code. You can find the best explanation in orgin DeepMind <span class="markup--anchor markup--p-anchor" data-action="open-inner-link" data-action-value="https://arxiv.org/pdf/1509.02971.pdf">paper</span>. (section 3). It is similar to Reinforce algorithmm which I discussed in my post.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="1fe298659042" data-is-flush-left="true" data-source="listing-----1fe298659042---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="1fe298659042" data-action-type="long-press" data-action-source="listing-----1fe298659042---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@markus.x.buchholz/the-policy-gradient-equation-is-shown-in-pseudo-code-1fe298659042?source=responses---------1-----------------------#--responses" data-action-source="responses---------1-----------------------">2 responses</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="1fe298659042" data-action-source="listing-----1fe298659042---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="1fe298659042"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div></div><div class="container u-hide js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-fixed u-sm-hide js-postShareWidget u-transition--fadeOut300" data-scroll="fixed" style="transform: translateY(150px);"><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5a823da91b43" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----5a823da91b43---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="5a823da91b43" data-action-type="long-press" data-action-source="post_share_widget-----5a823da91b43---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="5a823da91b43">65</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5a823da91b43" data-action-source="post_share_widget-----5a823da91b43---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/5a823da91b43/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/5a823da91b43/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a></li></ul></div></aside><div class="highlightMenu" data-action-scope="_actionscope_3"><div class="highlightMenu-inner"><div class="buttonSet"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="quote" data-action-source="quote_menu--------------------------highlight_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="quote-respond" data-action-source="quote_menu--------------------------respond_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless button--highlightMenu js-highlightMenuTwitterShare" href="https://medium.com/p/5a823da91b43/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action="twitter"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="highlight" data-action-source="quote_menu--------------------------privatenote_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"38048-00ee67b","currentUser":{"userId":"be025283e717","username":"vivekagr12199","name":"Vivek Agrawal","email":"vivekagr12199@gmail.com","imageId":"0*toB60eKEH3klSlAD.jpg","createdAt":1555886025012,"lastPostCreatedAt":1562172632717,"isVerified":true,"subscriberEmail":"","onboardingStatus":1,"googleAccountId":"101203869735328059129","googleEmail":"vivekagr12199@gmail.com","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":true,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":true,"isWriterProgramOptedOut":false,"writerProgramVersion":5,"writerProgramEnrolledAt":1555886025012,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":true},"currentUserHasUnverifiedEmail":false,"isAuthenticated":true,"isCurrentUserVerified":true,"language":"en-in","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.yPFOKEJjWpZVGvh54bK7SQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.cUeGLoj7sHNpuw-bFOuTeQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.6FOGY6ouqNMySk5yHMmW3A.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.d2Ua48DeSXmhnx6BikN_Iw.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.lGxiZyGF1luZqFEkF5YgGw.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.RzL89au1QllN2FPu92pO7w.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.cg-IrTO8WysnMg0c__FJHA.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.b2XUkpbaZgVu0fuO48AaIw.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.wvhZeq_iZ2e-AecOAxjiBg.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.qf6Gl40snMZcEXwnIYeTVw.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.WhICKSRsJMorvucd3Nl7Jg.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1562635554234:4d59a07b2eac","useragent":{"browser":"chrome","family":"chrome","os":"windows","version":75,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":1,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"isSeoBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_international_tax_withholding":true,"enable_international_tax_withholding_documentation":true,"enable_revised_first_partner_program_distro_on_email":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_primary_topic_for_mobile":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_embedding_based_diversification":true,"enable_pub_newsletters":true,"enable_lite_pub_header_menu":true,"enable_lite_claps":true,"enable_lite_post_manager_gear_menu":true,"enable_live_user_post_scoring":true,"enable_lite_post_highlights":true,"enable_lite_post_highlights_view_only":true,"enable_tick_landing_page":true,"enable_lite_private_notes":true,"enable_trumpland_landing_page":true,"enable_lite_email_sign_in_flow":true,"enable_daily_read_digest_promo":true,"enable_lite_paywall_alert":true,"enable_serve_recs_from_ml_rank_homepage":true,"enable_serve_recs_from_ml_rank_digest":true,"enable_serve_recs_from_ml_rank_app_highlights":true,"enable_lite_google_captcha":true,"enable_lite_branch_io":true,"enable_lite_notifications":true,"enable_ticks_digest_promo":true,"enable_lite_verify_email_butter_bar":true,"enable_lite_unread_notification_count":true},"xsrfToken":"KdR9CGheyWKQ","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"57f7414d6a6dc197\",\"ot-tracer-traceid\":\"0ffdc4e34dfa6c54\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"li_post_page","type":0,"url":"www.calendly.com"},{"promptId":"li_home_page","type":1,"url":"mediumuserfeedback.typeform.com/to/GcFjEO"},{"promptId":"li_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"IN","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618"]}}
// ]]></script><script charset="UTF-8" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/main-base.bundle.yPFOKEJjWpZVGvh54bK7SQ.js.download" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"5a823da91b43","versionId":"384b98504414","creatorId":"c9076eed918c","creator":{"userId":"c9076eed918c","name":"Markus Buchholz","username":"markus.x.buchholz","createdAt":1501836488487,"imageId":"1*12k3P8bjeeTvt1EoxSTDEg.jpeg","backgroundImageId":"","bio":"","twitterScreenName":"","socialStats":{"userId":"c9076eed918c","usersFollowedCount":77,"usersFollowedByCount":19,"type":"SocialStats"},"social":{"userId":"be025283e717","targetUserId":"c9076eed918c","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":1532419693000,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollectionId":"","title":"Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm.","detectedLanguage":"en","latestVersion":"384b98504414","latestPublishedVersion":"384b98504414","hasUnpublishedEdits":false,"latestRev":739,"createdAt":1551312690042,"updatedAt":1559489518450,"acceptedAt":0,"firstPublishedAt":1552757329005,"latestPublishedAt":1559489516297,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"GOAL","bodyModel":{"paragraphs":[{"name":"05c8","type":3,"text":"Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algorithm.","markups":[]},{"name":"fe0e","type":4,"text":"Credit: Maxuser","markups":[],"layout":1,"metadata":{"id":"1*_SHO6We5laYHoqOtAR-IEw.png","originalWidth":1951,"originalHeight":1091,"isFeatured":true}},{"name":"4ef1","type":10,"text":"GOAL","markups":[{"type":1,"start":0,"end":4}]},{"name":"4dbd","type":1,"text":"In this project, the goal was to train the 20 Agents (each Agent controls a double — joint robot arm) to maintain its position at the target location for as many time steps as possible. \n The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1. Project requirement is to get average score of +30 over 100 consecutive episodes.","markups":[]},{"name":"354b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*3RJDOwDKWpMn3omvA3zL5Q.png","originalWidth":602,"originalHeight":329}},{"name":"6600","type":1,"text":"2. INTRODUCTION","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":3,"end":15}]},{"name":"3d30","type":1,"text":"In previous project we use Q function to find the optimal policy (as the Q function) which approximates (in use of Q table and later deep neural network) selection of the best action to perform in every state. Formally, this could be formulated as,","markups":[{"type":3,"start":12,"end":19,"href":"https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p1_navigation","title":"","rel":"","anchorType":0}]},{"name":"ddf1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*jwwqpivvnWlwyJWJ6vcJ8w.png","originalWidth":262,"originalHeight":39}},{"name":"2e34","type":1,"text":"which means that the result of our policy π at every state s is the action with the largest Q (value function).","markups":[{"type":2,"start":42,"end":44},{"type":2,"start":59,"end":61}]},{"name":"e153","type":1,"text":"However, in both cases, whether we used a table for small state spaces or a neural network for much larger state spaces, we had to first estimate the optimal action value function before we could tackle the optimal policy. As we can depicted (above equation), when we apply Q-learning we endeavour for the optimal policy by looking for the largest value function. The recent idea here will be to find optimal policy directly without worrying about a value function at all.","markups":[]},{"name":"f86f","type":1,"text":"There are two main reasons why policy approach is more sufficient attitude to explore.","markups":[]},{"name":"a5b3","type":1,"text":"First of all, we care more about the total reward but not always about the highest values function (like DQN) at every state. To obtain these values (action and state), we have used the Bellman equation, which expresses the value on the current step via the values on the next step. Normally, when we must solve real, complex problems and the agent obtains the observation from the environment and needs to decide about what to do next, we need policy, not the value of the state or particular action. We need to know what to do next at each step (the action taken based on the highest value cannot be always optimal).","markups":[{"type":1,"start":0,"end":12}]},{"name":"5030","type":1,"text":"Another reason why policies may be more attractive than values is the environments with extreme number of actions or with a continuous action space.","markups":[{"type":1,"start":0,"end":14}]},{"name":"3b44","type":1,"text":"To be able to decide on the best action, we need to solve an optimization problem finding action a, which maximizes Q(s, a). In the case of an Atari game with several discrete actions, such attitude wasn’t a problem: we just approximated values of all actions and took the action with the largest Q (as argmax ). If our action space is not a small discrete set, but has a scalar value attached to it, such as the steering wheel angle (continuous value which should be regulated), this optimization problem becomes hard, as Q is usually represented by a highly nonlinear neural network (NN), so finding the argument which maximizes the function’s values can be treacherous. In such cases, it’s much more feasible to avoid values function and pay more attention on policy approach.","markups":[{"type":2,"start":297,"end":299},{"type":2,"start":523,"end":525}]},{"name":"a3c8","type":1,"text":"The consecutive question comes straight forward. We need to answer how technically can we approach this idea of estimating an optimal policy?","markups":[]},{"name":"4bf8","type":1,"text":"3. POLICY GRADIENTS. Reinforce algorithm","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":3,"end":40}]},{"name":"8e84","type":1,"text":"For our learning reason we can consider cart pole environment. In this case, the Agent base on the feedback from the environment (car position, car velocity, pole angle and the pole speed at the tip) decides about the one of the possible action. The Agent can, at each time step push the cart either left or right. In order to approximate the policy, as we made that in DQN we construct a neural network, that accepts the state as input.","markups":[]},{"name":"857b","type":4,"text":"Neural network approximates the policy","markups":[{"type":2,"start":0,"end":38}],"layout":1,"metadata":{"id":"1*-NHXgB50gmUGwgBJEIJuJw.png","originalWidth":602,"originalHeight":385}},{"name":"5fec","type":1,"text":"This time however, as output, the neural network returns the probability that the agent selects each possible action. The agent follows this policy and interacts with the environment by just passing the most recent state to the network. Network generates the action probabilities (probability distribution) and then the Agent samples from those probabilities to select an action as response (left or right).","markups":[{"type":1,"start":49,"end":72}]},{"name":"53cc","type":1,"text":"We need to remember that when we worked with DQN, the output of the network were Q-values, so if one of the action was converged to value of 0.3 and another action converged to value of 0.5, so due to our “strategy” argmax the action with higher value was preferred 100% of the time. Actually, we consider the probability distribution, so if the first action has a probability of 0.3 and the second 0.5, so the Agent can take the first action with 30% chance and the second with 50% chance.","markups":[{"type":1,"start":310,"end":334}]},{"name":"b6a0","type":1,"text":"Our objective then is to establish appropriate values for the network weights so that for each state that we pass into the neural network it returns action probabilities where the optimal action is most likely to be selected (has the highest probability). As the Agent interacts with the environment and learns more about which action is best for maximizing reward, it changes neural network weights. Changes of neural network weights happen in the rhythm of gradient update in a such a way that actions yielding high reward in a state will have a high probability and actions yielding low reward will have a low probability (transition of weights will increase the probability for the actions where the episode finishes with success or decrease the action probability where the episode finish with lost).","markups":[]},{"name":"2bb0","type":1,"text":"Approaching our solution in finding the optimal policy we need to subsequently define trajectory (τ), which can be expressed as a sequence of the length H (as Horizon) of the states and actions. As R(τ) we can define the reward for the trajectories we are considering.","markups":[{"type":1,"start":86,"end":97}]},{"name":"a991","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*AR1L9XM7vSCkf8bwvKUzwg.png","originalWidth":407,"originalHeight":66}},{"name":"10db","type":1,"text":"The goal here is still the same, we need to find the weights θ of the neural network that maximize expected return — U. As we discussed, we adjust the network weights.","markups":[]},{"name":"08cf","type":1,"text":"We can express U as following function,","markups":[]},{"name":"b18c","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*w-W2w0_y-ZUc7U4HmJjxsg.png","originalWidth":311,"originalHeight":83}},{"name":"6742","type":1,"text":"where R(τ) is a just the return corresponding to an arbitrary Trajectory τ. The other component in above formula P(τ; θ) is the probability of each possible Trajectory.","markups":[{"type":1,"start":116,"end":117},{"type":1,"start":123,"end":124}]},{"name":"6208","type":1,"text":"We can see that probability depends on the weights θ in the neural network (θ defines the policy","markups":[]},{"name":"bc90","type":1,"text":"which is used to select the actions in the Trajectory). As we discussed before, our goal is to find the value of θ that maximizes expected return. To do so we can compute the Gradient Ascent (opposite to gradient descent which we use to find the minimum of the function).","markups":[]},{"name":"cda0","type":1,"text":"Here, we need to perceive that in computation of the real value of gradient ascent stipulates that we need to evaluate all possible trajectories. It seems that this process can be very computationally expensive, so our approach here will be to just sample a few trajectories (m) using the policy and then use those m — trajectories only to estimate the gradient.","markups":[]},{"name":"b500","type":1,"text":"Finally, the gradient can be express as follows,","markups":[]},{"name":"d0a7","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*HCQUckMaM2CwKp2B9gyyRQ.png","originalWidth":602,"originalHeight":105}},{"name":"7726","type":1,"text":"Once we have an estimate for the gradient, we can use it to update the weights of the policy. Then, we repeatedly loop over these steps to converge to the weights of the optimal policy.","markups":[]},{"name":"43d6","type":1,"text":"Here we need to understand/recall one more thing. The output from the neural network is just the number (when 2 nodes like in the our Cart Pole example, we have two numbers). Computation of probability distribution involves the need of softmax function exertion (function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities).","markups":[{"type":2,"start":304,"end":305},{"type":2,"start":384,"end":385}]},{"name":"9b83","type":1,"text":"Other thing is the log of probability π, which refers to the policy, which is parameterized by theta. This mathematical operation can be understood as the standard cross entropy operation, which is used to to quantify the difference between two probability distributions. Finally, we can estimate how we should change the weights of the policy theta, if we want to change the log probability (remember what we specified before. The agent changes the weights to increase the probability for the actions where the episode finishes with success or decrease the probability of the actions, where the episode finish with lost).","markups":[]},{"name":"6c17","type":1,"text":"The pseudo code of Reinforce algorithm can be summarize as follows.","markups":[{"type":1,"start":19,"end":38}]},{"name":"77ee","type":10,"text":"Use the policy πθ​ to collect M — trajectories with horizon H.","markups":[{"type":2,"start":15,"end":17}]},{"name":"8638","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*lJlxNpXhs_g0GDKR2r-R8g.png","originalWidth":416,"originalHeight":96}},{"name":"d58e","type":1,"text":"2. Use the trajectories to estimate the policy gradient:","markups":[]},{"name":"caad","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*HCQUckMaM2CwKp2B9gyyRQ.png","originalWidth":602,"originalHeight":105}},{"name":"c2c1","type":1,"text":"3. Update the neural network weights of the policy ( α — is a hyper parameter of neural network, step — learning rate):","markups":[]},{"name":"d311","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*nzpUjjIGF4QG2K_QD7nU4w.png","originalWidth":170,"originalHeight":77}},{"name":"83d3","type":1,"text":"4. Loop over steps 1–3.","markups":[]},{"name":"b3c6","type":1,"text":"4. ACTOR — CRITIC METODS","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":3,"end":24}]},{"name":"0018","type":1,"text":"Our goal is to apply DDPG algorithm but due to complexity we should first approach our design throughout understanding new (discussed below) concepts.","markups":[]},{"name":"a922","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*SwQBktXlTXFxvzDyeFQp0g.png","originalWidth":377,"originalHeight":266}},{"name":"6fcf","type":1,"text":"Above picture depicts the main concept of Actor -Critic methods. Here Actor — Critic methods combine the value-based methods such as DQN and policy-based methods such as Reinforce.","markups":[]},{"name":"1e96","type":1,"text":"Previously we defined DQN Agent (in project 1) which learns to approximate the optimal action value function. If the Agent learns sufficiently well so deriving a good policy for the Agent it is straightforward.","markups":[]},{"name":"ee3b","type":1,"text":"On the other side the Reinforce Agent parameterizes the policy and learns to optimize it directly.","markups":[]},{"name":"d0c7","type":1,"text":"Here, the policy is usually stochastic, as we receive the distribution probability.","markups":[]},{"name":"1e9f","type":1,"text":"Right now, we will investigate deterministic policies, which take a state and return the single action (no stochasticity, the policy will be deterministic).","markups":[{"type":1,"start":31,"end":53}]},{"name":"3da7","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*uq7VvWI97IzIxP9UCem8gw.png","originalWidth":396,"originalHeight":289}},{"name":"66e4","type":1,"text":"In previous section we presented the Reinforce algorithm, which has to complete the episode before we can start training. For the environments, where every episode can last hundreds or even thousands of frames (like Atari games). It can be wasteful for the training perspective, where we have to interact with the environment in long perspective, only just to perform a single training step (in order to estimate Q as accurate as possible). It this case, the training batch becomes very large.","markups":[]},{"name":"5346","type":1,"text":"For the DQN however, it is possible to replace the exact value for a discounted reward with our estimation using the one-step Bellman equation:","markups":[]},{"name":"15e0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*aZ2lrgHL8AHSk4i6X5Ra1A.png","originalWidth":316,"originalHeight":72}},{"name":"0160","type":1,"text":"When we consider the Policy Gradient method (as we discussed above), we contemplated that the values V(s) or Q(s, a) exist anymore. In this case we apply the Actor — Critic method instead, where we use neural network to estimate V(s) and use this estimation to obtain Q.","markups":[]},{"name":"5bd8","type":1,"text":"Estimated gradient in Policy Gradient method is proportional to the discounted reward from the given state. However, the range of this reward is highly environment — dependent (it can happen that the Agent plays only short game — the Agent lose very quick (low value of reward) or the Agent is smart enough and plays the game for the longer time, while collecting the rewards). Large difference between rewards collection can seriously affect the training dynamics, as one lucky episode will dominate in the final gradient. In such occurrences, the policy gradient method has high variance, which can influence the training process can become unstable).","markups":[{"type":1,"start":581,"end":589}]},{"name":"207a","type":1,"text":"In reinforcement learning we look for the bias — variance trade — off (consider below figure), when the Agent tries to estimate value functions or policies from returns (we need to remember that the Agent samples only environment, so we are able to only estimate these expectation). Generally, the main effort in our domain (reinforcement learning) is an attempt to reduce the variance of algorithms while keeping bias to a minimum. The task is hard to achieve, but we will approach to some techniques that are designed to accomplish this.","markups":[]},{"name":"3667","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*nKs4yrMn9CbaSZ7uaqDnXg.png","originalWidth":602,"originalHeight":264}},{"name":"ca33","type":1,"text":"5. DEEP DETERMINISTIC POLICY GRADIENT (DDPG) algorithm","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":3,"end":54}]},{"name":"f46e","type":1,"text":"As it was discussed in Udacity Deep Reinforcement Learning nanoprogram there exist two complimentary ways for estimating expected returns.","markups":[]},{"name":"3d9c","type":1,"text":"First is the Monte — Carlo estimate, which roles out an episode in calculating the discounter total reward from the rewards sequence.","markups":[{"type":1,"start":13,"end":35}]},{"name":"3dd5","type":1,"text":"In Dynamic Programming, the Markov Decision Process (MDP) is solved by using value iteration and policy iteration. Both techniques require transition and reward probabilities to find the optimal policy. When the transition and reward probabilities are unknown, we use the Monte Carlo method to solve MDP. The Monte Carlo method requires only sample sequences of states, actions, and rewards. Monte Carlo methods are applied only to the episodic tasks.","markups":[{"type":1,"start":28,"end":52},{"type":1,"start":53,"end":56}]},{"name":"99cf","type":1,"text":"We can approach the Monte — Carlo estimate by considering that the Agent play in episode A. We start in state St and take action At. Based on the process the Agent transits to state St+1. From environment, the Agent receives the reward Rt+1. This process can be continued until the Agent reaches the end of the episode. The Agent can take part also in other episodes like B, C, and D. Some of those episodes will have trajectories that go through the same states, which influences that the value function is computed as average of estimates. Estimates for a state can vary across episodes so the Monte — Carlo estimates will have high variance.","markups":[]},{"name":"8f9a","type":1,"text":"On the other side, we can apply the Temporal Difference estimate. Here, the TD approximates the current estimate based on the previously learned estimate, which is also called bootstrapping (we try to predict the state values).","markups":[{"type":1,"start":36,"end":64},{"type":1,"start":176,"end":189}]},{"name":"78e6","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*6QCHeMdt3kYPmnNwYqbeyw.png","originalWidth":505,"originalHeight":121}},{"name":"7d33","type":1,"text":"Above equation is the difference (TD error) between the actual reward and the expected reward multiplied by the learning rate alpha (the learning rate, also called step size, used for convergence reason).","markups":[{"type":1,"start":34,"end":42}]},{"name":"06e5","type":1,"text":"TD estimates are low variance because you’re only compounding a single time step of randomness instead of a full rollout like in Monte — Carlo estimate. However, due to applying a bootstrapping (dynamic programming) the next state is only estimated. Estimated values introduce bias into our calculations. The agent will learn faster, but the converging problems can occur.","markups":[]},{"name":"cb7e","type":1,"text":"Deriving the Actor — Critic concept requires to consider first the policy — based approach (performed by AGENT). As we discussed before the Agent playing the game increases the probability of actions that lead to a win, and decrease the probability of actions that lead to losses. However, such process is cumbersome due to lot of data to approach the optimal policy.","markups":[{"type":1,"start":67,"end":90},{"type":1,"start":105,"end":110}]},{"name":"d95b","type":1,"text":"On the other hand, we can evaluate the value — based approach (performed by CRITIC), where the guesses are performed on-the-fly, throughout all the episode. At the beginning our guesses will be misaligned (not correct). But over time, when we capture more experience, we will be able to make solid guesses. Though, this is not a perfect approach either, guesses introduce a bias because they’ll sometimes be wrong, particularly because of a lack of experience.","markups":[{"type":1,"start":39,"end":62},{"type":1,"start":75,"end":82},{"type":1,"start":84,"end":85}]},{"name":"58d2","type":1,"text":"Based on this short analysis we can summarize that the Agent using policy — based approach is learning to act (agent learns by interacting with environment and adjusts the probabilities of good and bad actions, while in a value-based approach, the agent is learning to estimate states and actions.). In parallel we use a Critic, which is to be able to evaluate the quality of actions more quickly (proper action or not) and speed up learning. Actor-critic method is more stable than value — based agents, while requiring fewer training samples than policy-based agents.","markups":[]},{"name":"3b30","type":1,"text":"As a result of merge Actor — Critic we utilize two separate neural networks. The role of the Actor network is to determine the best actions (from probability distribution) in the state by tuning the parameter θ (weights). The Critic by computing the temporal difference error TD (estimating expected returns), evaluates the action generated by the Actor.","markups":[]},{"name":"070a","type":1,"text":"In previous project (DQN) we discussed about discrete environments where the number of action which could be performed by the Agent was limited. However, very often we operate with continuous environment, securing continuous motion. The number of action then can be unlimited (huge). This is one of the problems DDPG solves. DDPG algorithm uses Agent — Critic concept, where we use two deep neural networks.","markups":[]},{"name":"21f8","type":1,"text":"In DDPG, the Actor is used to approximate the optimal policy deterministically. That means we want always to generate the best believed action for any given state.","markups":[]},{"name":"cfbf","type":1,"text":"The Actor follows the policy-based approach, and learns how to act by directly estimating the optimal policy and maximizing reward through gradient ascent. The Critic however, utilizes the value-based approach and learns how to estimate the value of different state — action pairs.","markups":[]},{"name":"daff","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ZsfURoPWKsME4XKvhGkC9Q.png","originalWidth":621,"originalHeight":272}},{"name":"925a","type":1,"text":"The DDPG algorithm can be presented as follows (Reinforcement Learning with Python by Sudharsan Ravichandiran):","markups":[]},{"name":"5abc","type":1,"text":"1. The Actor and the Critic use separate neural network.","markups":[]},{"name":"f1ce","type":1,"text":"2. The Actor network with:","markups":[]},{"name":"f8c2","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*rMf74wXA1I43coTyMeWnrA.png","originalWidth":253,"originalHeight":64}},{"name":"d402","type":1,"text":"which takes input as a state s and results in the action a where","markups":[{"type":1,"start":23,"end":30},{"type":1,"start":50,"end":58}]},{"name":"a758","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*SgD0lAHLajtF5YmIjHEsNg.png","originalWidth":61,"originalHeight":55}},{"name":"4e5c","type":1,"text":"is the Actor network learning weights. The actor here is used to approximate the optimal policy deterministically. That means that the output is the best believed action for any given state. This is unlike a stochastic policy (probability distribution) in which we want the policy to learn a probability distribution over the actions. In DDPG, we want the believed best action every single time we query the actor network. The actor is basically learning the argmax a Q(S, a), which is the best action.","markups":[]},{"name":"11de","type":1,"text":"3. The Critic network","markups":[]},{"name":"d813","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*DLQzEM576I61ep3YFK7IPg.png","originalWidth":706,"originalHeight":76}},{"name":"0b7f","type":1,"text":"which takes an input as a state s and action a and returns the Q value where is the Critic network","markups":[{"type":1,"start":26,"end":33},{"type":1,"start":38,"end":46},{"type":2,"start":63,"end":65}]},{"name":"5c18","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*w3xvae_LR7l2_Bf4lNoS4w.png","originalWidth":69,"originalHeight":59}},{"name":"67d5","type":1,"text":"weights. The critic learns to evaluate the optimal action value function by using the actors best believed action.","markups":[]},{"name":"1023","type":1,"text":"4. We define a target network for both the Actor network","markups":[]},{"name":"9293","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*FsAMxzgq0kr1a_MDT6snxw.png","originalWidth":196,"originalHeight":79}},{"name":"2162","type":1,"text":"and Critic network respectively,","markups":[]},{"name":"8279","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*xadJJKgb67WWweCriNmc2g.png","originalWidth":253,"originalHeight":81}},{"name":"849d","type":1,"text":"where","markups":[]},{"name":"b933","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Nb1hks8St5aU5oMJch8sjg.png","originalWidth":178,"originalHeight":83}},{"name":"8b0c","type":1,"text":"are the weights of the target Actor and Critic network.","markups":[]},{"name":"a2fa","type":1,"text":"5. Next, we perform the update of Actor network weights with policy gradients and the Critic network weight with the gradients calculated from the TD error.","markups":[]},{"name":"b6fe","type":1,"text":"6. In order, to select correct action, first we have to add an exploration noise N to the action produced by Actor:","markups":[{"type":1,"start":75,"end":82},{"type":2,"start":81,"end":83}]},{"name":"e529","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Jk2mUgL-rBGLa08kN-gHoQ.png","originalWidth":284,"originalHeight":54}},{"name":"7349","type":1,"text":"(add noise to encourage exploration since policy is deterministic).","markups":[]},{"name":"09b9","type":1,"text":"7. Selected action in a state, s, receive a reward, r and move to a new state, s’.","markups":[{"type":2,"start":31,"end":32},{"type":2,"start":52,"end":54},{"type":2,"start":79,"end":81}]},{"name":"8922","type":1,"text":"8. We store this transition information in an experience replay buffer.","markups":[]},{"name":"96ac","type":1,"text":"9. As it is performed while we use DQN algorithm, we sample transitions from the replay buffer and train the network, and then we calculate the target Q value:","markups":[{"type":2,"start":151,"end":153}]},{"name":"0cfc","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*MgHdRUg85E28y4AQULDgMw.png","originalWidth":631,"originalHeight":58}},{"name":"2443","type":1,"text":"10. Then, we can compute the TD error as:","markups":[]},{"name":"1080","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2dv67i3u1vPYTPyOpkB46w.png","originalWidth":378,"originalHeight":80}},{"name":"aa5e","type":1,"text":"11. Subsequently, we perform the update of the Critic networks weights with gradients calculated from this loss L.","markups":[{"type":2,"start":112,"end":113}]},{"name":"a57f","type":1,"text":"12. Then, we update our policy network weights using a policy gradient.","markups":[]},{"name":"726c","type":1,"text":"13. Next, we update the weights of Actor and Critic network in the target network. In DDPG algorithm topology consist of two copies of network weights for each network, (Actor: regular and target) and (Critic: regular and target). In DDPG, the target networks are updated using a soft updates strategy. A soft update strategy consists of slowly blending regular network weights with target network weights. In means that every time step we make our target network be 99.99 percent of target network weights and only a 0.01 percent of regular network weights (slowly mix of regular network weights into target network weights).","markups":[{"type":1,"start":280,"end":301}]},{"name":"faaa","type":1,"text":"14. We update the weights of the target networks (Agent, Critic) slowly, which promotes greater stability (soft updates strategy):","markups":[]},{"name":"a83f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*O90_mr594IfWJTb4Bd_MBA.png","originalWidth":314,"originalHeight":116}},{"name":"b5e4","type":1,"text":"DDPG algorithm can be expressed in conscience shape as a pseudocode:","markups":[]},{"name":"f968","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*k20x517z-mh_54amzThOHA.png","originalWidth":650,"originalHeight":488}},{"name":"ebd1","type":1,"text":"General overview of DDPG algorithm flow was portrayed on underneath chart,","markups":[]},{"name":"8618","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*x6HIECcvr60kzSHdDU0zZw.png","originalWidth":754,"originalHeight":440}},{"name":"e52a","type":1,"text":"6. PROJECT SETUP. RESULTS.","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":3,"end":26}]},{"name":"662b","type":1,"text":"In Continuous Control project following setup of neural networks (for Agent and Critic) were involved.","markups":[]},{"name":"dc2f","type":1,"text":"Depicted below plot of rewards illustrates that the agent is able to receive an average reward (over 100 episodes, and over all 20 Agents) while playing 206 episodes.","markups":[]},{"name":"2132","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*30KmcuCecdEQo-aYyqqynQ.png","originalWidth":602,"originalHeight":381}},{"name":"ae30","type":1,"text":"Applied Deep Neural Network architecture:","markups":[{"type":1,"start":0,"end":41}]},{"name":"d95c","type":1,"text":"Input layer FC1: 37 nodes in, 64 nodes out\nHidden layer FC2: 64 nodes in, 64 nodes out\nHidden layer FC3: 64 nodes in, 64 nodes out\nOutput layer: 64 nodes in, 4 out — action size","markups":[]},{"name":"aa36","type":1,"text":"Applied hyperparameters:","markups":[{"type":1,"start":0,"end":24}]},{"name":"62ae","type":1,"text":"BUFFER_SIZE = int(1e5) # replay buffer size\nBATCH_SIZE = 64 # minibatch size\nGAMMA = 0.99 # discount factor\nTAU = 1e-3 # for soft update of target parameters\nLR = 5e-4 # learning rate\nUPDATE_EVERY = 4 # how often to update the network\nEpsilon start = 1.0\nEpsilon start = 0.01\nEpsilon decay = 0.999","markups":[]},{"name":"b1d1","type":1,"text":"7. IDEAS OF FUTURE WORK","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":3,"end":23}]},{"name":"7992","type":1,"text":"The discussed work compounds contemporary advances in deep learning and reinforcement learning. Modern combination yields exceptional results in solving challenging AI issues across a variety of domains. However, in this project a few limitations to presented approach remain. Future work should concentrate mainly on following tasks. Most notably, which was treated as a minor case was tuning of the hyper parameters. Future work can include more parameter choice and checks in order to limit number of episodes exhausting the project goal. This can also comprise the deep neural network architecture design. The other thing which will be reasonable to verify is the choice of applied algorithm. Here, it is suggest to deploy the A3C (Asynchronous Advantage Actor — Critic) algorithm which exploits multiple agents. Each Agent has its own set of network parameters. Additionally, the Agent interacts (in parallel to the other Agents) with its own copy of the environment. Agents (workers) follows a different exploration strategy to learn an optimal policy. Following this, the Agents compute value and policy loss so the derived gradient can be applied the global network. Proceeding process is continued and the advantage function is estimated. Finally, the global value loss and policy loss are computed. The policy loss function includes the entropy component, which reveals the spread of action probabilities. Low entropy secures that one of the actions has a higher probability then the other actions, so the Agent has a good opportunity for convenient action selection. Although, the high entropy displays that every action’s probability is the same, so the Agent will be unsure as to which action to perform. We use the entropy to improve exploration, by encouraging the Agent to be reliable of excavation of perfect (optimal) action. A3C algorithm can sufficiently speed up the training process.","markups":[{"type":1,"start":228,"end":229},{"type":1,"start":401,"end":417},{"type":1,"start":581,"end":608},{"type":1,"start":736,"end":773}]},{"name":"c89b","type":1,"text":"Please, find the full code for this project on my Github.","markups":[{"type":3,"start":50,"end":56,"href":"https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p2_continuous-control","title":"","rel":"","anchorType":0}]}],"sections":[{"name":"c996","startIndex":0}]},"postDisplay":{"coverless":true}},"virtuals":{"allowNotes":true,"previewImage":{"imageId":"1*_SHO6We5laYHoqOtAR-IEw.png","filter":"","backgroundSize":"","originalWidth":1951,"originalHeight":1091,"strategy":"resample","height":0,"width":0},"wordCount":3679,"imageCount":30,"readingTime":16.133018867924527,"subtitle":"GOAL","usersBySocialRecommends":[],"noIndex":false,"recommends":9,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"reinforcement-learning","name":"Reinforcement Learning","postCount":1556,"metadata":{"postCount":1556,"coverImage":{"id":"1*dmj5qeZdqnVLZTqj25SCpw.png","originalWidth":1600,"originalHeight":662}},"type":"Tag"},{"slug":"artificial-intelligence","name":"Artificial Intelligence","postCount":87780,"metadata":{"postCount":87780,"coverImage":{"id":"1*gAn_BSffVBcwCIR6bDgK1g.jpeg"}},"type":"Tag"},{"slug":"programming","name":"Programming","postCount":103716,"metadata":{"postCount":103716,"coverImage":{"id":"1*AmeD38st98PaTMaxfFKc1A.png","originalWidth":1920,"originalHeight":1080,"isFeatured":true}},"type":"Tag"},{"slug":"robotics","name":"Robotics","postCount":11251,"metadata":{"postCount":11251,"coverImage":{"id":"1*M8VTYbOl-iI8qBwlSPGTzA.jpeg","originalWidth":1920,"originalHeight":1280,"isFeatured":true}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":2,"links":{"entries":[{"url":"https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p1_navigation","alts":[],"httpStatus":200},{"url":"https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p2_continuous-control","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1559489517405},"isLockedPreviewOnly":false,"metaDescription":"","totalClapCount":65,"sectionCount":1,"readingList":0,"topics":[]},"coverless":true,"slug":"deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":false,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"1*_SHO6We5laYHoqOtAR-IEw.png","originalWidth":1951,"originalHeight":1091,"isFeatured":true}},{"name":"05c8","type":3,"text":"Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algorithm.","markups":[],"alignment":1},{"name":"4ef1","type":10,"text":"GOAL","markups":[{"type":1,"start":0,"end":4}],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"GOAL"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43","approvedHomeCollectionId":"","newsletterId":"","webCanonicalUrl":"https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43","mediumUrl":"https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-deep-deterministic-policy-gradient-ddpg-algoritm-5a823da91b43","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":1552757327990,"isProxyPost":false,"proxyPostFaviconUrl":"","proxyPostProviderName":"","proxyPostType":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"hideMeter":false,"collectionUserRelations":[],"mode":null,"references":{"User":{"c9076eed918c":{"userId":"c9076eed918c","name":"Markus Buchholz","username":"markus.x.buchholz","createdAt":1501836488487,"imageId":"1*12k3P8bjeeTvt1EoxSTDEg.jpeg","backgroundImageId":"","bio":"","twitterScreenName":"","socialStats":{"userId":"c9076eed918c","usersFollowedCount":77,"usersFollowedByCount":19,"type":"SocialStats"},"social":{"userId":"be025283e717","targetUserId":"c9076eed918c","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":1532419693000,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Social":{"c9076eed918c":{"userId":"be025283e717","targetUserId":"c9076eed918c","type":"Social"}},"SocialStats":{"c9076eed918c":{"userId":"c9076eed918c","usersFollowedCount":77,"usersFollowedByCount":19,"type":"SocialStats"}}}})
// ]]></script><script>window.PARSELY = window.PARSELY || { autotrack: false }</script><script id="parsely-cfg" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/p.js.download"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/main-common-async.bundle.cUeGLoj7sHNpuw-bFOuTeQ.js.download"></script><div id="weava-permanent-marker" date="1562635570180"></div><div id="weava-ui-wrapper"><div class="weava-drop-area-wrapper"><div class="weava-drop-area"></div>
<div class="weava-drop-area-text">Drop here!</div></div></div><script charset="UTF-8" src="./Deep Reinforcement Learning. Deep Deterministic Policy Gradient (DDPG) algoritm._files/main-notes.bundle.RzL89au1QllN2FPu92pO7w.js.download"></script></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>